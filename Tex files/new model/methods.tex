\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1.2in]{geometry}
\usepackage{amssymb, amsmath, amsthm}
\usepackage{graphicx}
\usepackage[titletoc]{appendix}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Commands

% Distributions
\newcommand{\Normaldist}{\text{Normal}}
\newcommand{\Gammadist}{\text{Gamma}}
\newcommand{\ScaledInvChiSquaredTdist}{\text{Scale-Inv-}$\chi^2$} % T for text
\newcommand{\ScaledInvChiSquareddist}{\text{Scale-Inv-}\chi^2}
\newcommand{\NegBindist}{\text{NegBin}}
\newcommand{\Multinomialdist}{\text{Multinomial}}
\newcommand{\Betadist}{\text{Beta}}
\newcommand{\Bernoullidist}{\text{Bernoulli}}
\newcommand{\Uniformdist}{\text{Uniform}}
\newcommand{\DPdist}{\text{DP}}
\newcommand{\TruncNormaldist}[2]{\text{TruncNormal}_{(#1, #2)}}

% Clustering
\newcommand{\ci}[1]{\mathbf{c}_{#1}}
\newcommand{\concc}{\mathbf{\theta}}
\newcommand{\probc}{\mathbf{\pi_{\mathbf{c}}}}

% Growth
\newcommand{\growth}{\mathbf{a}_1}
\newcommand{\growthi}[1]{\mathbf{a}_{1,#1}}
\newcommand{\meangrowth}{\mu_{\growth}}
\newcommand{\vargrowth}{\sigma^2_{\growth}}
\newcommand{\dofgrowth}{\nu_{\growth}}
\newcommand{\scalegrowth}{\tau^2_{\growth}}

% Self-interactions
\newcommand{\si}{\mathbf{a}_2}
\newcommand{\sii}[1]{\mathbf{a}_{2,#1}}
\newcommand{\meansi}{\mu_{\si}}
\newcommand{\varsi}{\sigma^2_{\si}}
\newcommand{\dofsi}{\nu_{\si}}
\newcommand{\scalesi}{\tau^2_{\si}}

% Interactions
\newcommand{\interact}{\mathbf{b}}
\newcommand{\interactij}[2]{\interact_{#1, #2}}
\newcommand{\interactcij}[2]{\interact_{\ci{#1}, \ci{#2}}}
\newcommand{\zinteract}{\mathbf{z}^{(\interact)}}
\newcommand{\zinteractij}[2]{\mathbf{z}^{(\interact)}_{#1, #2}}
\newcommand{\zinteractcij}[2]{\mathbf{z}^{(\interact)}_{\ci{#1}, \ci{#2}}}
\newcommand{\varinteract}{\sigma^2_{\interact}}
\newcommand{\dofinteract}{\nu_{\interact}}
\newcommand{\scaleinteract}{\tau^2_{\interact}}
\newcommand{\probinteract}{\mathbf{\pi}_{\interact}}

% Perturbations
\newcommand{\pert}{\mathbf{\gamma}}
\newcommand{\perti}[1]{\mathbf{\gamma}^{(#1)}} % Just the ith perturbation
\newcommand{\pertic}[2]{\mathbf{\gamma}^{(#1)}_{\ci{#2}}} % Perturbation i for the cith cluster
\newcommand{\zpert}{\mathbf{z}^{(\pert)}}
\newcommand{\zperti}[1]{\mathbf{z}^{(\pert, #1)}}
\newcommand{\zpertic}[2]{\mathbf{z}^{(\pert, #1)}_{\ci{#2}}}
\newcommand{\varpert}{\mathbf{\sigma}^2_{\pert}}
\newcommand{\varperti}[1]{\mathbf{\sigma}^2_{\pert, #1}}
\newcommand{\dofpert}{\nu_{\pert}}
\newcommand{\scalepert}{\tau^2_{\pert}}
\newcommand{\probpert}{\mathbf{\pi}_{\pert}}
\newcommand{\probperti}[1]{\mathbf{\pi}_{\pert, #1}}
\newcommand{\stepperti}[1]{h_{#1}}

% Process Variance
\newcommand{\pv}{\mathbf{\sigma}_w^2}
\newcommand{\pstd}{\mathbf{\sigma}_w}
\newcommand{\dofpv}{\nu_w}
\newcommand{\scalepv}{\tau^2_{\pv}}

% Latent space
\newcommand{\varsik}[4]{#1_{#2 #3}(#4)}
\newcommand{\musik}[3]{\mu_{#1 #2}(#3)}
\newcommand{\musi}[2]{\mu_{#1 #2}}
\newcommand{\musk}[2]{\mu_{#1}(#2)}
\newcommand{\musikexp}[4]{\mu^{#4}_{#1 #2}(#3)}
\newcommand{\xsik}[3]{\mathbf{x}_{#1 #2}(#3)}
\newcommand{\xsikexp}[4]{\mathbf{x}^{#4}_{#1 #2}(#3)}
\newcommand{\xsi}[2]{\mathbf{x}_{#1 #2}}
\newcommand{\xii}[1]{\mathbf{x}_{#1}}
\newcommand{\xsk}[2]{\mathbf{x}_{#1}(#2)}

% Error Model
\newcommand{\countsik}[3]{\mathbf{y}_{#1 #2}(#3)}
\newcommand{\countsk}[2]{\mathbf{y}_{#1}(#2)}
\newcommand{\countsi}[2]{\mathbf{y}_{#1 #2}}
\newcommand{\countparamone}{d_0}
\newcommand{\countparamtwo}{d_1}
\newcommand{\qpcrsk}[2]{\mathbf{Q}_{#1}(#2)}
\newcommand{\varpropsik}[3]{\sigma^2_{\text{prop}}(#1, #2, #3)}


% Miscellaneous
\newcommand{\dt}{\Delta_k}
\newcommand{\dtk}[1]{\Delta_{#1}} % Change in time
\newcommand{\reals}{\mathbb{R}} % Real numbers
\newcommand{\ints}{\mathbb{Z}} % Integers
\newcommand{\al}[1]{\mathbf{a}_{#1}} % either growth for self-=interactions
\newcommand{\dof}{\nu} % This is what is general use for dof of SICS
\newcommand{\scale}{\tau^2} % This is general use for scale of SICS


\title{MDSINE 2.0 Supplement}
\author{David Kaplan}
\begin{document}

\maketitle
\tableofcontents
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Notation}
\label{section:Notation}

We parameterize the \ScaledInvChiSquaredTdist distribution with degrees of freedom $\nu$ and scale $\tau^2$ as
\begin{align}
    \label{eqn:ScaledInvChiSquareddist}
    \ScaledInvChiSquareddist (x ; \nu, \tau^2) = 
        \frac{(\tau^2\nu/2)^{-\nu/2}}{\Gamma(\nu/2)} 
        x ^ {(-\nu/2)-1}
        \exp \Big( -\frac{-\nu \tau^2}{2x} \Big )
\end{align}
We parameterize the Gamma distribution with shape $k$ and scale $\theta$ as
\begin{align}
    \label{eqn:Gammadist}
    \Gammadist(x ; k, \theta) =
        \frac{1}{\Gamma(k)\theta^k} x^{k-1}
        \exp{\Big( -\frac{x}{\theta} \Big)}
\end{align}
We parameterize the Negative Binomial distribution with mean $\phi$ and dispersion $\epsilon$ as
\begin{align}
    \label{eqn:NegBindist}
    \NegBindist(y;\phi,\epsilon) & =
        \frac{\Gamma(r+y)}{y!\Gamma(r)}
        \Big(
            \frac{\phi}{r+\phi}
        \Big)^y
        \Big(
            \frac{r}{r+\phi}
        \Big)^r \\
    r & = \frac{1}{\epsilon}
\end{align}
We parameterize a truncated normal distribution with mean $\mu$, variance $\sigma^2$, lower bound $v_1$, and upper bound $v_2$ as
\begin{align}
    \TruncNormaldist{v_1}{v_2}(x ; \mu , \sigma^2) = 
        \frac{1}{(2 \pi)^{1/2} \sigma} 
        \frac{1}{
            \Phi \Big( \frac{v_2 - \mu}{\sigma} \Big) -
            \Phi \Big( \frac{v_1 - \mu}{\sigma} \Big)}
        \exp \left( \frac{- (x - \mu)^2}{2 \sigma^2}
        \right)
\end{align}
where $\Phi (\cdot)$ is the the cumulative distribution function of a standard normal distribution. Normal distributions are written in terms of the variance $\Normaldist(\mu, \sigma^2)$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Model Details}
\subsection{Dirichlet process for interactions and perturbations}
\label{subsection:dirichlet process}
We incorporate a Dirichlet Process (DP)-based clustering technique (\cite{cite:neal2000}, \cite{cite:ramussen2000}) to learn redundant interaction structures (interaction modules) and perturbation effects among OTUs. Let $\ci{i} \in \ints^+$ be the cluster assignment of OTU $i$. In the context of our dynamical systems module, only interaction coefficients between modules need to be learned ($\interactcij{i}{j}$), rather than interactions between each pair of OTUs ($\interactij{i}{j}$). If OTU $i$ and OTU $j$ are in different clusters, i.e. $\ci{i} \neq \ci{j}$, then $\interactcij{i}{j} \in \reals$ is the coefficient representing the (interaction) effect from cluster $\ci{j}$ to cluster $\ci{i}$. If OTU  and OTU $j$ are in the same cluster, i.e. $\ci{i} = \ci{j}$, then we assume there is no interaction. If OTU $l$ is in the same cluster as OTU $j$, different than OTU $i$, then $\interactcij{i}{l} = \interactcij{i}{j}$ by definition.

Additionally, perturbation effects are modeled on the cluster level: $\perti{j}$, where $\perti{j}$ is the $j^{\text{th}}$ perturbation for the cluster assigned to OTU $i$. If OTU $i$ and OTU $l$ are in the same cluster, $\pertic{j}{i} = \pertic{j}{l}$. Analogously, if OTU $i$ and OTU $l$ are not in the same cluster, $\pertic{j}{i} \neq \pertic{j}{l}$.

We generate the mixture weights $C_1, C_2,...$ from a "stick-breaking" construction with concentration parameter $\concc$:
\begin{align}
    V_1, V_2, ... & \sim \Betadist (1,\concc) \\
    C_k & = V_k \prod_{j=1}^{k-1}(1-V_j)
\end{align}
Atom locations locations $\eta_{1}, \eta_{2}, ...$ are drawn from a base distribution $G_0$.
\begin{align}
    \eta_1, \eta_2,... \sim G_0 \\
    \Theta = \sum_{k=1}^{\infty}C_k \delta_{\eta_k}
\end{align}
where $\Theta$ is a draw from a Dirchlet process with concentration parameter $\concc$ and base distribution $G_0$, or $\Theta \sim \DPdist(\concc, G_0)$. $\delta_{\eta_k}$ is an indicator function that is 0 everywhere, except for $\delta_{\eta_k}(C_k)=1$.

To specify the dirichlet process mixture model, we specify cluster assignment variable $\ci{i}$ for each OTU $i$, and a density $\probc$ which takes an atom location as a parameter.
\begin{align}
    \ci{i} & \sim \Multinomialdist (C_k) \\
    i | \ci{i} & \sim \probc( \cdot | \eta_{C_k})
\end{align}

\subsection{Baseline logistic growth}
We model the dynamics for OTU $i$ in subject $s$ at time $t_k$ with the stochastic version of the generalized Lotka-Volterra (gLV) equations:
\begin{align}
    \label{eqn: logistic growth}
    \log (\musik{s}{i}{k+1} ) & = 
        \log ( \xsik{s}{i}{k} ) + 
        \dt \Big[
            \growthi{i} \Big(
                1 + \sum_{p=1}^P \pertic{p}{i} \zpertic{p}{i} \stepperti{p}
            \Big) \\
            & \quad \quad + \sii{i} \xsik{s}{i}{k}  + 
            \sum_{\ci{i} \neq \ci{j}} 
                \interactcij{i}{j}
                \zinteractcij{i}{j}
                \xsik{s}{j}{k} \nonumber
        \Big]
\end{align}
\begin{align}
    \log ( \xsik{s}{i}{k+1} ) \sim \Normaldist ( \log ( \musik{s}{i}{k+1} ) , \dt \pv)
\end{align}
where $\xsik{s}{i}{k} \in \reals^{\ge 0}$ is the latent abundance of OTU $i$ at time $t_k$ for subject $s$, and $\dt = t_{k+1} - t_k$. The growth $\growthi{i}$ and self-interaction $\sii{i}$ variables are modeled for each OTU and are parameterized with a truncated normal distribution:
\begin{align}
    \growthi{i} & \sim \Normaldist_{(0, \infty)} ( \meangrowth , \vargrowth ) \\
    \sii{i} & \sim \Normaldist_{(-\infty , 0)} ( \meansi , \varsi )
\end{align}
We place \ScaledInvChiSquaredTdist priors on the variances:
\begin{align}
    \vargrowth & \sim \ScaledInvChiSquareddist (\dofgrowth, \scalegrowth) \\
    \varsi & \sim \ScaledInvChiSquareddist (\dofsi, \scalesi)
\end{align}
The variable $\interactcij{i}{j}$ represents the interaction coefficient from cluster $\ci{i}$ to cluster $\ci{j}$ and is parameterized with a normal distribution:
\begin{align}
    \interactcij{i}{j} & \sim \Normaldist ( 0, \varinteract ) \\
    \varinteract & \sim \ScaledInvChiSquareddist (\dofinteract, \scaleinteract)
\end{align}
The variable $\zinteractcij{i}{j}$ represents the binary indicator variable that selects the interaction from $\ci{i}$ to $\ci{j}$ and is parameterized with a Bernoulli distribution with probability $\probinteract$:
\begin{align}
    \zinteractcij{i}{j} & \sim \Bernoullidist (\probinteract) \\
    \probinteract & \sim \Betadist ( b_{1,\interact} , b_{2,\interact} )
\end{align}
The process variance $\pv$ is parameterized with an \ScaledInvChiSquaredTdist:
\begin{align}
    \pv \sim \ScaledInvChiSquareddist (\dofpv, \scalepv)
\end{align}

\subsection{Perturbations}
We assume the baseline growth rate can be modulated by a perturbation. The variable $\pertic{p}{i}$ represents the magnitude of the $p^{\text{th}}$ perturbation on cluster $\ci{i}$ and is parameterized with a normal distribution:
\begin{align}
    \pertic{p}{i} & \sim \Normaldist ( 0, \varperti{p} ) \\
    \varperti{p} & \sim \ScaledInvChiSquareddist (\dofpert, \scalepert)
\end{align}
The period that $\perti{p}$ is active is given by the step function $\stepperti{p}$:
\begin{align}
    \stepperti{p} (k) = \begin{cases}
        1 & t^{\text{on}}_p < t_k \le t^{\text{off}}_p \\
        0 & \text{otherwise}
    \end{cases}
\end{align}
Additionally, there is a binary indicator variable $\zpertic{p}{i}$ that selects whether cluster $\ci{i}$ is affected by perturbation $\perti{p}$ and is parameterized by a Bernoulli distribution:
\begin{align}
    \zpertic{p}{i} & \sim \Bernoullidist (\probperti{p}) \\
    \probperti{p} & \sim \Betadist ( b_{1,\pert} , b_{2,\pert} )
\end{align}

\subsection{Error model}
\label{subsection:error model def}
The observed data are the sequencing counts $\countsik{s}{i}{k}$ and qPCR measurements $\qpcrsk{s}{k}$. We model $\countsik{s}{i}{k}$ with a negative binomial distribution \cite{cite:negbin}:
\begin{align}
    \countsik{s}{i}{k} & \sim \NegBindist \Big( 
        \phi_{si}(\xsk{s}{k}, r_s(k)),
        \epsilon_{si}(\xsk{s}{k}, \countparamone, \countparamtwo)
        \Big) \\
    \phi_{si}(\xsk{s}{k}, r_s(k)) & = r_s (k) 
        \frac{\xsik{s}{i}{k}}{\sum_i \xsik{s}{i}{k}} \\
    \epsilon_{si}(\xsk{s}{k}, \countparamone, \countparamtwo) & = 
        \frac{\countparamone}
        {\xsik{s}{i}{k} / \sum_i \xsik{s}{i}{k}} + \countparamtwo
\end{align}
where $r_s (k)$ is the total number of reads at time $t_k$ for subject $s$, and $\countparamone$ and $\countparamtwo$ are the negative binomial dispersion scaling parameters which are pre-trained on raw reads. We can model $\qpcrsk{s}{k}$ with a normal distribution:
\begin{align}
    \qpcrsk{s}{k} \sim \Normaldist \Big(
        \sum_i \xsik{s}{i}{k}, \sigma^2_{\qpcrsk{s}{k}}
    \Big)
\end{align}
where $\sigma^2_{\qpcrsk{s}{k}}$ is the variance over the triplicate measurements of the qPCR measurement at time $t_k$ for subject $s$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Model inference}
Before we perform inference on the model, we learn the negative binomial dispersion parameters ($\countparamone$ and $\countparamtwo$) offline and then fix them during inference. To see how inference on these parameters is performed, seed appendix \ref{appendix:learning negbin dispersion params}. Inference on all other parameters is performed using Markov Chain Monte Carlo (MCMC). We use Gibbs steps for variables with conjugate priors and Metropolis-Hastings (MH) steps otherwise. The order of the main sampling loop is as follows:
\begin{enumerate}
    \item Sample cluster interaction indicators $\zinteract$ and probability $\probinteract$.
    \item Sample perturbation indicators $\zperti{p}$ and probabilities $\probperti{p}$.
    \item Sample interaction magnitudes $\interactcij{i}{j}$ and perturbation magnitudes $\pertic{p}{i}$ jointly.
    \item Sample prior variances $\varinteract$ and $\varperti{p}$
    \item Sample growths $\growth$ and self-interactions $\si$ separately by randomly choosing which one to update first.
    \item Sample prior variances $\vargrowth$ and $\varsi$.
    \item Sample process variance $\pv$.
    \item Sample latent trajectory $\xsik{s}{i}{k}$.
    \item Sample cluster assignments $\ci{i}$.
    \item Sample concentration parameter $\concc$.
\end{enumerate}

\subsection{Logistic growth parameters}
Since $\interactcij{i}{j}$ and $\pertic{p}{i}$ have conjugate normal priors, they can be sampled with straight forward Gibbs sampling.
The priors of $\growthi{i}$ and $\sii{i}$ are truncated normal but are still conjugate and thus can be sampled with Gibbs sampling. To see a derivation of the posterior see appendix \ref{appendix:sampling growth and si}.

\subsection{Prior variances}

\subsubsection{Perturbation and interaction prior variances}
Since $\varinteract$ and $\varperti{p}$ have conjugate $\ScaledInvChiSquareddist$ priors, they can be sampled with straight forward Gibbs sampling.

\subsubsection{Growth and self-interaction prior variances}
The prior variances for $\vargrowth$ and $\varsi$ cannot be sampled directly and thus we do MH step. In the following section, $\al{l}, l=1,2$ refers to either the growths or the self-interactions (sampling is the same for either). The proposal for the $j^{\text{th}}$ MH step is a $\ScaledInvChiSquareddist$ distribution if the likelihood was a normal distribution instead of a truncated normal distribution:
\begin{align}
    (\sigma^{2}_{\al{l}})^{(*)} = \ScaledInvChiSquareddist ( \dof , \scale )
\end{align}
where
\begin{align}
    \dof & = \dof_{\al{l}} + n_o \\
    \scale & = \frac
        {\dof_{\al{l}} \scale_{\al{l}} + 
            \sum_{i=1}^{n_o}(\al{l,i} - \mu_{\al{l}})^2}
        {\dof_{\al{l}} + n_o}
\end{align}
The target distribution conditional on all other parameters $\Omega$ is:
\begin{align}
    p(\sigma_{\al{l}}^2 ; \Omega) \propto 
        \TruncNormaldist{v_1}{v_2}
        (\al{l}; \mu_{\al{}l}, \sigma_{\al{l}}^2)
\end{align}
where $v_1$ and $v_2$ are the respective truncation points for the growths or self-interactions. The acceptance probability for the $j^{\text{th}}$ MH step is
\begin{align}
    r_{\text{accept}}^{(j)} = \frac
        {
            p((\sigma^{2}_{\al{l}})^{(*)}; \Omega)
            \ScaledInvChiSquareddist((\sigma^{2}_{\al{l}})^{(j-1)} ; \dof, \scale)
        }
        {
            p((\sigma^{2}_{\al{l}})^{(j-1)}; \Omega)
            \ScaledInvChiSquareddist((\sigma^{2}_{\al{l}})^{(*)} ; \dof, \scale)
        }
\end{align}
The next value for $(\sigma^2_{\al{l}})^{(j)}$ is given by:
\begin{align}
    (\sigma^2_{\al{l}})^{(j)} = \begin{cases}
        (\sigma^2_{\al{l}})^{(*)} & \text{with probability } \min (1, r_{\text{accept}}^{(j)}) \\
        (\sigma^2_{\al{l}})^{(j-1)} & \text{otherwise}
    \end{cases}
\end{align}

\subsection{Process variance}
Since $\pv$ has a conjugate prior, it can be updated with straight forward Gibbs sampling.

\subsection{Cluster interaction and perturbation indicators}
A Gibbs step is used to update the interaction indicator $\zinteract$ and perturbation indicator $\zpert$ variable to determine if an interaction or perturbation is present or not. Due to conjugacy, $\zinteract$ and $\zpert$ can be integrated out jointly conditioned on the growth rates and self-interactions ($\growth$ and $\si$). A derivation of this marginalization can be seen in appendix \ref{appendix:marginalization}. In the following equations, let $F_u (\zinteractcij{i}{j} , \zpertic{p}{i} | \growth, \si)$, $u \in \{ 0, 1 \}$ be the marginal likelihood that $\zinteractcij{i}{j} = u$ or $\zpertic{p}{i} = 0$:
\begin{align}
    F_u ( \zinteractcij{i}{j}, \zpertic{p}{i} | \growth \si) = \int \prod_{s} \prod_{k} 
        \Normaldist \left( 
            \log ( \xsik{s}{i}{k} ) ; 
            \log (\musik{s}{i}{k} ) , 
            \dt \pv \right) 
        \text{d}(\pert, \interact)
\end{align}
where $\log (\musik{s}{i}{k} )$ is the mean (defined in equation \ref{eqn: logistic growth}) calculated for the current assignments of the indicators $\zpert$ and $\zinteract$.
\subsubsection{Sampling indicators of interactions}
Let
\begin{align}
    P_0^{(\interact)} ( \zinteractcij{i}{j} = 0 , 
        \zpert | \growth , \si) = F_0 (\zinteractcij{i}{j} = 0) \Bernoullidist (0 ; \probinteract) \\
    P_1^{(\interact)} ( \zinteractcij{i}{j} = 1 , 
        \zpert | \growth , \si) = F_1 (\zinteractcij{i}{j} = 1) \Bernoullidist (1 ; \probinteract)
\end{align}
be the posterior probabilities of assigning $\zinteractcij{i}{j}$ to 0 and 1, respectively. Given both likelihoods, we sample $u | \Omega$.

\subsubsection{Sampling indicators of perturbations}
Let
\begin{align}
    P_0^{(\pert)} ( \zpertic{p}{i} = 0 , 
        \zinteract | \growth , \si) = F_0 (\zpertic{p}{i} = 0) \Bernoullidist (0 ; \probperti{p}) \\
    P_1^{(\pert)} ( \zpertic{p}{i} = 1 , 
        \zinteract | \growth , \si) = F_1 (\zpertic{p}{i} = 1) \Bernoullidist (1 ; \probperti{p})
\end{align}
be the posterior probabilities of assigning $\zpertic{p}{i}$ to 0 and 1, respectively. Given both likelihoods, we sample $u | \Omega$.

\subsection{Cluster interaction and perturbation probabilities}
Since $\probinteract$ and $\probperti{p}$ have conjugate beta priors, they can be sampled with straightforward Gibbs sampling.

\subsection{Latent trajectory}
At each time point $t_k$, OTU $i$, subject $s$, and MH step $j$, the following jumping proposal is used:
\begin{align}
    \log \Big( \xsikexp{s}{i}{k}{(*)} \Big) \sim \Normaldist \Big(
        \log \Big( \xsikexp{s}{i}{k}{(j-1)} \Big), 
        \varpropsik{s}{i}{k} \Big)
\end{align}
The proposal variance the initial proposal variance is set to $\varpropsik{s}{i}{k} = \log ( \xsikexp{s}{i}{k}{2} / 100 )$ and is tuned during the first half of burn-in to adjust the acceptance rate to $0.44$; the optimal rate for a scalar MH step \cite{cite:BDA}. The unnormalized target distribution conditional on all other parameters $\Omega$ is:
\begin{align}
    p(\xsik{s}{i}{k}; \Omega) & \propto
        \NegBindist \Big(
            \countsik{s}{i}{k} ;
            \phi (\xsk{s}{k}),
            \epsilon (\xsk{s}{k}) \Big) \\
        & \times \Normaldist \Big(
            \qpcrsk{s}{k} ;
            \sum_i \xsik{s}{i}{k} ,
            \sigma^2_{\qpcrsk{s}{k}} \Big) \\
        & \times \Normaldist \Big(
            \log \Big( \xsik{s}{i}{k} \Big) ;
            \log \Big( \musik{s}{i}{k} \Big) , 
            \dt \pv \Big) \\
        & \times \Normaldist \Big(
            \log \Big( \xsik{s}{i}{k+1} \Big) ; 
            \log \Big( \musik{s}{i}{k+1} \Big) , 
            \dtk{k+1} \pv \Big)
\end{align}
where $\phi$ and $\epsilon$ were defined in section \ref{subsection:error model def}. Note that when we are sampling the last time point $(k = n_T)$ we do not calculate the likelihood of the future time point (set the log-likelihood to 0). when $k=0$, we cannot calculate the probability from the previous time point so we sample from the prior of $\mathbf{x}$. Additionally, if we are sampling an intermediate time point (There is no qPCR or count measurement for the time and subject we are sampling), we do not calculate the likelihood for the qPCR or counts (set the log-likelihood to 0). The acceptance probability for the $j^{\text{th}}$ MH sample is calculated as the ratio:
\begin{align}
    r_{\text{accept}}^{(j)} = \frac
        {
            p \Big( \log \left( \xsikexp{s}{i}{k}{(*)} \right) ; \Omega \Big)
            \Normaldist \Big( 
                \log \left( \xsikexp{s}{i}{k}{(j-1)} \right) ;
                \log \left( \xsikexp{s}{i}{k}{(*)} \right), 
                \varpropsik{s}{i}{k} \Big)}
        {
            p \Big( \log \left( \xsikexp{s}{i}{k}{(j-1)} \right) ; \Omega \Big)
            \Normaldist \Big( 
                \log \left( \xsikexp{s}{i}{k}{(*)} \right) ;
                \log \left(\xsikexp{s}{i}{k}{(j-1)} \right), 
                \varpropsik{s}{i}{k} \Big)}
\end{align}
Because our proposal distribution is symmetric, the forward and reverse jumping likelihoods are equal to each other, so our acceptance probability simplifies to:
\begin{align}
    r_{\text{accept}}^{(j)} = \frac
        {p \Big( \log \left( \xsikexp{s}{i}{k}{(*)} \right) ;
        \Omega \Big)}
        {p \Big( \log \left( \xsikexp{s}{i}{k}{(j-1)} \right) ; \Omega \Big)}
\end{align}
where
\begin{align}
    p \Big(  \xsik{s}{i}{k} ; \Omega \Big) & \propto 
        \Big( \sigma_Q \pv \sqrt{\dtk{k} \dtk{k+1}} \Big)^{-1} \\
        & \times \exp \left(
            -\frac{1}{2} 
            \left(\frac
                {\qpcrsk{s}{k} - \sum_i \xsik{s}{i}{k}}
                {\sigma_{\qpcrsk{s}{k}}} \right)^2 \right) \nonumber \\ 
        & \times \exp \left(
            -\frac{1}{2} 
            \frac
                {\left(
                    \log \left(\xsik{s}{i}{k} \right) - 
                    \log \left(\musik{s}{i}{k} \right) \right)^2 }
                { \dt \pv} \right) \nonumber \\ 
        & \times \exp \left(
            -\frac{1}{2} 
            \frac
                {\left(
                   \log \left( \xsik{s}{i}{k+1} \right) - 
                   \log \left( \musik{s}{i}{k+1} \right) 
                \right)^2 }
                { \dtk{k+1} \pv} \right) \nonumber \\
        & \times \frac
            {\Gamma (r + \countsik{s}{i}{k})}
            {\countsik{s}{i}{k} ! \Gamma (r)}
            \left(
                \frac
                    {\phi (\xsik{s}{i}{k})}
                    {r + \phi(\xsik{s}{i}{k})}
            \right)^{\countsik{s}{i}{k}}
            \left(
                \frac{r}{r + \phi(\xsik{s}{i}{k})}
            \right)^r \nonumber
\end{align}
where $r = 1/\epsilon(\xsik{s}{i}{k})$. If we eliminate variables that are not dependent on $\xsik{s}{i}{k}$, we are able to simplify to
\begin{align}
    p \Big(  \xsik{s}{i}{k} ; \Omega \Big) & \propto 
        \Big( \sigma_Q \pv \Big)^{-1} \\
        & \times \exp \left(
            -\frac{1}{2} 
            \left(\frac
                {\qpcrsk{s}{k} - \sum_i \xsik{s}{i}{k}}
                {\sigma_{\qpcrsk{s}{k}}} \right)^2 \right) \nonumber \\ 
        & \times \exp \left(
            -\frac{1}{2} 
            \frac
                {\left(
                    \log \left(\xsik{s}{i}{k} \right) - 
                    \log \left(\musik{s}{i}{k} \right) \right)^2 }
                { \dt  \pv} \right) \nonumber \\ 
        & \times \exp \left(
            -\frac{1}{2} 
            \frac
                {\left(
                   \log \left( \xsik{s}{i}{k+1} \right) - 
                   \log \left( \musik{s}{i}{k+1} \right) 
                \right)^2 }
                { \dtk{k+1} \pv} \right) \nonumber \\
        & \times
            \left(
                \frac
                    {\phi (\xsik{s}{i}{k})}
                    {r + \phi(\xsik{s}{i}{k})}
            \right)^{\countsik{s}{i}{k}}
            \left(
                \frac{r}{r + \phi(\xsik{s}{i}{k})}
            \right)^r \nonumber
\end{align}
The next values for $\xsik{s}{i}{k}$ are given by:
\begin{align}
    \xsikexp{s}{i}{k}{(j)} = \begin{cases}
        \xsikexp{s}{i}{k}{(*)} & \text{with probability }     
            \min(1, r^{(j)}_{\text{accept}}) \\
        \xsikexp{s}{i}{k}{(j-1)} & \text{otherwise}
    \end{cases}
\end{align}

\subsection{Cluster assignments}
The cluster assignment parameter $\ci{}$ is sampled suing Gibbs sampling according to algorithm 8 described in \cite{cite:neal2000}. First, we sample the assignments of OTU $i$ by marginalizing out the clusters. In the following equations, $F_m (\pertic{p}{i}, \ci{i} | \growth , \si )$ denotes the marginal likelihood for OTU $i$ assigned to cluster $m$. A derivation for marginalization can be seen in appendix \ref{appendix:marginalization}:
\begin{align}
    F_m (\xii{i} | \growth , \si ) = \int
        \prod_{s,k} \Normaldist \left(
            \log (\xsik{s}{i}{k} );
            \log (\musikexp{s}{i}{k}{(m)}) ,
            \dt \pv
        \right)
        \text{d}(\pert, \interact)
\end{align}
where $\log ( \musikexp{s}{i}{k}{(m)} )$ is the log mean of the dynamics when the OTU $i$ is assigned to cluster $m$
\begin{align}
    \log(\musikexp{s}{i}{k}{(m)}) & = \log ( \xsik{s}{i}{k-1} ) + 
        \dt \Bigg[
            \growthi{i} \left(
                1 + \sum_{p=1}^P \pert^{(p)}_{m} \zperti{p}_{m} \stepperti{p}
            \right) \\
            & \quad \quad + \sii{i} \xsik{s}{i}{k-1} +
            \sum_{m \neq \ci{j}} 
                \interactij{m}{\ci{j}}
                \zinteractij{m}{\ci{j}}
                \xsik{s}{i}{k-1}
        \Bigg] \nonumber 
\end{align}
Conditional on the growth rates and the self-interactions, the posterior distribution for cluster assignment $\ci{i}$ is
\begin{align}
    P(m=\ci{i} | \ci{-i}, \Omega) \propto \begin{cases}
        n_{-i,m} F_m(\xii{si}) & \text{for } m \in \ci{} \\
        \concc F_m (\xii{si}) & \text{for } m \text{ new cluster}
    \end{cases}
\end{align}
where $n_{-i,m}$ are the number of OTUs in cluster $m$ excluding OTU $i$. Given all these likelihoods, we then sample $m|\Omega$. In the case when calculating the likelihood of staying in a singleton cluster ($n_{-i,m}=0$), then we set $n_{-i,m}=\concc$.

\subsection{Cluster concentration}
The cluster concentration parameter $\concc$ is sampled according to the auxiliary variable method described in \cite{cite:escobarandwest1995}. Our posterior is a mixture of Gamma distributions with auxiliary variable $\eta$:
\begin{align}
\label{eqn:sample concc}
    P(\concc | \eta, n_c) & \sim \pi_\eta \Gammadist \left(
        \alpha_\concc + n_c , \hat{\beta}_\concc + \right) \\
        & \quad  (1 - \pi_\eta)\Gammadist \left( 
            \alpha_\concc + n_c - 1, \hat{\beta}_\concc
            \right) \nonumber
\end{align}
where
\begin{align}
    \hat{\beta}_\concc & = \frac{1}{1/\beta_\concc + \log (\eta)} \\
    \frac{\pi_\eta}{1-\pi_\eta} & = \frac
        {\alpha_\concc + n_c - 1}
        {n_o (\beta_\concc - \log (\eta))}
\end{align}
\begin{align}
    \label{eqn:sample eta}
    P(\eta | \concc, n_o) & \sim \Betadist ( \concc + 1, n_o )
\end{align}
where $n_c$ is the current number of clusters and $n_o$ are the number of OTUs. At each Gibbs step, using the current number of clusters $n_c$ and current value of the cluster concentration $\concc$, we can draw a new value of $\concc$ by
\begin{enumerate}
    \item Sample $\eta$ using (\ref{eqn:sample eta}).
    \item Sample a new $\concc$ from the mixture (\ref{eqn:sample concc}) with the current mixture weights.
\end{enumerate}
We repeat the above steps within a single Gibbs step until $\concc$ settles on a value. In MDSINE 2.0 we sample 20 times.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Initialization}
\subsection{Growth rates}
The prior mean of the growth rates is set to 2:
\begin{align}
    \meangrowth = 2
\end{align}
The prior variance of the growth rates is set by squaring the mean of the growth rates obtained when doing a simple logistic growth linear regression on the raw data (we exclude time points that are within perturbation periods) and then inflating by 100 to make it more diffuse:
\begin{align}
    Y & = \left[ \frac{\log (x_{si}(k+1)) - \log ( x_{si}(k) ) }{\dt} \right] \\
    X & = \left[
        \begin{bmatrix}
            1 & & \\
            & \ddots & \\
            & & 1
        \end{bmatrix}
        \begin{bmatrix}
            x_{s1}(k) & & \\
            & \ddots & \\
            & & x_{s n_o}(k)
        \end{bmatrix} \right] \\
    [a_1 \quad a_2] & = (X^T X)^{-1} X^T Y \\
    \vargrowth & = 100 \left( \frac{\sum_{i=1}^{n_o} a_{1,i}}{n_o} \right)^2 
\end{align}
The degrees of freedom is set to be diffuse ($\dofgrowth = 2.5 $) and the scale is set so that the mean of the prior is equal to the variance we calculated above:
\begin{align}
    \scalegrowth = \frac{\vargrowth (\dofgrowth - 2)}{\dofgrowth}
\end{align}
The initial value of the growth rates are all set to the mean:
\begin{align}
    \growthi{i} = \meangrowth \quad i = 1, \dots, n_o
\end{align}

\subsection{Self-interactions}
The initialization of the self-interactions is similar to that of the growth rates. We perform linear regression on the data conditional on the prior mean of the growth rates and then we set the variance similarly:
\begin{align}
     Y & = \left[ 
        \frac{\log (x_{si}(k+1)) - \log ( x_{si}(k) )}{\dt} 
        - \meangrowth \right] \\
     X & =
        \begin{bmatrix}
            x_{s1}(k) & & \\
            & \ddots & \\
            & & x_{s n_o}(k)
        \end{bmatrix} \\
    [a_2] & = (X^T X)^{-1} X^T Y \\
    \varsi & = 100 \left(
        \frac{\sum_{i=1}^{n_o} a_{2,i} }{n_o}
    \right)^2
\end{align}
The degrees of freedom of the prior variance is set to be diffuse ($\dofsi = 2.5$) and the scale is set so that the mean is equal to the variance we calculated above:
\begin{align}
    \scalesi = \frac{\varsi (\dofsi - 2)}{\dofsi}
\end{align}
The prior mean is set to the mean of the regressed values:
\begin{align}
    \meansi = \frac{\sum_{i=1}^{n_o}a_{2,i}}{n_o}
\end{align}
The initial value is set to the mean of the prior:
\begin{align}
    \sii{i} = \meansi \quad i = 1, \dots , n_o
\end{align}

\subsection{Cluster assignments and concentration}
We initialize the clusters using agglomerative clustering with a spearman similarity. The number of clusters is set to the expected number of clusters of a dirichlet process ($n_c = \log (n_o )$). We set the prior of the concentration parameter $\concc$ to a Gamma distribution:
\begin{align}
    \concc \sim \Gammadist (10^{-5}, 10^{5})
\end{align}
We set the initial value of $\concc$ to the mean of the prior.

\subsection{Interactions and perturbations}
We initialize the indicators of the interactions and perturbations to be all off. We set the prior of a perturbation and an interaction to be diffuse and a 50\% chance of there being there:
\begin{align}
    b_{1,\interact} = b_{2,\interact} = b_{1,\pert} = b_{2,\pert} = 0.5 
\end{align}
The initial value of $\probperti{p}$ and $\probinteract$ is set to the mean of the prior.

The prior means of the perturbations and the interactions are set to 0:
\begin{align}
    \mu_{\interact} & = 0 \\
    \mu_{\pert} & = 0
\end{align}
The prior variance of the perturbations are all set to 10:
\begin{align}
    \varperti{p} = 10 \quad p = 1, \dots, P
\end{align}
The prior variance of the interactions is set where it is diffuse and the mean is scaled down from the self-interactions mean: 
\begin{align}
    \dofinteract & = 2.5 \\
    \scaleinteract & = \frac{\scalesi}{10} \\
    \varinteract & = \mathbb{E}[\ScaledInvChiSquareddist (
        \dofinteract, \scaleinteract)]
\end{align}

\subsection{Latent trajectory}
We initialize the latent trajectory with LOESS interpolation from the data and truncate the abundance to be above zero by sampling a tight coupling to the interpolation:
\begin{align}
    \musik{s}{i}{k} & = \text{LOESS}(\mathbf{Q}, \mathbf{y}, s, i, k) \\
    \xsik{s}{i}{k} & \sim \Normaldist \left(
        \musik{s}{i}{k} , \sigma^2_q \right) \\
    \sigma^2_q & = 10^{-4} \musikexp{s}{i}{k}{2} + 10^{-4}
\end{align}
The prior of the latent trajectory is set to be diffuse:
\begin{align}
    \log ( \xsik{s}{i}{k} ) \sim \Normaldist ( \log (10^7) , 10^{10})
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Benchmarking}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{appendices}
\section{Derivation of marginalization}
\label{appendix:marginalization}
Let
\begin{align}
  y \sim \Normaldist(Xw,\Sigma_1)
\end{align}
with the prior on $w$ given as
\begin{align}
  w \sim \Normaldist(\mu_2, \Sigma_2)
\end{align}
The design matrix $X$ has dimensions $n \times d$.
\begin{align}
  p_{y|w}p_w & =
    \frac{1}{(2 \pi)^{n/2} |\Sigma_1|^{1/2}}
    \frac{1}{(2 \pi)^{d/2} |\Sigma_2|^{1/2}}
    \exp{ \Big(
      -\frac{1}{2}(y - Xw)^T \Sigma_1^{-1} (y-Xw)
      -\frac{1}{2}(w - \mu_2)^T \Sigma_1^{-1} (w-\mu_2) \Big)} \\
    & = \frac{1}{(2 \pi)^{n/2} |\Sigma_1|^{1/2}}
    \frac{1}{(2 \pi)^{d/2} |\Sigma_2|^{1/2}} \\
    & \quad \quad \cdot \exp{\big(
      -\frac{1}{2}(w^T(X^T \Sigma_1^{-1} X + \Sigma_2^{-1})w
      -(y^T \Sigma_1^{-1} X + \mu_2^T \Sigma_2^{-1})w )
    \big)} \\
    & \quad \quad \cdot \exp{\big(
        -\frac{1}{2}(
          - w^T(X^T \Sigma_1^{-1}y + \Sigma_2^{-1} \mu_2)
          + \mu_2^T \Sigma_2^{-1} \mu_2 + y^T \Sigma_1^{-1}y
        )
    \big)}
\end{align}
Using the following definitions
\begin{align}
  \Sigma_3^{-1} & = X^T \Sigma_1^{-1} X + \Sigma_2^{-1} \\
  \mu_3 & = \Sigma_3 (X^T \Sigma_1^{-1}y + \Sigma_2^{-1} \mu_2)
\end{align}
we can simplify the above to
\begin{align}
    p_{y|w}p_w & =
    \frac{1}{(2 \pi)^{n/2} |\Sigma_1|^{1/2}}
    \frac{1}{(2 \pi)^{d/2} |\Sigma_2|^{1/2}}
    \exp{\big(
      -\frac{1}{2} (w^T \Sigma_3^{-1} w - \mu_3^T \Sigma_3^{-1} w - w^T \Sigma_3^{-1} \mu_3)
    \big)} \\
    & \quad \quad \cdot \exp{\big(
      -\frac{1}{2} (\mu_2^T \Sigma_2^{-1} \mu_2 + y^T \Sigma^{-1} y)
    \big)}
\end{align}
Completing the square on the first line exponent
\begin{align}
  p_{y|w}p_w & =
    \frac{1}{(2 \pi)^{n/2} |\Sigma_1|^{1/2}}
    \frac{1}{(2 \pi)^{d/2} |\Sigma_2|^{1/2}}
    \exp{\big(
      -\frac{1}{2} (w - \mu_3)^T \Sigma_3^{-1} (w - \mu_3)
    \big)} \\
    & \quad \quad \cdot \exp{\big(
      -\frac{1}{2}(
        -\mu_3^T \Sigma_3^{-1} \mu_3 + \mu_2^T \Sigma_2^{-1} \mu_2
        + y^T \Sigma_1^{-1} y)
    \big)}
\end{align}
Before we integrate out $w$, we need to multiply and divide by $|\Sigma_3|^{1/2}$ to make this normal with respect to $w$
\begin{align}
  p_{y|w}p_w & =
    \frac{1}{(2 \pi)^{n/2} |\Sigma_1|^{1/2}}
    \frac{|\Sigma_3|^{1/2}}{|\Sigma_2|^{1/2}}
    \frac{1}{(2 \pi)^{d/2} |\Sigma_3|^{1/2}}
    \exp{\big(-\frac{1}{2}(w-\mu_3)^T \Sigma_3^{-1} (w-\mu_3) \big)} \\
    & \quad \quad \cdot \exp{\big(
    -\frac{1}{2}(
      -\mu_3^T \Sigma_3^{-1} \mu_3 + \mu_2^T \Sigma_2^{-1} \mu_2
      + y^T \Sigma_1^{-1} y)
    \big)}
\end{align}
Now we have an exact multivariate Gaussian $\frac{1}{(2 \pi)^{d/2} |\Sigma_3|^{1/2}} \exp{\big( -\frac{1}{2}(w - \mu_3)^T \Sigma_3^{-1} (w - \mu_3) \big)}$. Now if we integrate out $w$ the multivariate normal integral is 1 and
\begin{align}
  \label{eqn:marginalization}
  \int p_{y|w}p_w \text{d}w =
    \frac{1}{(2 \pi)^{n/2} | \Sigma_1 |^{1/2}}
    \frac{| \Sigma_3 |^{1/2}}{| \Sigma_2 |^{1/2}}
    \exp{\big(
      -\frac{1}{2}(
        \mu_2^T \Sigma_2^{-1} \mu_2 + y^T \Sigma_1^{-1} y - \mu_3^T \Sigma_3^{-1} \mu_3
      )
    \big)}
\end{align}

\section{Learning negative binomial dispersion parameters}
\label{appendix:learning negbin dispersion params}
TODO

\section{Gibbs sampling of growth and self-interactions}
\label{appendix:sampling growth and si}
\subsection{Sampling growths}
\label{appendix:sampling growth}
The prior of the growths are defined as
\begin{align}
    \label{appendix eqn: growth prior}
    p(\growthi{i}) \sim \TruncNormaldist{0}{\infty}(\meangrowth, \vargrowth)
\end{align}
Equation (\ref{appendix eqn: growth prior}) can be rewritten as:
\begin{align}
    p(\growthi{i}) = c \Normaldist (\meangrowth , \vargrowth) 
        \mathbf{1} \{ \growthi{i} > 0 \}
\end{align}
where $c$ is the normalizing constant which is independent of $\growthi{i}$. The likelihood of the data conditional on all other parameters except for growth is as follows:
\begin{align}
    \varsik{g}{s}{i}{k} & = 
        \frac{
            \frac
                {\log (\xsik{s}{i}{k+1}) - \log (\xsik{s}{i}{k}) }{\dt}
                - \sii{i} \xsik{s}{i}{k} 
                    - \sum_{\ci{i} \neq \ci{j}}
                        \interactcij{i}{j} 
                        \zinteractcij{i}{j}
                        \xsik{s}{j}{k}
        }
        {
            1 + \sum_{p=1}^P \pertic{p}{i} \zpertic{p}{i} \stepperti{p}
        } \\
    \sigma^2_g (s,i,k) & = \frac{\pv}
        {
            \dt
            \left( 1 + \sum_{p=1}^P \pertic{p}{i} \zpertic{p}{i} \stepperti{p} \right)^2
        }\\
    p(\varsik{g}{s}{i}{k} | \growthi{i}) & \sim  \Normaldist \left(
        \growthi{i}, \sigma^2_g (s,i,k)
    \right)
\end{align}
Using Bayes rule, the posterior of $\growthi{i}$ is as follows:
\begin{align}
    \label{eqn:derivation of truncated normal posterior}
    \growthi{i} | g_i 
        & \propto 
            p(\growthi{i})
            \prod_{s,k} p(\varsik{g}{s}{i}{k} | \growthi{i}) \\
        & \propto
            \exp \left(
                - \frac{(\growthi{i} - \meangrowth)^2}{2 \vargrowth}
            \right) \mathbf{1} \{ \growthi{i} > 0 \}
            \prod_{s,k}
            \exp \left(
                - \frac{(\varsik{g}{s}{i}{k} - \growthi{i})^2}
                    {2 \sigma^2_g (s,i,k)}
        \right) \\
        & \propto \exp \left(
            - \frac{(\growthi{i} - \meangrowth)^2}{2 \vargrowth}
            - \sum_{s,k} \frac
                {(\varsik{g}{s}{i}{k} - \growthi{i})^2}
                {2 \sigma^2_g (s,i,k)}
        \right) \mathbf{1} \{ \growthi{i} > 0 \} \\
        & \propto \exp \Bigg[
            - \sum_{s,k} \frac{1}
                {2 \vargrowth \sigma^2_g (s,i,k) 
                / (\vargrowth + \sigma^2_g (s,i,k))} \\
            & \quad \times \left(
                \growthi{i} - \frac
                    {\sigma^2_g (s,i,k) \meangrowth + \vargrowth \varsik{g}{s}{i}{k}}
                    {\vargrowth + \sigma^2_g (s,i,k)}
            \right)
        \Bigg] \mathbf{1} \{ \growthi{i} > 0 \}
\end{align}
This is the kernel of the normal distribution with the usual mean and variance (as if we had done the derivation for an untruncated prior), but truncated greater than 0: 
\begin{align}
    \mu_{\growthi{i}}^{'} & = \frac{1}
        {\frac{1}{\vargrowth} + 
        \sum_{s,k} \frac{1}{\sigma_g^2 (s,i,k) } } 
        \left(
            \frac{\meangrowth}{\vargrowth} + 
            \sum_{s,k} \frac{\varsik{g}{s}{i}{k}}{\sigma^2_g (s,i,k)}
        \right) \\
    \sigma^{2'}_{\growthi{i}} & = \left(
        \frac{1}{\vargrowth} +
        \sum_{s,k} \frac{1}{\sigma^2_g (s,i,k)}
    \right)^{-1}
\end{align}
All you do is add the indicator function and adjust the normalizing constant.

\subsection{Sampling self-interactions}
Sampling the values of the self-interactions is very similar to sampling the values of the growths as described in appendix \ref{appendix:sampling growth} because the prior is a truncated normal distribution and the likelihood is a normal distribution. The only difference is the truncation ($(-\infty , 0)$ instead of $(0, \infty)$) and how we define $\varsik{g}{s}{i}{k}$ and $\sigma^2_g (s,i,k)$:
\begin{align}
    \varsik{g}{s}{i}{k} & = 
        \frac{
            \frac
                {\log (\xsik{s}{i}{k+1}) - \log (\xsik{s}{i}{k}) }{\dt}
                - \growthi{i} \left(
                    1 + \sum_{p=1}^P \pertic{p}{i} \zpertic{p}{i} \stepperti{p}
                \right)
                    - \sum_{\ci{i} \neq \ci{j}}
                        \interactcij{i}{j} 
                        \zinteractcij{i}{j}
                        \xsik{s}{j}{k}
        }
        {
            \xsik{s}{i}{k}
        } \\
    \sigma^2_g (s,i,k) & = \frac{\pv}
        {
            \dt \xsikexp{s}{i}{k}{2}
        }\\
    p(\varsik{g}{s}{i}{k} | \sii{i}) & \sim  \Normaldist \left(
        \sii{i}, \sigma^2_g (s,i,k)
    \right)
\end{align}
Using the above definitions, we can do an analogous derivation that started at equation (\ref{eqn:derivation of truncated normal posterior}) to get the following posterior:
\begin{align}
    \mu_{\sii{i}}^{'} & = \frac{1}
        {\frac{1}{\varsi} + 
        \sum_{s,k} \frac{1}{\sigma_g^2 (s,i,k) } } 
        \left(
            \frac{\meansi}{\varsi} + 
            \sum_{s,k} \frac{\varsik{g}{s}{i}{k}}{\sigma^2_g (s,i,k)}
        \right) \\
    \sigma^{2'}_{\sii{i}} & = \left(
        \frac{1}{\varsi} +
        \sum_{s,k} \frac{1}{\sigma^2_g (s,i,k)}
    \right)^{-1}
\end{align}

\end{appendices}

\bibliographystyle{plain}
\begin{thebibliography}{}
	\bibitem{cite:BDA}
	A. Gelman, H. S. Stern, J. B. Carlin, D. B. Dunson, A. Vehtari, and D. B. Rubin, \textit{Bayesian Data Analysis Third Edition}. Chapman and Hall/CRC, 2013.

  \bibitem{cite:neal2000}
	R. M. Neal, "Markov chain sampling methods for dirchlet process mixture models," \textit{Journal of computational and graphical models}, 9(2):249-265, 2000.

  \bibitem{cite:ramussen2000}
	C. E. Ramussen, "The infinite gaussian mixture model," \textit{Advances in Information Processing Systems 12}, 2000.

  \bibitem{cite:negbin}
  P.J. McMurdie and S. Holmes, "Waste not, want not: why rarefying microbiome data is inadmissible," \textit{PLOS Computational Biology}, vol. 10, no. 4, p. e1003531, 2014.

  \bibitem{cite:gibsonICML}
  T. E. Gibson and G. K. Gerber, "Robust and scalable models of microbiome dynamcis", \textit{International Conference on Machine Leanring}, pp. 1758-1767, 2018.

  \bibitem{cite:MDSINE}
  B. Vanni, B. Tzen, N. Ling, M. Simmons, T. Tanoue, E. Bogart, L. Deng, V. Yeliseyev, M. L. Delaney, Q. Liu, B. Olle, R. R. Stein, K. Honda, L. Bry,G. K. Gerber, "Mdsine: Microbial dynamical systems inference engine for microbiome time-series analysis," \textit{Genome Biology}, 17(1):121, 2016.

  \bibitem{cite:escobarandwest1995}
  M. D. Escobar and M. West, "Bayesian density estimation and inference using mixtures," \textit{Journal of the American Statistical Association}, vol. 90, no. 430, pp 577-588, 1995.

\end{thebibliography}
\end{document}
