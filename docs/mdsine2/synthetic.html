<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.1" />
<title>mdsine2.synthetic API documentation</title>
<meta name="description" content="Generating synthetic data â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>mdsine2.synthetic</code></h1>
</header>
<section id="section-intro">
<p>Generating synthetic data </p>
<h2 id="references">References</h2>
<p>[1] Robust and Scalable Models of Microbiome Dynamics, TE Gibson, GK Gerber (2018)</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#39;&#39;&#39;Generating synthetic data 

References
----------
[1] Robust and Scalable Models of Microbiome Dynamics, TE Gibson, GK Gerber (2018)
&#39;&#39;&#39;
import numpy as np
import logging
import time
import copy

import random
import numpy.random as npr
import scipy.stats
from sklearn.cluster import AgglomerativeClustering

from . import pylab as pl
from . import model
from . import diversity

class SyntheticData(pl.Saveable):
    &#39;&#39;&#39;This class is used to generate synthetic data using fixed
    or sampled topologies. It also gives us the ability to convert
    the internal representation into a `pylab.base.Study` so
    we can use it in inference.

    We add the parameters for the dynamical system to self.dynamics,
    which we assume to be a Discretized Generalized Lotka-Voltera 
    Dynamics with clustered interactions and perturbations. The 
    dynamical class is specified in the module `model`.

    NOTE: We cannot specify the dynamical class until we have our `asvs` 
    object so we wait to define it then, and it is done automatically 
    once we call the function `set_asvs`

    Parameters
    ----------
    n_days : numeric
        Total length of days
    perturbations_additive : bool
        If True, model the dynamics as additive. Else model it as multiplicative
    &#39;&#39;&#39;
    def __init__(self, n_days, perturbations_additive):
        self.G = pl.graph.Graph(name=&#39;synthetic&#39;) # make local graph
        self.data = []
        self.times = []
        self.dt = None
        self.dynamics = None
        self.n_replicates = 0
        self.perturbations_additive = perturbations_additive

        self.dt = None
        self.n_time_steps = None
        self.n_days = n_days

    @property
    def perturbations(self):
        try:
            return self.dynamics.perturbations
        except:
            raise pl.UndefinedError(&#39;You need to define `dynamics` ({}) first.&#39;.format(
                type(self.dynamics)))

    def get_full_interaction_matrix(self):
        &#39;&#39;&#39;Make the interaction matrix. If `with_interactions` is False,
        you&#39;re effectively only getting the self-interaction terms.
        &#39;&#39;&#39;
        A = self.dynamics.interactions.get_datalevel_value_matrix(set_neg_indicators_to_nan=False)
        for i in range(A.shape[0]):
            A[i,i] = -self.dynamics.self_interactions[i]
        return A

    def set_asvs(self, n_asvs=None, sequences=None, filename=None, asvs=None):
        &#39;&#39;&#39;Sets the ASVset. If you have a list of sequences you want to read in,
        pass in the list of `sequences` and it will create a new ASV for every sequence. 
        If you  just want to specify how many ASVs you want and you dont care about 
        sequences, then specify the number of ASVs with `n_asvs`. If there is an
        ASVSet saved on file, you can load it with the keyword `filename`.

        Defines the dynamics once done.

        Example:
            &gt;&gt;&gt; self.set_asvs(n_asvs=5)
            5 ASVs with random sequences

            &gt;&gt;&gt; seq = [&#39;AAAA&#39;, &#39;TTTT&#39;, &#39;GGGG&#39;, &#39;CCCC&#39;]
            &gt;&gt;&gt; self.set_asvs(sequences=seq)
            3 ASVs with the sequences specified above
            
            &gt;&gt;&gt; self.set_asvs(filename=&#39;pickles/test.pkl&#39;)
            Reads in the ASVSet saved at &#39;pickles/test.pkl&#39;

        Parameters
        ----------
        n_asvs : int, Optional
            How many ASVs you want. This is unnecessary if you specify a list of 
            sequences.
        sequences : array(str), Optional
            The sequence for each ASV. If you do not want any, dont specify anything
            and specify the number of ASVs. If nothing is provided it will create
            a random sequence of sequences for each ASV
        filename : str, Optional
            The filename to lead the ASV
        asvs : pylab.base.ASVSet
            This is an ASVSet object

        Returns
        -------
        pl.ASVSet
            This is the ASVSet that gets created
        &#39;&#39;&#39;
        a = n_asvs is not None
        b = sequences is not None
        c = filename is not None
        d = asvs is not None

        if a + b + c + d != 1:
            raise TypeError(&#39;Only one of `n_asvs` ({}), `sequences` ({}), &#39;\
                &#39;`filename` ({}), or asvs ({})  can be specified&#39;.format(
                type(n_asvs), type(sequences), type(filename), type(asvs)))
        
        if n_asvs is not None:
            if not pl.isint(n_asvs):
                raise TypeError(&#39;`n_asvs` ({}) must be an int&#39;.format(type(n_asvs)))
            self.asvs = pl.ASVSet()
            for i in range(n_asvs):
                name = &#39;ASV_{}&#39;.format(i)
                seq = &#39;&#39;.join(random.choices([&#39;A&#39;,&#39;T&#39;,&#39;G&#39;,&#39;C&#39;,&#39;U&#39;], k=50))
                self.asvs.add_asv(name=name, sequence=seq)
        elif sequences is not None:
            if not pl.isarray(sequences):
                raise TypeError(&#39;`sequences` ({}) must be an array&#39;.format(type(sequences)))
            if not np.all(pl.itercheck(sequences, pl.isstr)):
                raise TypeError(&#39;All elements in `sequences` must be strs: {}&#39;.format(
                    pl.itercheck(sequences, pl.isstr)))
            for i, seq in enumerate(sequences):
                name = &#39;ASV_{}&#39;.format(i)
                self.asvs.add_asv(name=name, sequence=seq)
        elif filename is not None:
            if not pl.isstr(filename):
                raise TypeError(&#39;`filename` ({}) must be a str&#39;.format(type(filename)))
            self.asvs = pl.ASVSet.load(filename)
        else:
            self.asvs = asvs

        self.dynamics = model.gLVDynamicsSingleClustering(asvs=self.asvs, 
            perturbations_additive=self.perturbations_additive)
        
    def set_cluster_assignments(self, clusters=None, n_clusters=None, evenness=None):
        &#39;&#39;&#39;Create clusters for the interactions and perturbations. If you have the cluster
        assignments already, set them with `clusters`. else we can randomly generate them 
        with the parameters `n_clusters` and `evenness`.

        Parameters
        ----------
        clusters : list(list(int))
            These are the cluster assignments of the ASVs
        n_clusters : int
            Number of clusters to create
        evenness : str, 1 or 2-dim array
            How to initialize the clusters. This can be generated automatically or by 
            reading in a similarity matrix.
            If it is a str:
                &#39;even&#39;: Have each cluster have as close to even number of clusters as possible
                &#39;heavy-tail&#39;: TODO : NOT IMPLEMENTED
                &#39;sequence&#39; : TODO : NOT IMPLEMENTED (given the sequences, make an adjacency matrix)
            If it is an array:
                If 1-dim
                    This is how you assign the clusters by index to each cluster. This is the 
                    initialization format for pylab.cluster.Clustering
                If it is 2-dim
                    This is a 2 dimensional DISTANCE matrix. It will build the cluster 
                    assignments given this distance matrix

        Returns
        -------
        pl.cluster.Clustering
            This is the clustering object that gets created

        See also
        --------
        pylab.cluster.Clustering.__init__
        &#39;&#39;&#39;
        if self.asvs is None:
            raise ValueError(&#39;Must specify the ASVSet before by calling `self.set_asvs`&#39;)

        if clusters is None:
            if not pl.isint(n_clusters):
                raise TypeError(&#39;`n_clusters` ({}) must be an int&#39;.format(n_clusters))

            clusters = []
            start = 0
            if pl.isstr(evenness):
                if evenness == &#39;even&#39;:
                    size = int(len(self.asvs)/n_clusters)
                    for _ in range(n_clusters-1):
                        clusters.append(np.arange(start,start+size, dtype=int))
                        start += size
                    clusters.append(np.arange(start, len(self.asvs), dtype=int))
                elif evenness == &#39;sequence&#39;:
                    logging.info(&#39;Making affinity matrix from sequences&#39;)
                    evenness = np.diag(np.ones(len(self.asvs), dtype=float))

                    for i in range(len(self.asvs)):
                        for j in range(len(self.asvs)):
                            if j &lt;= i:
                                continue
                            # Subtract because we want to make a similarity matrix
                            dist = 1-diversity.beta.hamming(
                                list(self.asvs[i].sequence), list(self.asvs[j].sequence))
                            evenness[i,j] = dist
                            evenness[j,i] = dist

                    # print(evenness)

                elif evenness == &#39;heavy-tail&#39;:
                    raise NotImplementedError(&#39;`heavy-tail` not implemented yet&#39;)
                else:
                    raise ValueError(&#39;cluster evenness ({}) not recognized&#39;.format(evenness))
            if pl.isarray(evenness):
                evenness = np.asarray(evenness)
                if evenness.ndim == 1:
                    clusters = evenness.tolist()
                elif evenness.ndim == 2:
                    if evenness.shape[0] != evenness.shape[1]:
                        raise ValueError(&#39;Must be a square matrix&#39;)
                    if evenness.shape[0] != len(self.asvs):
                        raise ValueError(&#39;Length of the side ({}) must be the same as the number of ASVs ({})&#39;.format(
                            evenness.shape[0], len(self.asvs)))
                    
                    c = AgglomerativeClustering(
                        n_clusters=n_clusters,
                        affinity=&#39;precomputed&#39;,
                        linkage=&#39;average&#39;)
                    assignments = c.fit_predict(evenness)
                    clusters = {}
                    for oidx,cidx in enumerate(assignments):
                        if cidx not in clusters:
                            clusters[cidx] = []
                        clusters[cidx].append(oidx)
                    clusters = [val for val in clusters.values()]
                else:
                    raise ValueError(&#39;`evenness` ({}) must be a 1 or 2-dimensional array&#39;.format(
                        evenness.ndim))
            else:
                raise TypeError(&#39;`evenness` ({}) must be either a string or an array&#39;.format(
                    type(evenness)))

        # Initialize clustering object
        logging.info(&#39;cluster assignments: {}&#39;.format(clusters))
        self.dynamics.clustering = pl.Clustering(clusters = clusters, items=self.asvs, G=self.G)
        return self

    def shuffle_cluster_assignments(self, p):
        &#39;&#39;&#39;Shuffle the cluster assignments that were specified in `set_cluster_assignments`.

        `p` indicates what proportion of the ASVs to be reassigned. Example: `p=.1` means
        that you want to shuffle 10% of the ASVs.

        NOTE: THIS SHOULD BE CALLED BEFORE YOU CALL THE FUNCTION `sample_dynamics`.

        Parameters
        ----------
        p : float
            Proportion of the ASVs to be shuffled
        &#39;&#39;&#39;
        if self.dynamics.clustering is None:
            raise ValueError(&#39;Must specfiy the clusters before you call this function&#39;)
        if not pl.isnumeric(p):
            raise TypeError(&#39;`p` ({}) should be a numeric&#39;.format(type(p)))
        if p &lt; 0 or p &gt; 1:
            raise ValueError(&#39;`p` ({}) should be 0 =&lt; p =&lt; 1&#39;.format(p))

        n = int(p*len(self.asvs))
        oidxs = np.random.randint(len(self.asvs), size=n)

        logging.info(&#39;ASV indices to shuffle: {}&#39;.format(oidxs))

        for oidx in oidxs:
            curr_cid = self.dynamics.clustering.idx2cid[oidx]
            assigned_cid = curr_cid
            while assigned_cid == curr_cid:
                assigned_cid = random.choice(self.dynamics.clustering.order)

            self.dynamics.clustering.move_item(idx=oidx, cid=assigned_cid)

        logging.info(&#39;new cluster assignments: {}&#39;.format(self.dynamics.clustering.toarray()))

    def sample_single_perturbation(self, start, end, prob_pos, prob_affect, prob_strength, 
        mean_strength, std_strength):
        &#39;&#39;&#39;Sample a perturbation to add to the system.

        If there are no clusters that are selected, we sample again until we get at least 1.

        Defaults for strength parameters
        --------------------------------
        `prob_strength = [0.2, 0.4, 0.4]`
        `mean_strength   = [0.5, 1.0, 2.0]`
        `std_strength  = 0.1`

        Parameters
        ----------
        start : float
            - Time to start the perturbation.
        end : float
            - Time to end the perturbation
        pob_pos : float, [0,1]
            - This is the probability that the perturbation is going to be positive
            - Sampled from a Bernoulli distribution
        mean_strength : array
            These are the means of the magnitudes of the perturbations to sample around
        prob_strength : array
            These are the probabilities to sample a mean magnitude of the perturbation
        std_strength : float
            This is the standard deviation to sample the magnitude of the perturbation
        prob_affect : float ([0,1]), str
            - This is the probability it will affect a cluster (positive indicator)
            - Sampled from a Bernoulli distribution
            - If str, then only that number of clusters is set.
              Example:
                &#39;1&#39; means only one cluster is set

        Returns
        -------
        pylab.contrib.ClusterPerturbation
            This is the cluster perturbation that was sampled
        &#39;&#39;&#39;
        if self.dynamics.clustering is None:
            raise TypeError(&#39;Clustering module is None, this is likely because you did &#39; \
                &#39;not set the dynamics&#39;)

        # Check variables
        if not pl.isnumeric(start):
            raise TypeError(&#39;`start` ({}) must be a numeric&#39;.format(type(start)))
        if not pl.isnumeric(end):
            raise TypeError(&#39;`end` ({}) must be a numeric&#39;.format(type(end)))
        if end &lt;= start:
            raise ValueError(&#39;`start` ({}) must be strictly smaller than `end` ({})&#39;.format(
                start,end))
        if not pl.isfloat(prob_pos):
            raise TypeError(&#39;`prob_pos` ({}) must be a numeric&#39;.format(type(prob_pos)))
        elif prob_pos &lt; 0 or prob_pos &gt; 1:
            raise ValueError(&#39;`prob_pos` ({}) must be [0,1]&#39;.format(prob_pos))
        if pl.isstr(prob_affect):
            set_num = True
            try:
                pa = int(prob_affect)
            except:
                logging.critical(&#39;Cannot cast `prob_affect` ({}) as an int&#39;.format(prob_affect))
                raise
            if pa &lt; 0:
                raise ValueError(&#39;`prob_affect` ({}) must be &gt; 0&#39;.format(prob_affect))
            if pa &gt; len(self.dynamics.clustering.clusters):
                raise ValueError(&#39;`prob_affect` ({}) must be less than n_clusters&#39;.format(prob_affect))
        else:
            set_num = False
            pa = None
            if not pl.isfloat(prob_affect):
                raise TypeError(&#39;`prob_affect` ({}) must be a numeric&#39;.format(type(prob_affect)))
            elif prob_affect &lt; 0 or prob_affect &gt; 1:
                raise ValueError(&#39;`prob_affect` ({}) must be [0,1]&#39;.format(prob_affect))
        # check the strengths
        if not pl.isarray(prob_strength):
            raise TypeError(&#39;`prob_strength` ({}) be an array&#39;.format(type(prob_strength)))
        for ele in prob_strength:
            if not pl.isfloat(ele):
                raise TypeError(&#39;`every element in `prob_strength` ({}) must be a float&#39;.format( 
                    type(ele)))
            if ele &lt;= 0:
                raise ValueError(&#39;`every probability in `prob_strength` ({}) must be &gt; 0&#39;.format(ele))
        if not pl.isarray(mean_strength):
            raise TypeError(&#39;`mean_strength` ({}) must be an array&#39;.format(type(mean_strength)))
        for ele in mean_strength:
            if not pl.isnumeric(ele):
                raise TypeError(&#39;Every element in `mean_strength` ({}) must be a numeric&#39;.format(ele))
            if ele &lt; 0:
                raise ValueError(&#39;Every element in `mean_strength` ({}) must be positive.&#39; \
                    &#39; If you want a perturbation to have a negative magnitude then set the &#39; \
                    &#39;`prob_pos` parameter to a very low nuber&#39;.format(ele))
        if len(mean_strength) != len(prob_strength):
            raise ValueError(&#39;`mean_strength` ({}) and `prob_strength` ({}) must have the same number&#39; \
                &#39; of elements&#39;.format(len(mean_strength), len(prob_strength)))
        if not pl.isnumeric(std_strength):
            raise TypeError(&#39;`std_strength` ({}) must be a numeric&#39;.format(type(std_strength)))
        if std_strength &lt;= 0:
            raise ValueError(&#39;`std_stregnth` ({}) must be &gt; 0&#39;.format(std_strength))

        # Set indicator at the ASV level
        if set_num:
            # Pick the number of clusters on
            order = copy.deepcopy(self.dynamics.clustering.order)
            order = list(order)
            indicator = np.zeros(len(order), dtype=bool)

            # pick the cids to set to true
            while pa &gt; 0:
                # pick a cluster
                idx = npr.randint(0, len(order))
                indicator[self.dynamics.clustering.cid2cidx[order[idx]]] = True
                order.pop(idx)
                pa -= 1
        else:
            i = 0
            while True:
                indicator = np.zeros(len(order), dtype=bool)
                for cidx in range(len(self.dynamics.clustering.clusters)):
                    indicator[cidx] = bool(pl.random.bernoulli.sample(prob_affect))
                if np.sum(indicator) &gt; 0:
                    break
                if i == 1000:
                    raise ValueError(&#39;Assigning cluster ids failed 1000 times. set `prob_affect` {}&#39; \
                        &#39; to a larger number&#39;.format(prob_affect))
                i += 1

        magnitude = np.zeros(len(self.dynamics.clustering), dtype=float)
        for i,ind in enumerate(indicator):
            if ind:
                # sample the magnitude
                sign = pl.random.bernoulli.sample(prob_pos) * 2 - 1
                mean = np.random.choice(mean_strength, p=prob_strength)
                magnitude[i] = pl.random.normal.sample(mean=mean*sign, 
                    std=std_strength)

        a = pl.contrib.ClusterPerturbation(start=start, end=end, magnitude=magnitude,
            clustering=self.dynamics.clustering, indicator=indicator, G=self.G)
        
        logging.info(&#39;Perturbation:\n\tstart,end ({},{})\n\t&#39; \
            &#39;magnitude {}\n\tindicator {}&#39;.format(start,end,a.cluster_array(),
            indicator))

        if self.dynamics.perturbations is None:
            self.dynamics.perturbations = [a]
        else:
            self.dynamics.perturbations.append(a)
        return a

    def set_single_perturbation(self, start, end, magnitude, indicator):
        &#39;&#39;&#39;Sets a single perturbation - no sampling. This assumes that you have
        initialized the system - (`clustering` is not None)

        Parameters
        ----------
        start, end (float)
            - Time to start/end the perturbation
        magnitude (float)
            - This is the strength of the perturbation
        indicator (int, np.ndarray)
            - If it is an int, this is the cluster id that it is positive for
            - If it is an array, it must either be the length of the
              number of clusters, and must be either a boolean or 1,0s
        &#39;&#39;&#39;
        if self.dynamics.clustering is None:
            raise ValueError(&#39;Must sample the system before you call this function&#39;)
        a = pl.contrib.ClusterPerturbation(start=start, end=end, magnitude=magnitude,
            indicator=indicator, clustering=self.dynamics.clustering, G=self.G)
        if self.dynamics.perturbations is None:
            self.dynamics.perturbations = [a]
        else:
            self.dynamics.perturbations.append(a)
        return a

    def icml_topology(self, n_asvs=13, max_abundance=None):
        &#39;&#39;&#39;Recreate the dynamical system used in the ICML paper [1]. If you use the 
        default parameters you will get the same interaction matrix that was used in [1].

        We rescale the self-interactions and the interactions (and potentially growth) by 150
        so that the time-scales of the trajectories happen over days instead of minutes

        Parameters
        ----------
        n_asvs : int
            These are how many ASVs to include in the system. We will always have 3 clusters and
            the proportion of ASVs in each cluster is as follows:
                cluster 1 - 5/13
                cluster 2 - 6/13
                cluster 3 - 2/13
            We assign each ASV to each cluster the best we can, any extra ASVs we put into cluster 3
        max_abundance : numeric, None
            This is the abundance to set the maximum. All of the other 
            parameters change proportionally. If `None` then we assume no change.
        &#39;&#39;&#39;
        if not pl.isint(n_asvs):
            raise TypeError(&#39;`n_asvs` ({}) must be an int&#39;.format(type(n_asvs)))
        if n_asvs &lt; 3:
            raise ValueError(&#39;`n_asvs` ({}) must be &gt;= 3&#39;.format(n_asvs))
        if max_abundance is not None:
            if not pl.isnumeric(max_abundance):
                raise TypeError(&#39;`max_abundance` ({}) must be a numeric&#39;.format(type(max_abundance)))
            if max_abundance &lt;= 0:
                raise ValueError(&#39;`max_abundance` ({}) must be &gt; 0&#39;.format(max_abundance))
        
        # Generate ASVs
        self.set_asvs(n_asvs)

        # Make cluster assignments with the approximate proportions
        c0size = int(5*n_asvs/13)
        c1size = int(6*n_asvs/13)
        c2size = int(n_asvs - c0size - c1size)

        frac = 150*n_asvs/13

        clusters = [
            np.arange(0, c0size, dtype=int).tolist(), 
            np.arange(c0size, c0size+c1size, dtype=int).tolist(),
            np.arange(c0size+c1size, c0size+c1size+c2size, dtype=int).tolist()]

        self.dynamics.clustering = pl.Clustering(clusters=clusters, items=self.asvs, G=self.G)
        self.dynamics.interactions = pl.Interactions(clustering=self.dynamics.clustering, 
            use_indicators=True, G=self.G)

        c0 = self.dynamics.clustering.order[0]
        c1 = self.dynamics.clustering.order[1]
        c2 = self.dynamics.clustering.order[2]
        for interaction in self.dynamics.interactions:
            if interaction.target_cid == c0 and interaction.source_cid == c1:
                interaction.value = 3/frac
                interaction.indicator = True
            elif interaction.target_cid == c0 and interaction.source_cid == c2:
                interaction.value = -1/frac
                interaction.indicator = True
            elif interaction.target_cid == c2 and interaction.source_cid == c0:
                interaction.value = 2/frac
                interaction.indicator = True
            elif interaction.target_cid == c2 and interaction.source_cid == c1:
                interaction.value = -4/frac
                interaction.indicator = True
            else:
                interaction.value = 0
                interaction.indicator = False
        
        self.dynamics.growth = pl.random.uniform.sample(low=18, high=22, size=n_asvs)/150
        # self.dynamics.growth = pl.random.uniform.sample(.1, 0.6, size=n_asvs)
        # self.dynamics.growth = pl.random.uniform.sample(low=.1 + 0.2, high=math.log(10) - 0.2, size=n_asvs)
        # self.dynamics.growth = pl.random.uniform.sample(low=0.12, high=2, size=n_asvs)
        self.dynamics.self_interactions = np.ones(n_asvs, dtype=float)*(5)/150
        # self.dynamics.self_interactions = pl.random.uniform.sample(-.05/150, -50/150, size=n_asvs)

        if max_abundance is not None:
            # self.dynamics.growth = pl.random.uniform.sample(low=.1, high=math.log(10), size=n_asvs)
            # self.dynamics.growth = pl.random.uniform.sample(low=.21, high=.5, size=n_asvs)/2
            # rescale the abundance such that hte max the max is ~max_abundance
            frac = 25/max_abundance
            self.dynamics.self_interactions *= frac
            for interaction in self.dynamics.interactions:
                if interaction.indicator:
                    interaction.value *= frac
    
    def icml_topology_real(self, n_asvs=13, max_abundance=None, scale_interaction=None):
        &#39;&#39;&#39;Recreate the dynamical system used in the ICML paper [1]. If you use the 
        default parameters you will get the same interaction matrix that was used in [1].

        We rescale the self-interactions and the interactions (and potentially growth) by 150
        so that the time-scales of the trajectories happen over days instead of minutes

        Parameters
        ----------
        n_asvs : int
            These are how many ASVs to include in the system. We will always have 3 clusters and
            the proportion of ASVs in each cluster is as follows:
                cluster 1 - 5/13
                cluster 2 - 6/13
                cluster 3 - 2/13
            We assign each ASV to each cluster the best we can, any extra ASVs we put into cluster 3
        max_abundance : numeric, None
            This is the abundance to set the maximum. All of the other 
            parameters change proportionally. If `None` then we assume no change.
        &#39;&#39;&#39;
        if not pl.isint(n_asvs):
            raise TypeError(&#39;`n_asvs` ({}) must be an int&#39;.format(type(n_asvs)))
        if n_asvs &lt; 3:
            raise ValueError(&#39;`n_asvs` ({}) must be &gt;= 3&#39;.format(n_asvs))
        if max_abundance is not None:
            if not pl.isnumeric(max_abundance):
                raise TypeError(&#39;`max_abundance` ({}) must be a numeric&#39;.format(type(max_abundance)))
            if max_abundance &lt;= 0:
                raise ValueError(&#39;`max_abundance` ({}) must be &gt; 0&#39;.format(max_abundance))
        if scale_interaction is None:
            scale_interaction = 1
        # Generate ASVs
        self.set_asvs(n_asvs)

        # Make cluster assignments with the approximate proportions
        c0size = int(5*n_asvs/13)
        c1size = int(6*n_asvs/13)
        c2size = int(n_asvs - c0size - c1size)

        frac = 150*n_asvs/13

        clusters = [
            np.arange(0, c0size, dtype=int).tolist(), 
            np.arange(c0size, c0size+c1size, dtype=int).tolist(),
            np.arange(c0size+c1size, c0size+c1size+c2size, dtype=int).tolist()]

        self.dynamics.clustering = pl.Clustering(clusters=clusters, items=self.asvs, G=self.G)
        self.dynamics.interactions = pl.Interactions(clustering=self.dynamics.clustering, 
            use_indicators=True, G=self.G)

        c0 = self.dynamics.clustering.order[0]
        c1 = self.dynamics.clustering.order[1]
        c2 = self.dynamics.clustering.order[2]
        for interaction in self.dynamics.interactions:
            if interaction.target_cid == c0 and interaction.source_cid == c1:
                interaction.value = scale_interaction*3/frac
                interaction.indicator = True
            elif interaction.target_cid == c0 and interaction.source_cid == c2:
                interaction.value = scale_interaction*(-1)/frac
                interaction.indicator = True
            elif interaction.target_cid == c2 and interaction.source_cid == c0:
                interaction.value = scale_interaction*2/frac
                interaction.indicator = True
            elif interaction.target_cid == c2 and interaction.source_cid == c1:
                interaction.value = scale_interaction*(-4)/frac
                interaction.indicator = True
            else:
                interaction.value = 0
                interaction.indicator = False
        
        # self.dynamics.growth = pl.random.uniform.sample(low=18, high=22, size=n_asvs)/150
        self.dynamics.growth = pl.random.uniform.sample(0.5, 1.5, size=n_asvs)
        # self.dynamics.growth = pl.random.uniform.sample(low=.1 + 0.2, high=math.log(10) - 0.2, size=n_asvs)
        self.dynamics.self_interactions = np.ones(n_asvs, dtype=float)*(5)/150
        # self.dynamics.self_interactions = pl.random.uniform.sample(-.05/150, -50/150, size=n_asvs)

        if max_abundance is not None:
            # self.dynamics.growth = pl.random.uniform.sample(low=.1, high=math.log(10), size=n_asvs)
            # self.dynamics.growth = pl.random.uniform.sample(low=.21, high=.5, size=n_asvs)/2
            # rescale the abundance such that hte max the max is ~max_abundance
            frac = 25/max_abundance
            self.dynamics.self_interactions *= frac
            for interaction in self.dynamics.interactions:
                if interaction.indicator:
                    interaction.value *= frac

    def icml_perturbations(self, starts, ends):
        &#39;&#39;&#39;Set the perturbations. Made to be informative. The `starts` and
        `end` are starts and ends of 3 perturbations
        &#39;&#39;&#39;
        if self.dynamics.clustering is None:
            raise TypeError(&#39;Clustering module is None, this is likely because you did &#39; \
                &#39;not set the dynamics&#39;)

        if not pl.isarray(starts):
            raise TypeError(&#39;`starts` ({}) must be an array&#39;.format(type(starts)))
        if len(starts) != 3:
            raise ValueError(&#39;`starts` ({}) must be 3 elements long&#39;.format(len(starts)))
        for ele in starts:
            if not pl.isnumeric(ele):
                raise TypeError(&#39;Each element in `starts` ({}) must be an array ({})&#39;.format(
                    ele, starts))
        if not pl.isarray(ends):
            raise TypeError(&#39;`ends` ({}) must be an array&#39;.format(type(ends)))
        if len(ends) != 3:
            raise ValueError(&#39;`ends` ({}) must be 3 elements long&#39;.format(len(ends)))
        for ele in ends:
            if not pl.isnumeric(ele):
                raise TypeError(&#39;Each element in `ends` ({}) must be an array ({})&#39;.format(
                    ele, ends))
        for idx in range(len(starts)):
            start = starts[idx]
            end = ends[idx]

            if end &lt;= start:
                raise ValueError(&#39;`start` ({}) must be smaller than `end` ({})&#39;.format(
                    start,end))

        # Set perturbations
        c0 = self.dynamics.clustering.order[0]
        c1 = self.dynamics.clustering.order[1]
        c2 = self.dynamics.clustering.order[2]

        # Set perturbation 0
        if self.perturbations_additive:
            magnitude0 = np.asarray([
                    # pl.random.normal.sample(mean=-1, std=0.1),
                    0, pl.random.normal.sample(mean=0.5, std=0.1), 0])
        else:
            magnitude0 = np.asarray([
                    # pl.random.normal.sample(mean=-1, std=0.1),
                    0, pl.random.normal.sample(mean=1, std=0.1), 0])
        indicator0 = np.array([False, True, False], dtype=bool)
        p0 = pl.contrib.ClusterPerturbation(start=starts[0], end=ends[0], 
            magnitude=magnitude0, indicator=indicator0, G=self.G, 
            clustering=self.dynamics.clustering)

        # Set perturbation 1
        if self.perturbations_additive:
            magnitude1 = np.asarray([
                    pl.random.normal.sample(mean=1, std=0.1), 0,
                    pl.random.normal.sample(mean=-2, std=0.1)])
        else:
            magnitude1 = np.asarray([
                    pl.random.normal.sample(mean=1, std=0.1), 0,
                    pl.random.normal.sample(mean=-2, std=0.1)])
        indicator1 = np.array([True, False, True], dtype=bool)
        p1 = pl.contrib.ClusterPerturbation(start=starts[1], end=ends[1], 
            magnitude=magnitude1, indicator=indicator1, G=self.G, 
            clustering=self.dynamics.clustering)

        # Set perturbation 2
        if self.perturbations_additive:
            magnitude2 = np.asarray([
                    0, pl.random.normal.sample(mean=-0.5, std=0.1),
                    pl.random.normal.sample(mean=1, std=0.1)])
        else:
            magnitude2 = np.asarray([
                    0, pl.random.normal.sample(mean=-0.5, std=0.1),
                    pl.random.normal.sample(mean=1, std=0.1)])
        indicator2 = np.array([False, True, True], dtype=bool)
        p2 = pl.contrib.ClusterPerturbation(start=starts[2], end=ends[2], 
            magnitude=magnitude2, indicator=indicator2, G=self.G, 
            clustering=self.dynamics.clustering)

        self.dynamics.perturbations = [p0,p1,p2]

    def set_timepoints(self, times):
        &#39;&#39;&#39;Times to set the timepoints

        Parameters
        ----------
        times : np.ndarray
        &#39;&#39;&#39;
        if not pl.isarray(times):
            raise TypeError(&#39;`times` ({}) must be an array&#39;.format(times))
        times = np.sort(np.array(times))
        self.master_times = times
        return self

    def set_times_without_timeseries(self, N, D=&#39;auto&#39;, initial_growth=4, pretransition=1, 
        posttransition=2, transition_density=2, uniform_sampling=False):
        &#39;&#39;&#39;Set the time points for the replicates. 
        
        It is highly recommended that you call this function to set the timepoints
        of the synthetic trajectories instead of directly calling `set_timepoints_without_timeseries` 
        or setting them yourself.

        Types of time setting
        ---------------------
        There are three ways we can set the spacing of the timepoints: (1) We can
        set them with a uniform spacing by setting the parameter `uniform_spacing=True`.
        (2) We can space the timepoints non-uniformly using the algorithm 
        `synthetic.set_timepoints_without_timeseries`. (3) We can manually set the times with the parameter 
        `N` if `N` is an array. In terms of precidence of the parameters:
            (1) If `N` is an array, then we set them according to times
            (2) If `uniform_sampling = True`, then we ignore the parameters for the
                non-uniform
            (3) If `uniform_sampling = False`, then we do non-uniform sampling
 
        Non-uniform sampling
        --------------------
        We set the times according to the denisty intervals sepcified
        in D. If D is auto, we set the density to be `transition_density` 
        more intense than regular time points in the following scenarios:
            - For `initial_growth` days from the start of the trajectory
            - For `pretransition` days before a transition from off- and
              on- a perturbation (starting or ending of a perturbation)
            - For `posttransition` days after a transition from off- and
              on- a perturbation (starting or ending of a perturbation)

        Parameters
        ----------
        N : int, array
            If an int, it represents how many timepoints to allocate. Look at 
            `set_timepoints_without_timeseries` for more information.
            If an array, these are the times to set.
        D : str, 3-tuple, list(3-tuple)
            If this is a string, then we set it according to the densities
            specified above. Otherwise, these are a list of intervals that
            we want specific densities for. If all of the densities are not
            adjacent then we add in intervals in between with a density of 1.
            None of the densities listed in D can overlap. For options on 
        initial_growth : numeric, None
            How many days to double sample during the first `initial_growth` days.
            If None then we do not set the initial growth
        pretransition, posttransition : numeric
            How many days before and after, respectively, of a perturbation 
            transition to have double density. If None then we do not set it.
        transition_density : numeric
            How dense to make the higher densities (initial growth and transition
            times). Must be &gt;= 1

        See also
        --------
        synthetic.set_timepoints_without_timeseries
        &#39;&#39;&#39;
        if pl.isarray(N):
            # Set times manually
            N = np.sort(np.unique(N))
            if np.any(N &lt; 0):
                raise ValueError(&#39;Every value in `times` ({}) must be positive or 0&#39;.format(N))
            if np.any(N &gt; self.n_days):
                raise ValueError(&#39;Values in `N` ({}) out of range&#39;.format(N))
            self.master_times = N
            return
            
        if not pl.isbool(uniform_sampling):
            raise TypeError(&#39;`uniform_sampling` ({}) must be a bool&#39;.format(type(uniform_sampling)))
        
        if uniform_sampling:
            if not pl.isint(N):
                raise TypeError(&#39;`N` ({}) must be an int&#39;.format(type(N)))
            if N &lt;= 0:
                raise ValueError(&#39;`N` ({}) must be &gt; 0&#39;.format(N))
            ts = np.arange(0,self.n_days, step=self.n_days/N)
            for i in range(len(ts)):
                ts[i] = round(ts[i], 2)
            self.master_times = ts
            return

        # Else do non-uniform sampling
        if D == &#39;auto&#39;:
            if not pl.isnumeric(transition_density):
                raise TypeError(&#39;`transition_density` ({}) must be a numeric&#39;.format(
                    type(transition_density)))
            if transition_density &lt; 1:
                raise TypeError(&#39;`transition_density` ({}) must be &gt;= 1&#39;.format(
                    transition_density))
            if initial_growth is not None:
                if not pl.isnumeric(initial_growth):
                    raise TypeError(&#39;`initial_growth` ({}) should be a numeric&#39;.format( 
                        type(initial_growth)))
                if initial_growth &lt; 0:
                    raise ValueError(&#39;`initial_growth` ({}) must be positive&#39;.format(
                        initial_growth))
            if pretransition is not None:
                if not pl.isnumeric(pretransition):
                    raise TypeError(&#39;`pretransition` ({}) should be a numeric&#39;.format( 
                        type(pretransition)))
                if pretransition &lt; 0:
                    raise ValueError(&#39;`pretransition` ({}) must be positive&#39;.format(
                        pretransition))
            if posttransition is not None:
                if not pl.isnumeric(posttransition):
                    raise TypeError(&#39;`posttransition` ({}) should be a numeric&#39;.format( 
                        type(posttransition)))
                if posttransition &lt; 0:
                    raise ValueError(&#39;`posttransition` ({}) must be positive&#39;.format(
                        posttransition))

            # Sort perturbations if there are
            if self.dynamics.perturbations is not None:
                perts = []
                pert_starts = []
                for perturbation in self.dynamics.perturbations:
                    perts.append((perturbation.start, perturbation.end))
                    pert_starts.append(perturbation.start)
                
                idxs = np.argsort(pert_starts)
                temp = []
                for idx in idxs:
                    temp.append(perts[idx])
                perts = temp

                # fail if the start or end of a perturbation is greater than n_days
                for s,e in perts:
                    if s &gt;= self.n_days or e &gt; self.n_days:
                        raise ValueError(&#39;Perturbation start and end (`{}`,`{}`) &#39; \
                            &#39; is out of range for the number of days `{}`&#39;.format( 
                                s,e,self.n_days))
            else:
                perts = None

            # Add the densities in order
            D = []
            
            # Set initial growth
            l = np.min([initial_growth, self.n_days])
            D.append((0, l, transition_density))

            # Set for each perturbation:
            if perts is not None:
                for s, e in perts:
                    D.append((s-pretransition, s+posttransition, transition_density))
                    l = np.min([e+posttransition,self.n_days])
                    D.append((e-pretransition, l, transition_density))
        else:
            # check D
            if pl.istuple(D):
                D = [D]
            if not pl.isarray(D):
                raise TypeError(&#39;`D` ({}) must be an array&#39;.format(type(D)))
            for ele in D:
                if not pl.istuple(ele):
                    raise ValueError(&#39;Each element in D ({}) must be a tuple&#39;.format(
                        type(ele)))
                if len(ele) != 3:
                    raise ValueError(&#39;Each element in D must have 3 elements ({})&#39; \
                        &#39;&#39;.format(len(ele)))
                
                s,e,d = ele
                if not np.all(pl.itercheck([s,e,d], pl.isnumeric)):
                    raise TypeError(&#39;All values in ({},{},{}) must be numerics&#39;.format( 
                        type(s), type(e), type(d)))
                if s &lt; 0 or s &gt;= self.n_days or s &gt;= e:
                    raise ValueError(&#39;`start` ({}) out of range&#39;.format(s))
                if e &gt; self.n_days:
                    raise ValueError(&#39;`end` ({}) out of range&#39;.format(e))
                if d &lt; 1:
                    raise ValueError(&#39;`density` ({}) must be &gt;= 1&#39;.format(d))
            
            # Order by the start
            starts = [s for (s,e,d) in D]
            idxs = np.argsort(starts)
            temp = []
            for idx in idxs:
                temp.append(D[idx])
            D = temp

            # Check if any of the intervals overlap
            # If they overlap and they have different densities then 
            # throw an error
            for i in range(len(D)-1):
                si, ei, di = D[i]
                sj, ej, dj = D[i+1]

                if si == sj:
                    raise ValueError(&#39;Two intervals ({} and {}) cannot have the &#39; \
                        &#39;same start point&#39;.format(D[i], D[i+1]))
                if ei &gt; sj:
                    if di != dj:
                        raise ValueError(&#39;Intervals {} and {} overlap and have &#39; \
                            &#39;different densities&#39;.format(D[i], D[i+1]))

        # Merge any overlapping time periods. If the time periods are not 
        # overlapping then add in extra intervals with the background density
        # 1
        D_new = [D[0]]
        i = 1
        while True:
            if i == len(D):
                break

            # Compare the last element in D_new to the ith element of D
            si, ei, di = D_new[-1]
            sj, ej, dj = D[i]

            if ei &gt; sj and ei &lt; ej:
                # they overlap, merge
                if di != dj:
                    raise ValueError(&#39;Something went wrong. D_new: {}, D: {}, &#39; \
                        &#39;i: {}&#39;.format(D_new, D, i))
                D[-1] = (si, ej, di)
            else:
                if ei &lt; sj:
                    # These do not overlap, add an intermediate
                    D_new.append((ei, sj, 1))
                elif ei != sj:
                    raise ValueError(&#39;Something went wrong. D_new: {}, D: {}, &#39; \
                        &#39;i: {}&#39;.format(D_new, D, i))
                D_new.append(D[i])
            i += 1
        D = D_new

        # check if the last timepoint goes up to the last time
        s,e,d = D[-1]
        if e &lt; self.n_days:
            D.append((e,self.n_days,1))
        if e &gt; self.n_days:
            raise ValueError(&#39;Last interval {} ends after the set number of days {}&#39; \
                &#39;&#39;.format(D[-1], self.n_days))

        # Set the days that the perturbations start and end as essential
        essential_timepoints = None
        if self.dynamics.perturbations is not None:
            essential_timepoints = []
            for perturbation in self.dynamics.perturbations:
                essential_timepoints += [perturbation.start, perturbation.end]

        # Set the timepoints for each replicate
        self.master_times = set_timepoints_without_timeseries(N=N, T_start=0, T_end=self.n_days, D=D,
            move_timepoints_if_fail=True, essential_timepoints=essential_timepoints)

    def generate_trajectories(self, dt, init_dist=None, processvar=None):
        &#39;&#39;&#39;Generate gLV dynamics given the dynamics sampled or set

        Parameters
        ----------
        init_dist : pylab.variables.RandomVariable
            This is the distribution that we are sampling the initial 
            condition from
        n_replicates : int
            How many replicates to make
        dt : float
            This is the sampling rate to generate the trajectories
        - processvar : pl.dynamics.BaseProcessVariance
            Must be a subset of process variance class
        &#39;&#39;&#39;
        if processvar is not None:
            if not pl.isprocessvariance(processvar):
                raise TypeError(&#39;`processvar` ({}) not recognized&#39;.format(
                    type(processvar)))
        if init_dist is not None:
            self.init_dist = init_dist
        self.processvar = processvar
        self.n_replicates += 1
        
        self.dt = dt
        n_valid = 1

        while n_valid &gt; 0:
            init_abundance = self.init_dist.sample(size=len(self.asvs)).reshape(-1,1)
            d = self.dynamics.integrate(
                initial_conditions=init_abundance,
                dt=self.dt, processvar=processvar, subsample=True, 
                times=self.master_times, n_days=self.n_days)
            if not np.any(np.isnan(d[&#39;X&#39;])):
                # valid, we dont have to resample
                self.data.append(d[&#39;X&#39;])
                self.times.append(d[&#39;times&#39;])
                n_valid -= 1
            else:
                logging.info(&#39;resampling&#39;)
                print(d[&#39;X&#39;])

        return self
    
    def stability(self):
        return self.dynamics.stability()

    def simulateRealRegressionDataDMD(self, subjset, alpha, qpcr_scale=None):
        &#39;&#39;&#39;This function converts the synthetic trajectories into &#34;real&#34; data
        by simulating read counts and qPCR measurements. We base the sampling
        on parameters that we learn from the real data (MouseSet `subjset`). We assume
        that the real data has already been filtered.

        This uses a dirichlet multinomial to simulate the reads

        Simulating qPCR measurements
        ----------------------------
        We simulate the qPCR measurements with a lognormal distribution that has
        the standard deviation `qpcr_scale`. If `qpcr_scale` is not given, then we fit
        fit the qPCR measurements in `subjset` and use that value.

        Simulating count data
        ---------------------
        We first simulate the read depth for each day from a negative binomial
        that was fitted using the read depth of our data from `subjset` (using function 
        minimization). To generate the indivual counts for each ASV, we then &#39;sample&#39; 
        from a dirichlet multinomial using the sample read depth as our counts and the 
        `alpha` parameter as the multiplicative concentration for our relative abundances, where
            $\mathbf{\alpha}_i = \alpha * r_i$,
            $\mathbf{\alpha}_i$ is the concentration parameter for ASV $i$ to sample from a dirichlet
            distribution,
            $r_i$ is the relative abundance for ASV $i$,
            $\alpha$ is the input concentration
        To emulate a dirichlet multinomial distribution, we first sample the relative abundances 
        of each of the ASVs from a dirichlet distribution using our alpha term as sepcified above, 
        and then sample from a multinomial distribution given our sampled a read depth and our 
        probabilities sampled concentrations for each ASV from the dirichlet distribution.

        Parameters
        ----------
        subjset : pl.Study
            This is the real data that we are fitting to
        alpha : numeric, Optional
            This is a parameter for how much noise you have in sampling the individual reads.
            The larger the number, the less noise there is in the system. The smaller the number,
            the more noise there is in sampling the relative abundances
        qpcr_scale : numeric, None, Opional
            This is the scale of the lognormal distribution used to generate the qPCR measurements
            of the simulated data

        Returns
        -------
        pl.Study
            This is the subject set that contains the data in terms of counts and absolute abundance
        &#39;&#39;&#39;
        logging.info(&#39;Fitting real data&#39;)
        # load Study and filter
        if type(subjset) == str:
            subjset = pl.Study.load(subjset)
        elif not pl.isstudy(subjset):
            raise ValueError(&#39;`subjset` ({}) must eb a pl.Study object&#39;.format(
                type(subjset)))
        if not pl.isnumeric(alpha):
            raise ValueError(&#39;`alpha` ({}) must either be a float or an int&#39;.format(type(alpha)))
        elif alpha &lt; 0:
            raise ValueError(&#39;`alpha` ({}) must be greater than 0&#39;.format(alpha))
        if qpcr_scale is not None:
            if not pl.isnumeric(qpcr_scale):
                raise ValueError(&#39;`qpcr_scale` ({}) must either be a float or an int&#39;.format(
                    type(qpcr_scale)))
            elif qpcr_scale &lt; 0:
                raise ValueError(&#39;`qpcr_scale` ({}) must be greater than 0&#39;.format(
                    qpcr_scale))
        
        # Fit qPCR with lognormal if necessary
        if qpcr_scale is not None:
            data = []
            for subj in subjset:
                for t, qpcr in subj.qpcr.items():
                    d = np.log(qpcr.data)
                    data = np.append(data, d - np.mean(d))
            qpcr_scale = np.std(data)
            logging.info(&#39;qpcr fitted scale: {}&#39;.format(qpcr_scale))

        # Fit read depth with negative binomial
        read_depths = np.asarray([])
        for subj in subjset:
            read_depths = np.append(read_depths, subj.read_depth())
        n_pred, p_pred = _fit_nbinom(read_depths)
        logging.info(&#39;read depth negbin n: {}, p: {}&#39;.format(n_pred, p_pred))
        readdepth_negbin = scipy.stats.nbinom(n_pred, p_pred)
        
        # Make data, record the data with noise
        ret_subjset = pl.Study(asvs=self.asvs)
        for ridx in range(self.n_replicates):
            mid = str(ridx)
            ret_subjset.add(name=mid)

            for tidx in range(len(self.times[ridx])):
                # make time id
                t = self.times[ridx][tidx]

                # Sample counts
                sum_abund = np.sum(self.data[ridx][:,tidx])
                rel = self.data[ridx][:,tidx] / sum_abund
                probs = (scipy.stats.dirichlet.rvs(rel*alpha)).flatten()
                read_depth = readdepth_negbin.rvs()
                ret_subjset[mid].reads[t] = scipy.stats.multinomial.rvs(read_depth, probs).ravel()

                # Sample qPCR
                triplicates = np.exp(np.log(sum_abund) + qpcr_scale * npr.normal(size=3))
                ret_subjset[mid].qpcr[t] = pl.qPCRdata(cfus=triplicates, mass=1., 
                    dilution_factor=1.)

            ret_subjset[mid].times = self.times[ridx]
        
        # Add in the perturbations
        if self.dynamics.perturbations is not None:
            for perturbation in self.dynamics.perturbations:
                ret_subjset.add_perturbation(perturbation.start, perturbation.end)
        return ret_subjset

    def simulate_reads(self, a0, a1, read_depth):
        &#39;&#39;&#39;This function converts the synthetic trajectories into &#34;real&#34; data
        by simulating read counts and qPCR measurements. We base the sampling
        on parameters that we learn from the real data (MouseSet `subjset`). We assume
        that the real data has already been filtered.

        Simulating count data
        ---------------------
        We first fit the read depth for each day (`r_k`) from a negative binomial from the 
        read depth of our data (`subjset`) using function minimization. We then use `r_k`, 
        `a_0`, and `a_1` with the relative abundances to sample from a negative binomial 
        distirbution. We then use the relative abundances from this sample as the concentrations
        for a multinomial distribution with read depth `r_k`.

        Parameters
        ----------
        subjset : pl.Study
            This is the real data that we are fitting to
        a0, a1 : numeric
            These are the negative binomial dispersion parameters that we are using to 
            simulate the data
        &#39;&#39;&#39;
        if not np.all(pl.itercheck([a0, a1], pl.isnumeric)):
            raise TypeError(&#39;`a0` ({}) and `a1` ({}) must be numerics&#39;.format(
                type(a0), type(a1)))
        if a0 &lt; 0 or a1 &lt; 0:
            raise ValueError(&#39;`a0` ({}) and `a1` ({}) must be &gt; 0&#39;.format(a0, a1))
        
        # Make data, record the data with noise
        n_time_points = 0
        for times in self.times:
            n_time_points += len(times)

        ret_subjset = pl.Study(asvs=self.asvs)
        data = self.data
        times = self.times
        for ridx in np.arange(self.n_replicates):
            mid = str(ridx)
            ret_subjset.add(name=mid)
            # self.data_w_noise.append(np.zeros(shape=data[ridx].shape,dtype=float))

            for tidx in range(len(times[ridx])):
                # make time id
                t = times[ridx][tidx]

                # Sample counts
                sum_abund = np.sum(data[ridx][:,tidx])
                rel = data[ridx][:,tidx] / sum_abund

                phi = read_depth * rel
                eps = a0 / rel + a1

                reads = pl.random.negative_binomial.sample(phi, eps)
                # concentration = reads/np.sum(reads)
                # ret_subjset[mid].reads[t] = scipy.stats.multinomial.rvs(
                #     r_k, concentration).ravel()
                ret_subjset[mid].reads[t] = reads

            ret_subjset[mid].times = times[ridx]
        
        # Add in the perturbations
        if self.dynamics.perturbations is not None:
            for perturbation in self.dynamics.perturbations:
                ret_subjset.add_perturbation(perturbation.start, perturbation.end)
        self.subjset_with_noise = ret_subjset

    def simulate_qpcr(self, qpcr_noise_scale):
        &#39;&#39;&#39;This function converts the synthetic trajectories into &#34;real&#34; data
        by simulating read counts and qPCR measurements. We base the sampling
        on parameters that we learn from the real data (MouseSet `subjset`). We assume
        that the real data has already been filtered.

        We assume that the function `simulate_reads`

        Simulating qPCR measurements
        ----------------------------
        We simulate the qPCR measurements with a lognormal distribution that was
        fitted using the data from `subjset`. We use this parameterization to sample
        a qPCR measurement with mean from the total biomass of the simulated data.

        Parameters
        ----------
        qpcr_noise_scale : numeric
            This is the parameter to scale the `s` parameter learned by the lognormal
            distribution.
        &#39;&#39;&#39;
        if not pl.isnumeric(qpcr_noise_scale):
            raise TypeError(&#39;`qpcr_noise_scale` ({}) must either be a float or an int&#39;.format(
                type(qpcr_noise_scale)))
        elif qpcr_noise_scale &lt; 0:
            raise ValueError(&#39;`qpcr_noise_scale` ({}) must be greater than 0&#39;.format(
                qpcr_noise_scale))

        for ridx in np.arange(self.n_replicates):
            mid = str(ridx)
            subj = self.subjset_with_noise[mid]

            for tidx, t in enumerate(self.times[ridx]):
                
                # Sample qPCR
                sum_abund = np.sum(self.data[ridx][:,tidx])
                triplicates = np.exp(np.log(sum_abund) + qpcr_noise_scale * pl.random.normal.sample(size=3))
                subj.qpcr[t] = pl.qPCRdata(
                    cfus=triplicates, mass=1., dilution_factor=1.)

    def get_subjset(self):
        &#39;&#39;&#39;Return the subjectset with the noise

        Returns
        -------
        pylab.base.Study
        &#39;&#39;&#39;
        return self.subjset_with_noise

    def simulateRealRegressionDataNegBinMD(self, a0, a1, qpcr_noise_scale, subjset, replicates=&#39;all&#39;, read_depth=None):
        &#39;&#39;&#39;This function converts the synthetic trajectories into &#34;real&#34; data
        by simulating read counts and qPCR measurements. We base the sampling
        on parameters that we learn from the real data (MouseSet `subjset`). We assume
        that the real data has already been filtered.

        This uses a negative binomial distribution to simiulate the reads.

        Simulating qPCR measurements
        ----------------------------
        We simulate the qPCR measurements with a lognormal distribution that was
        fitted using the data from `subjset`. We use this parameterization to sample
        a qPCR measurement with mean from the total biomass of the simulated data.

        Simulating count data
        ---------------------
        We first fit the read depth for each day (`r_k`) from a negative binomial from the 
        read depth of our data (`subjset`) using function minimization. We then use `r_k`, 
        `a_0`, and `a_1` with the relative abundances to sample from a negative binomial 
        distirbution. We then use the relative abundances from this sample as the concentrations
        for a multinomial distribution with read depth `r_k`.

        Parameters
        ----------
        subjset : pl.Study
            This is the real data that we are fitting to
        a0, a1 : numeric
            These are the negative binomial dispersion parameters that we are using to 
            simulate the data
        qpcr_noise_scale : numeric
            This is the parameter to scale the `s` parameter learned by the lognormal
            distribution.
        replicates : str, array, int
            Which replicates to make the data for. If `replicates=&#39;all&#39;`, then we make
            it for all of the replicates. If it is an array, we assume it is an array of
            replicate indices of the replicates that you want.

        Returns
        -------
        pl.Study
            This is the subject set that contains the data in terms of counts and absolute abundance
        &#39;&#39;&#39;
        logging.info(&#39;Fitting real data&#39;)
        if pl.isstr(subjset):
            subjset = pl.base.Study.load(subjset)
        elif not pl.isstudy(subjset):
            raise TypeError(&#39;`subjset` ({}) must be a pylab.base.SubjsetSet&#39;.format( 
                type(subjset)))
        if not pl.isnumeric(qpcr_noise_scale):
            raise TypeError(&#39;`qpcr_noise_scale` ({}) must either be a float or an int&#39;.format(
                type(qpcr_noise_scale)))
        elif qpcr_noise_scale &lt; 0:
            raise ValueError(&#39;`qpcr_noise_scale` ({}) must be greater than 0&#39;.format(
                qpcr_noise_scale))
        if pl.isstr(replicates):
            if replicates == &#39;all&#39;:
                replicates = np.arange(len(self.data))
            else:
                raise ValueError(&#39;`replicates` ({}) not recognized&#39;.format(replicates))
        elif pl.isint(replicates):
            replicates = [replicates]
        if pl.isarray(replicates):
            for i in replicates:
                if not pl.isint(i):
                    raise TypeError(&#39;`replicates` ({}) must be ints&#39;.format(type(i)))
                if i &gt;= len(self.data):
                    raise IndexError(&#39;`replicates` ({}) out of range ({})&#39;.format(
                        i, len(self.data)))
        else:
            raise TypeError(&#39;`replicates` ({}) type not recognized&#39;.format(type(replicates)))


        # # Fit qPCR with lognormal
        # data = []
        # for subj in subjset:
        #     for t, qpcr in subj.qpcr.items():
        #         if np.any(np.isnan(qpcr.data)):
        #             continue
        #         data.append(qpcr.data)
        # std_biomass = _fit_qpcr(data) * self.qpcr_noise_scale
        std_biomass = qpcr_noise_scale
        logging.info(&#39;lognormal s: {}&#39;.format(std_biomass))

        # Fit read depth with negative binomial
        if read_depth is None:
            read_depths = np.asarray([])
            for subj in subjset:
                read_depths = np.append(read_depths, subj.read_depth())
            n_pred, p_pred = _fit_nbinom(read_depths)
            logging.info(&#39;negbin n: {}, p: {}&#39;.format(n_pred, p_pred))
            negbin_read_depth = scipy.stats.nbinom(n_pred, p_pred)

        # Make data, record the data with noise
        n_time_points = 0
        for times in self.times:
            n_time_points += len(times)

        ret_subjset = pl.Study(asvs=self.asvs)
        data = self.data
        times = self.times
        for ridx in replicates:
            mid = str(ridx)
            ret_subjset.add(name=mid)
            # self.data_w_noise.append(np.zeros(shape=data[ridx].shape,dtype=float))

            for tidx in range(len(times[ridx])):
                # make time id
                t = times[ridx][tidx]

                # Sample counts
                sum_abund = np.sum(data[ridx][:,tidx])
                rel = data[ridx][:,tidx] / sum_abund
                if read_depth is None:
                    r_k = negbin_read_depth.rvs()
                else:
                    r_k = read_depth
                read_depth = 75000

                phi = r_k * rel
                eps = a0 / rel + a1

                reads = pl.random.negative_binomial.sample(phi, eps)
                # concentration = reads/np.sum(reads)
                # ret_subjset[mid].reads[t] = scipy.stats.multinomial.rvs(
                #     r_k, concentration).ravel()
                ret_subjset[mid].reads[t] = reads

                # Sample qPCR
                triplicates = np.exp(np.log(sum_abund) + std_biomass * npr.normal(size=3))
                ret_subjset[mid].qpcr[t] = pl.qPCRdata(
                    cfus=triplicates, mass=1., dilution_factor=1.)

            ret_subjset[mid].times = times[ridx]
        
        # Add in the perturbations
        if self.dynamics.perturbations is not None:
            for perturbation in self.dynamics.perturbations:
                ret_subjset.add_perturbation(perturbation.start, perturbation.end)
        return ret_subjset

    def simulateExactSubjset(self):
        &#39;&#39;&#39;This function is effectively the same as `simulateRealRegressionData*` but 
        we add an :math:`\epsilon` amount of measurement noise.

        Returns
        -------
        pylab.Base.Study
        &#39;&#39;&#39;
        subjset = pl.Study(asvs=self.asvs)
        data = self.data
        times = self.times

        for ridx in range(len(data)):
            mid = str(ridx)
            subjset.add(name=mid)

            for tidx in range(len(times[ridx])):
                t = times[ridx][tidx]

                # Make &#34;exact&#34; counts we read depth at ~1000000
                total_abund = np.sum(data[ridx][:,tidx])
                rel = data[ridx][:,tidx]/total_abund
                counts = np.asarray(rel * 1000000, dtype=int)
                subjset[mid].reads[t] = counts

                # make &#34;exact&#34; qpcr by having very little qpcr noise
                subjset[mid].qpcr[t] = pl.qPCRdata( 
                    cfus=pl.random.normal.sample(mean=total_abund, std=1e-10, size=3), 
                    mass=1., dilution_factor=1.)
            subjset[mid].times = times[ridx]

        # Add in the perturbations
        if self.dynamics.perturbations is not None:
            for perturbation in self.dynamics.perturbations:
                subjset.add_perturbation(perturbation.start, perturbation.end, 
                    name=perturbation.name)
        
        return subjset


def _fit_nbinom(X, initial_params=None):
    from scipy.special import gammaln
    from scipy.special import psi
    from scipy.special import factorial
    from scipy.optimize import fmin_l_bfgs_b as optim
    &#39;&#39;&#39;Fit Total read depth with a negative binomial distribution
    Source: https://github.com/gokceneraslan/fit_nbinom/blob/master/fit_nbinom.py

    Parameters
    ----------
    X : np.ndarray
        - Vector of observations

    Returns
    -------
    n, p
        - Fitted parameters (for scipy parameterization)
    &#39;&#39;&#39;
    infinitesimal = np.finfo(np.float).eps

    def log_likelihood(params, *args):
        r, p = params
        X = args[0]
        N = X.size

        #MLE estimate based on the formula on Wikipedia:
        # http://en.wikipedia.org/wiki/Negative_binomial_distribution#Maximum_likelihood_estimation
        result = np.sum(gammaln(X + r)) \
            - np.sum(np.log(factorial(X))) \
            - N*(gammaln(r)) \
            + N*r*np.log(p) \
            + np.sum(X*np.log(1-(p if p &lt; 1 else 1-infinitesimal)))

        return -result

    def log_likelihood_deriv(params, *args):
        r, p = params
        X = args[0]
        N = X.size

        pderiv = (N*r)/p - np.sum(X)/(1-(p if p &lt; 1 else 1-infinitesimal))
        rderiv = np.sum(psi(X + r)) \
            - N*psi(r) \
            + N*np.log(p)

        return np.array([-rderiv, -pderiv])

    if initial_params is None:
        #reasonable initial values (from fitdistr function in R)
        m = np.mean(X)
        v = np.var(X)
        size = (m**2)/(v-m) if v &gt; m else 10

        #convert mu/size parameterization to prob/size
        p0 = size / ((size+m) if size+m != 0 else 1)
        r0 = size
        initial_params = np.array([r0, p0])

    bounds = [(infinitesimal, None), (infinitesimal, 1)]
    optimres = optim(log_likelihood,
                     x0=initial_params,
                     #fprime=log_likelihood_deriv,
                     args=(X,),
                     approx_grad=1,
                     bounds=bounds)

    params = optimres[0]
    # Return n and p
    return params[0], params[1]

def _fit_qpcr(X):
    &#39;&#39;&#39;Take log of the data, subtract the mean of the triplicates (in log space),
    fit residual to a normal distribution, return the fitted standard deviation.

    Since the shape may not be totally uniform (not 3 samples for each qpcr data
    point, do it element by element)

    Parameters
    ----------
    X : 2-dim np.ndarray
        - dim 0: references the triplicate index
        - dim 2: references the triplicate

    Returns
    -------
    std (float)
        - This is the fitted shape parameter of the lognormal distribution
    &#39;&#39;&#39;
    if not pl.isarray(X):
        raise ValueError(&#39;`X` ({}) must be an array&#39;.format(type(X)))

    # Take the log of the data
    for i in range(len(X)):
        X[i] = np.log(X[i])

    # Center for each triplicate in log space
    for i in range(len(X)):
        X[i] = X[i] - np.mean(X[i])

    # Fit with a normal distribution
    data = np.array([])
    for i in range(len(X)):
        data = np.append(data, X[i])
    _, s = scipy.stats.norm.fit(data)

    return s

def set_timepoints_without_timeseries(N, T_start, T_end, D, move_timepoints_if_fail=True, 
    essential_timepoints=None):
    &#39;&#39;&#39;Set time points for trajectory.

    This is a direct implementation of the algorithm specified in the method
    supplement.

    We deterministically allocate timepoints such that the density over certain
    time periods is proportionally higher than other time periods. The default
    denisty is 1. Additionally, we choose timepoints such that:
        t1 = set_timepoints_without_timeseries(N, ...)
        t2 = set_timepoints_without_timeseries(M, ...)

        We have two time-series that have all the same parameters but
        M &gt;= N, then t1 is a subset of t2.

    Note that every subsection of the time series must have at least 1 point.

    Essential Timepoints
    --------------------
    Essential timepoints are timepoints that must be included.

    Moving timepoints if necessary
    ------------------------------
    Each interval needs to have at least one timepoint allocated to it (more if there
    are additional essential timepoints). If by the end of the allocation process there is 
    no time points in one of the intervals, this will fail. To avoid failing - we can set 
    the flag `move_timepoints_if_fail` to True. This moves a timepoint allocated from
    the most populated interval to the one/s with 0. If there are multiple timepoints 
    that have the same number of timepoints, then we use the earliest interval.
    Example:
        D_size = [5,6,2,1,2,3,0] &gt;&gt;&gt; [5,5,2,1,2,3,1]
        D_size = [5,6,2,0,2,3,0] &gt;&gt;&gt; [4,5,2,1,2,3,1]

    Allocating timepoints proportionally
    ------------------------------------
    We add the timepoints proportionally to each interval. If there are extra timepoints
    at the end of this step then we sequentially add them to the interval based on their
    priority. From a high level, the highest density has the highest priority. If multiple
    intervals have the same density, then we add the timepoints sequentially from left
    to right to only the intervals that have the high priority. Exceptions are: If an
    interval has no timepoints, it has the highest priority.

    Parameters
    ----------
    N : int
        Total number of timepoints to allocate
    T_start, T_end : float
        These are the start and end times of the time-series, respectivelly.
        `T_start` is inclusive and `T_end` is not inclusive.
    D : list(3-tuple), 3-tuple, None
        These are the densities which we allocate to
    move_timepoints_if_fail : bool
        If False, we raise an exception if there is an interval that does not have
        a timepoint by the end of allocation. If True, we move around the timepoints
        such that None of the intervals have no timepoints. We do this using the 
        algorithm described above.
    essential_timepoints : list, None
        If not None, these are a list of numeric timepoints that we must include.
    &#39;&#39;&#39;
    # Type and value checking
    if not pl.isbool(move_timepoints_if_fail):
        raise TypeError(&#39;`move_timepoints_if_fail` ({}) must be a bool&#39;.format( 
            type(move_timepoints_if_fail)))
    if not pl.isint(N):
        raise TypeError(&#39;`N` ({}) must be an int&#39;.format(type(N)))
    if N &lt;= 0:
        raise ValueError(&#39;`N` ({}) must be &gt; 0&#39;.format(N))
    if pl.istuple(D):
        D = [D]
    for ele in D:
        if not pl.istuple(ele):
            raise TypeError(&#39;Every element in `D` ({}) must be a tuple&#39;.format(
                type(ele)))
        if len(ele) != 3:
            raise ValueError(&#39;`Every element in `D` must be a tuple of length 3 ({})&#39;.format( 
                len(ele)))
        start, stop, density = ele
        if not pl.isnumeric(start):
            raise TypeError(&#39;`start` ({}) must be a numeric&#39;.format(type(start)))
        if start &lt; T_start or start &gt; T_end:
            raise ValueError(&#39;`start` ({}) out of range&#39;.format(start))
        if not pl.isnumeric(stop):
            raise TypeError(&#39;`stop` ({}) must be a numeric&#39;.format(type(stop)))
        if stop &lt;= T_start or stop &gt; T_end:
            raise ValueError(&#39;`stop` ({}) out of range&#39;.format(stop))
        if stop &lt;= start:
            raise ValueError(&#39;`stop` ({}) must be &gt; `start` ({})&#39;.format(stop, start))
        if not pl.isnumeric(density):
            raise TypeError(&#39;`density` ({}) must be a numeric&#39;.format(density))
        if density &lt; 1:
            raise ValueError(&#39;`density` ({}) must be &gt;= 1&#39;.format(density))
    
    # Make sure that all of the densities are disjoint to each other and that they
    # span the entire time-series
    start_there = False
    end_there = False
    for (s,e,d) in D:
        if s == T_start:
            start_there = True
        if e == T_end:
            end_there = True
    if not (start_there and end_there):
        raise ValueError(&#39;`D` ({}) does not include the start and end points&#39;.format(D))
    for i, (si,ei,di) in enumerate(D):
        si_there = si == T_start
        ei_there = ei == T_end
        for j, (sj, ej, dj) in enumerate(D):
            if i == j:
                continue
            if (si &gt;= sj and si &lt; ej) or (ei &gt; sj and ei &lt;= ej):
                raise ValueError(&#39;`{}`th interval ({}) is contained in the `{}`th interval ({})&#39;.format( 
                    i, (si,ei,di), j, (sj,ej,dj)))
            if not si_there:
                si_there = si == ej
            if not ei_there:
                ei_there = ei == sj
        if (not si_there) or (not ei_there):
            raise ValueError(&#39;In {}th interval, either `{}` or `{}` was not adjacent to any other &#39; \
                &#39;interval ({})&#39;.format(i, si, ei, D))
    total_length = 0
    for s,e,d in D:
        total_length += (e-s)
    if total_length != (T_end - T_start):
        raise ValueError(&#39;lengths ({}) and ({}) are not the same&#39;.format(total_length, (T_end-T_start)))

    if essential_timepoints is not None:
        if pl.isnumeric(essential_timepoints):
            essential_timepoints = [essential_timepoints]
        if not pl.isarray(essential_timepoints):
            raise TypeError(&#39;`essential_timepoints` ({}) must be an array&#39;.format(
                type(essential_timepoints)))
        for ele in essential_timepoints:
            if not pl.isnumeric(ele):
                raise TypeError(&#39;Each element in `essential_timepoints` ({} must be a numeric&#39;.format(
                    type(ele)))
            if ele &lt; T_start or ele &gt; T_end:
                raise ValueError(&#39;Timepoint ({}) in `essential_timepoints` out of range ({}, {})&#39;.format(
                    ele, T_start, T_end))
        essential_timepoints = np.unique(essential_timepoints)
        if len(essential_timepoints) &gt; N:
            raise ValueError(&#39;There cannot be more essential timepoints ({}) than times to allocate ({})&#39;.format(
                len(essential_timepoints), N))

    # Order D (by the start)
    D_new = []
    while len(D) &gt; 0:
        # Get the smallest start
        min_s = None
        min_i = None
        for i, (s,_,_) in enumerate(D):
            if min_s is None:
                min_s = s
                min_i = 0
                continue
            if s &lt; min_s:
                min_s = s
                min_i = i
        D_new.append(D.pop(min_i))
    D = D_new

    # Perform algorithm
    D_size = []
    for s,e,d in D:
        D_size.append(d * (e - s))
    D_total = np.sum(D_size)
    D_size = [int(s * N / D_total) for s in D_size] # casting to an int rounds down
    D_size = np.asarray(D_size, dtype=int)

    # Set the essential points
    min_D_size = []
    for s,e,_ in D:
        cnt = 1 # Must have at least 1 point
        if essential_timepoints is not None:
            for essential_point in essential_timepoints:
                if essential_point &gt;= s and essential_point &lt; e:
                    cnt += 1
        min_D_size.append(cnt)
    min_D_size = np.asarray(min_D_size, dtype=int)

    # Add in extra points if necessary
    if np.sum(D_size) != N:
        Nextra = N - np.sum(D_size)

        # First we check if there are any intervals that have less than necessary times
        while np.any(D_size &lt; min_D_size):
            if Nextra == 0:
                break
            # Get the first index
            j = np.where(D_size &lt; min_D_size)[0][0]
            D_size[j] += 1
            Nextra -= 1

        if Nextra &gt; 0:
            dmax = np.max([d for (_,_,d) in D])
            I = [] # interval indexes that have the highest density
            for i, (s,e,d) in enumerate(D):
                if d == dmax:
                    I.append(i)
            if len(I) == 1:
                D_size[I[0]] += Nextra
            else:
                j = 0
                while Nextra &gt; 0:
                    i = I[j]
                    D_size[i] += 1
                    Nextra -= 1
                    j += 1
                    if j == len(I):
                        j = 0
    
    # check if every interval has the minimum
    if np.any(D_size &lt; min_D_size):
        if move_timepoints_if_fail:
            while np.any(D_size &lt; min_D_size):
                logging.info(&#39;An interval was flagged to not have a time point allocated. Moved&#39;)
                # find the first instance of the max
                max_size = np.max(D_size)
                index_to_take = np.where(D_size == max_size)[0][0]
                
                # Find the first interval with less than the minimum
                index_to_give = np.where(D_size &lt; min_D_size)[0][0]

                D_size[index_to_give] += 1
                D_size[index_to_take] -= 1

                if D_size[index_to_take] &lt; min_D_size[index_to_take]:
                    print(&#39;D_size&#39;, D_size)
                    print(&#39;min_D_size&#39;, min_D_size)
                    print(&#39;index_to_take&#39;, index_to_take)
                    raise ValueError(&#39;This CAN happen but will only happen when there are a lot of essential&#39; \
                        &#39; timepoints and not a lot of timepoints in total. Increase N.&#39;)

        else:
            raise ValueError(&#39;An interval has less than the minimum required timepoints. &#39;\
                &#39;Must increase N ({}) or set the flag &#39; \
                &#39;`move_timepoints_if_fail` to True. D: {}, D_size: {}, min_D_size: {}&#39;.format(
                    N, D, D_size, min_D_size))

    # Set essential timepoints
    essential_D = [[s] for s,_,_ in D]
    if essential_timepoints is not None:
        for essential_point in essential_timepoints:
            # Find where this essential timepoint goes
            for j, (s,e,_) in enumerate(D):
                if essential_point &gt;= s and essential_point &lt; e:
                    # Check to see if essential_timepoint is == to the start. skip if it is
                    if essential_point not in essential_D[j]:
                        essential_D[j].append(essential_point)


    T_N = []
    for i, (si, ei, di) in enumerate(D):
        n = D_size[i]
        Ti = essential_D[i]

        if n &gt; 1:
            done = False
            while not done:
                Ttemp = []
                for k in range(len(Ti)-1):
                    if D_size[i] == (len(Ti) + len(Ttemp)):
                        break
                    newval = (Ti[k] + Ti[k+1])/2
                    Ttemp = np.append(Ttemp, newval)

                if D_size[i] == (len(Ti) + len(Ttemp)):
                    done = True
                else:
                    newval = (Ti[-1] + ei) / 2
                    Ttemp = np.append(Ttemp, newval)
                Ti = np.sort(np.append(Ti, Ttemp))
        T_N = np.append(T_N, Ti)

    return np.sort(T_N)

def issynthetic(x):
    &#39;&#39;&#39;Checks whether the input is a subclass of SyntheticData

    Parameters
    ----------
    x : any
        Input instance to check the type of SyntheticData
    
    Returns
    -------
    bool
        True if `x` is of type SyntheticData, else False
    &#39;&#39;&#39;
    return x is not None and issubclass(x.__class__, SyntheticData)

def make_semisynthetic(chain, min_bayes_factor, init_dist_start, init_dist_end,
    set_times=True, hdf5_filename=None):
    &#39;&#39;&#39;Make a semi synthetic system. We take the system learned in the chain and
    we set the modeling parameters of `SyntheticData` to the learned system. We assume
    that the chain that we pass in was run with a fixed topology.

    Notation
    --------
    In the following procedure, variable names are capitalized if they are constants or
    they are the variables/parameters that are taken from the variable `chain`.
    
    How the synthetic system is set
    -------------------------------
    n_asvs: Set to the number in chain.
    clustering: The clusters assignments are set to the value of the Clustering class
    interactions: Set to the expected value of the posterior. We only include interactions
        whose bayes factor is greater than `min_bayes_factor`.
    perturbations: The number of perturbations is set to be the same as what is in the
        chain. The topology and values of the perturbations are set to the expected value
        of the posterior. We only include perturbation effects whose bayes factor is
        greater than `min_bayes_factor`
    growth and self-interactions: These are set to the learned values for each of the 
        asvs.
    init_dist: The distirbution of the initial timepoints are set by fitting a log normal
        ditribution to the `init_dist_timepoint`th timepoint

    Parameters
    ----------
    chain : str, pylab.inference.BaseMCMC
        This is chain or the file location of the chain
    min_bayes_factor : numeric
        This is the minimum bayes factor needed for a perturbation/interaction
        to be used in the synthetic dataset
    set_times : bool
        If True, we set the times of the subject to be be a union of all of the subjects
        in chain
    init_dist_timepoint : numeric
        Which timepoint to set the initial distribution to. If nothing is provided it will
        set it to the first timepoint (0)
    hdf5_filename : str
        Location of the HDF5 object

    Returns
    -------
    synthetic.SyntheticData
    &#39;&#39;&#39;
    from names import STRNAMES

    if pl.isstr(chain):
        chain = pl.inference.BaseMCMC.load(chain)
    if not pl.isMCMC(chain):
        raise TypeError(&#39;`chain` ({}) is not a pylab.inference.BaseMCMC object&#39;.format(
            type(chain)))
    if hdf5_filename is not None:
        chain.tracer.filename = hdf5_filename

    if not pl.isnumeric(min_bayes_factor):
        raise TypeError(&#39;`min_bayes_factor` ({}) nmust be a numeric&#39;.format(
            type(min_bayes_factor)))
    if min_bayes_factor &lt; 0:
        raise ValueError(&#39;`min_bayes_factor` ({}) must be &gt;= 0&#39;.format(min_bayes_factor))
    if not pl.isbool(set_times):
        raise TypeError(&#39;`set_times` ({}) must be a bool&#39;.format(type(set_times)))
    # if init_dist_timepoint is None:
    #     init_dist_timepoint = 0
    # if not pl.isnumeric(init_dist_timepoint):
    #     raise TypeError(&#39;`init_dist_timepoint` ({}) must be a numeric&#39;.format(
    #         type(init_dist_timepoint)))

    GRAPH = chain.graph
    DATA = GRAPH.data
    SUBJSET = DATA.subjects
    ASVS = DATA.asvs
    logging.info(&#39;Number of ASVs ({})&#39;.format(len(ASVS)))

    n_days = -1
    for subj in SUBJSET:
        maxday = np.max(subj.times)
        if maxday &gt; n_days:
            n_days = maxday
    logging.info(&#39;Number of days: {}&#39;.format(n_days))

    GROWTH = GRAPH[STRNAMES.GROWTH_VALUE]
    SELF_INTERACTIONS = GRAPH[STRNAMES.SELF_INTERACTION_VALUE]
    INTERACTIONS = GRAPH[STRNAMES.INTERACTIONS_OBJ]
    CLUSTERING = GRAPH[STRNAMES.CLUSTERING_OBJ]
    PERTURBATIONS = GRAPH.perturbations
    perturbations_additive = GROWTH.perturbations_additive

    synth = SyntheticData(n_days=n_days,
        perturbations_additive=perturbations_additive)

    synth.set_asvs(asvs=DATA.asvs)
    synth.set_cluster_assignments(clusters=CLUSTERING.toarray())

    # Set the interactions
    # --------------------
    synth.dynamics.interactions = pl.Interactions(clustering=synth.dynamics.clustering, 
        use_indicators=True, G=synth.G)
    logging.info(&#39;Generating bayes factors&#39;)
    bayes_factors_asvs = INTERACTIONS.generate_bayes_factors_posthoc(
        prior=GRAPH[STRNAMES.CLUSTER_INTERACTION_INDICATOR].prior,
        section=&#39;posterior&#39;)
    logging.info(&#39;Getting values&#39;)
    cluster_interactions_asvs = pl.variables.summary(INTERACTIONS, set_nan_to_0=False,
        section=&#39;posterior&#39;, only=[&#39;mean&#39;])[&#39;mean&#39;]

    logging.info(&#39;Set the interactions&#39;)
    for interaction in synth.dynamics.interactions:

        # Get the target and source cluster index of the interaction
        tcidx = synth.dynamics.clustering.cid2cidx[interaction.target_cid]
        scidx = synth.dynamics.clustering.cid2cidx[interaction.source_cid]

        # Get a set of target and source asv indices of the interaction (it can
        # be any of them because we assume the chain was run with a fixed topology)
        taidx = list(CLUSTERING.clusters[CLUSTERING.order[tcidx]].members)[0]
        saidx = list(CLUSTERING.clusters[CLUSTERING.order[scidx]].members)[0]

        # If the bayes factor of the interaction is greater than the minimum bayes
        # factor, then we set the interaction. If it is less, then we set the 
        # indicator to false
        if bayes_factors_asvs[taidx, saidx] &gt; min_bayes_factor:
            interaction.value = cluster_interactions_asvs[taidx, saidx]
            interaction.indicator = True
        else:
            interaction.value = 0
            interaction.indicator = False

    # Set the perturbations
    # ---------------------
    _ps = []
    for PERTURBATION in PERTURBATIONS:
        perturbation = pl.contrib.ClusterPerturbation(
            start=PERTURBATION.start, end=PERTURBATION.end,
            G=synth.G, clustering=synth.dynamics.clustering)

        # values and bayes factors are on an asv level
        values = pl.variables.summary(PERTURBATION, section=&#39;posterior&#39;, 
            only=[&#39;mean&#39;])[&#39;mean&#39;]
        indicator_trace = ~np.isnan(PERTURBATION.get_trace_from_disk(section=&#39;posterior&#39;))
        bayes_factor = pl.variables.summary(indicator_trace, only=[&#39;mean&#39;])[&#39;mean&#39;]
        bayes_factor = bayes_factor/(1. - bayes_factor)
        bayes_factor = bayes_factor * (PERTURBATION.probability.prior.b.value + 1) / \
            (PERTURBATION.probability.prior.a.value + 1)

        cluster_order = synth.dynamics.clustering.order
        for cidx in range(len(CLUSTERING)):
            
            aidx = list(CLUSTERING.clusters[CLUSTERING.order[cidx]].members)[0]
            if bayes_factor[aidx] &lt; min_bayes_factor:
                perturbation.magnitude.value[cluster_order[cidx]] = 0
                perturbation.indicator.value[cluster_order[cidx]] = False
            else:
                perturbation.magnitude.value[cluster_order[cidx]] = \
                    values[aidx]
                perturbation.indicator.value[cluster_order[cidx]] = True

        _ps.append(perturbation)
    synth.dynamics.perturbations = _ps

    # Set the growth and self-interactions
    # ------------------------------------
    synth.dynamics.growth = pl.variables.summary(GROWTH, section=&#39;posterior&#39;, 
        only=[&#39;mean&#39;])[&#39;mean&#39;]
    synth.dynamics.self_interactions = pl.variables.summary(
        SELF_INTERACTIONS, section=&#39;posterior&#39;, only=[&#39;mean&#39;])[&#39;mean&#39;]

    # Set the timepoints if possible
    # ------------------------------
    if set_times:
        times = []
        for subj in SUBJSET:
            times = np.append(times, subj.times)
        times = np.sort(np.unique(times))
        synth.master_times = times

    # Set the initial distribution
    # ----------------------------
    # values = []
    # for subj in SUBJSET:
    #     if init_dist_timepoint in subj.times:

    #         idx = np.searchsorted(subj.times, init_dist_timepoint)

    #         matrix = subj.matrix()[&#39;abs&#39;]
    #         values = np.append(values, matrix[:,idx].ravel())
    #     else:
    #         logging.warning(&#39;Timepoint `{}` not in subject `{}` ({})&#39;.format(
    #             init_dist_timepoint, subj.name, subj.times))
    # values = np.asarray(values)
    # values = values[values &gt; 0]
    # logvalues = np.log(values)
    synth.init_dist = pl.variables.Uniform(low=init_dist_start, high=init_dist_end)

    return synth
    
def subsample_timepoints(times, N, required=None):
    &#39;&#39;&#39;Subsample the timepoints `times` so that it has `N` timepoints.

    If required is not None, it is a list of timepoints that must be
    included (start/ends of perturbations, etc.) in the return times.

    Parameters
    ----------
    times : np.ndarray
        An array of timepoints
    N : int
        Total number of timepoints to be remaining
    required : None, np.ndarray
        An array of timepoints that need to be included in the return array
    &#39;&#39;&#39;
    if not pl.isarray(times):
        raise TypeError(&#39;`times` ({}) must be an array&#39;.format(type(times)))
    times = np.sort(np.array(times))

    if not pl.isint(N):
        raise TypeError(&#39;`N` ({}) must be an int&#39;.format(type(N)))
    if N &lt;= 1:
        raise ValueError(&#39;`N` ({}) must be &gt; 1&#39;.format(N))
    if required is not None:
        if not pl.isarray(required):
            raise TypeError(&#39;`required` ({}) must be an array&#39;.format(type(required)))
        for ele in required:
            if ele not in times:
                raise ValueError(&#39;({}) in `required` is not in times: ({})&#39;.format(ele,times))

        if N - len(required) &lt;= 0:
            raise ValueError(&#39;The number of required points ({}) is more than the total number &#39; \
                &#39;of points that need to remain ({})&#39;.format(len(required), N))

        add_at_end = []
        for ele in required:
            # get index at 
            idx = np.searchsorted(times, ele)
            add_at_end.append(ele)
            times = np.delete(times, [idx])
        N -= len(add_at_end)

    l = len(times)
    if N &gt; l/2:
        # These are the indices to take away
        idxs = np.arange(0, l, step=l/(l-N), dtype=int)
        a = np.delete(times, idxs)

    else:
        # These are the indicates to keep
        idxs = np.arange(0, l, step=l/N, dtype=int)
        a = times[idxs]

    if required is not None:
        a = np.sort(np.append(a, add_at_end))
    return a

    </code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="mdsine2.synthetic.issynthetic"><code class="name flex">
<span>def <span class="ident">issynthetic</span></span>(<span>x)</span>
</code></dt>
<dd>
<div class="desc"><p>Checks whether the input is a subclass of SyntheticData</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>any</code></dt>
<dd>Input instance to check the type of SyntheticData</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>bool</code></dt>
<dd>True if <code>x</code> is of type SyntheticData, else False</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def issynthetic(x):
    &#39;&#39;&#39;Checks whether the input is a subclass of SyntheticData

    Parameters
    ----------
    x : any
        Input instance to check the type of SyntheticData
    
    Returns
    -------
    bool
        True if `x` is of type SyntheticData, else False
    &#39;&#39;&#39;
    return x is not None and issubclass(x.__class__, SyntheticData)</code></pre>
</details>
</dd>
<dt id="mdsine2.synthetic.make_semisynthetic"><code class="name flex">
<span>def <span class="ident">make_semisynthetic</span></span>(<span>chain, min_bayes_factor, init_dist_start, init_dist_end, set_times=True, hdf5_filename=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Make a semi synthetic system. We take the system learned in the chain and
we set the modeling parameters of <code><a title="mdsine2.synthetic.SyntheticData" href="#mdsine2.synthetic.SyntheticData">SyntheticData</a></code> to the learned system. We assume
that the chain that we pass in was run with a fixed topology.</p>
<h2 id="notation">Notation</h2>
<p>In the following procedure, variable names are capitalized if they are constants or
they are the variables/parameters that are taken from the variable <code>chain</code>.</p>
<h2 id="how-the-synthetic-system-is-set">How The Synthetic System Is Set</h2>
<p>n_asvs: Set to the number in chain.
clustering: The clusters assignments are set to the value of the Clustering class
interactions: Set to the expected value of the posterior. We only include interactions
whose bayes factor is greater than <code>min_bayes_factor</code>.
perturbations: The number of perturbations is set to be the same as what is in the
chain. The topology and values of the perturbations are set to the expected value
of the posterior. We only include perturbation effects whose bayes factor is
greater than <code>min_bayes_factor</code>
growth and self-interactions: These are set to the learned values for each of the
asvs.
init_dist: The distirbution of the initial timepoints are set by fitting a log normal
ditribution to the <code>init_dist_timepoint</code>th timepoint</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>chain</code></strong> :&ensp;<code>str, pylab.inference.BaseMCMC</code></dt>
<dd>This is chain or the file location of the chain</dd>
<dt><strong><code>min_bayes_factor</code></strong> :&ensp;<code>numeric</code></dt>
<dd>This is the minimum bayes factor needed for a perturbation/interaction
to be used in the synthetic dataset</dd>
<dt><strong><code>set_times</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, we set the times of the subject to be be a union of all of the subjects
in chain</dd>
<dt><strong><code>init_dist_timepoint</code></strong> :&ensp;<code>numeric</code></dt>
<dd>Which timepoint to set the initial distribution to. If nothing is provided it will
set it to the first timepoint (0)</dd>
<dt><strong><code>hdf5_filename</code></strong> :&ensp;<code>str</code></dt>
<dd>Location of the HDF5 object</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>synthetic.SyntheticData</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def make_semisynthetic(chain, min_bayes_factor, init_dist_start, init_dist_end,
    set_times=True, hdf5_filename=None):
    &#39;&#39;&#39;Make a semi synthetic system. We take the system learned in the chain and
    we set the modeling parameters of `SyntheticData` to the learned system. We assume
    that the chain that we pass in was run with a fixed topology.

    Notation
    --------
    In the following procedure, variable names are capitalized if they are constants or
    they are the variables/parameters that are taken from the variable `chain`.
    
    How the synthetic system is set
    -------------------------------
    n_asvs: Set to the number in chain.
    clustering: The clusters assignments are set to the value of the Clustering class
    interactions: Set to the expected value of the posterior. We only include interactions
        whose bayes factor is greater than `min_bayes_factor`.
    perturbations: The number of perturbations is set to be the same as what is in the
        chain. The topology and values of the perturbations are set to the expected value
        of the posterior. We only include perturbation effects whose bayes factor is
        greater than `min_bayes_factor`
    growth and self-interactions: These are set to the learned values for each of the 
        asvs.
    init_dist: The distirbution of the initial timepoints are set by fitting a log normal
        ditribution to the `init_dist_timepoint`th timepoint

    Parameters
    ----------
    chain : str, pylab.inference.BaseMCMC
        This is chain or the file location of the chain
    min_bayes_factor : numeric
        This is the minimum bayes factor needed for a perturbation/interaction
        to be used in the synthetic dataset
    set_times : bool
        If True, we set the times of the subject to be be a union of all of the subjects
        in chain
    init_dist_timepoint : numeric
        Which timepoint to set the initial distribution to. If nothing is provided it will
        set it to the first timepoint (0)
    hdf5_filename : str
        Location of the HDF5 object

    Returns
    -------
    synthetic.SyntheticData
    &#39;&#39;&#39;
    from names import STRNAMES

    if pl.isstr(chain):
        chain = pl.inference.BaseMCMC.load(chain)
    if not pl.isMCMC(chain):
        raise TypeError(&#39;`chain` ({}) is not a pylab.inference.BaseMCMC object&#39;.format(
            type(chain)))
    if hdf5_filename is not None:
        chain.tracer.filename = hdf5_filename

    if not pl.isnumeric(min_bayes_factor):
        raise TypeError(&#39;`min_bayes_factor` ({}) nmust be a numeric&#39;.format(
            type(min_bayes_factor)))
    if min_bayes_factor &lt; 0:
        raise ValueError(&#39;`min_bayes_factor` ({}) must be &gt;= 0&#39;.format(min_bayes_factor))
    if not pl.isbool(set_times):
        raise TypeError(&#39;`set_times` ({}) must be a bool&#39;.format(type(set_times)))
    # if init_dist_timepoint is None:
    #     init_dist_timepoint = 0
    # if not pl.isnumeric(init_dist_timepoint):
    #     raise TypeError(&#39;`init_dist_timepoint` ({}) must be a numeric&#39;.format(
    #         type(init_dist_timepoint)))

    GRAPH = chain.graph
    DATA = GRAPH.data
    SUBJSET = DATA.subjects
    ASVS = DATA.asvs
    logging.info(&#39;Number of ASVs ({})&#39;.format(len(ASVS)))

    n_days = -1
    for subj in SUBJSET:
        maxday = np.max(subj.times)
        if maxday &gt; n_days:
            n_days = maxday
    logging.info(&#39;Number of days: {}&#39;.format(n_days))

    GROWTH = GRAPH[STRNAMES.GROWTH_VALUE]
    SELF_INTERACTIONS = GRAPH[STRNAMES.SELF_INTERACTION_VALUE]
    INTERACTIONS = GRAPH[STRNAMES.INTERACTIONS_OBJ]
    CLUSTERING = GRAPH[STRNAMES.CLUSTERING_OBJ]
    PERTURBATIONS = GRAPH.perturbations
    perturbations_additive = GROWTH.perturbations_additive

    synth = SyntheticData(n_days=n_days,
        perturbations_additive=perturbations_additive)

    synth.set_asvs(asvs=DATA.asvs)
    synth.set_cluster_assignments(clusters=CLUSTERING.toarray())

    # Set the interactions
    # --------------------
    synth.dynamics.interactions = pl.Interactions(clustering=synth.dynamics.clustering, 
        use_indicators=True, G=synth.G)
    logging.info(&#39;Generating bayes factors&#39;)
    bayes_factors_asvs = INTERACTIONS.generate_bayes_factors_posthoc(
        prior=GRAPH[STRNAMES.CLUSTER_INTERACTION_INDICATOR].prior,
        section=&#39;posterior&#39;)
    logging.info(&#39;Getting values&#39;)
    cluster_interactions_asvs = pl.variables.summary(INTERACTIONS, set_nan_to_0=False,
        section=&#39;posterior&#39;, only=[&#39;mean&#39;])[&#39;mean&#39;]

    logging.info(&#39;Set the interactions&#39;)
    for interaction in synth.dynamics.interactions:

        # Get the target and source cluster index of the interaction
        tcidx = synth.dynamics.clustering.cid2cidx[interaction.target_cid]
        scidx = synth.dynamics.clustering.cid2cidx[interaction.source_cid]

        # Get a set of target and source asv indices of the interaction (it can
        # be any of them because we assume the chain was run with a fixed topology)
        taidx = list(CLUSTERING.clusters[CLUSTERING.order[tcidx]].members)[0]
        saidx = list(CLUSTERING.clusters[CLUSTERING.order[scidx]].members)[0]

        # If the bayes factor of the interaction is greater than the minimum bayes
        # factor, then we set the interaction. If it is less, then we set the 
        # indicator to false
        if bayes_factors_asvs[taidx, saidx] &gt; min_bayes_factor:
            interaction.value = cluster_interactions_asvs[taidx, saidx]
            interaction.indicator = True
        else:
            interaction.value = 0
            interaction.indicator = False

    # Set the perturbations
    # ---------------------
    _ps = []
    for PERTURBATION in PERTURBATIONS:
        perturbation = pl.contrib.ClusterPerturbation(
            start=PERTURBATION.start, end=PERTURBATION.end,
            G=synth.G, clustering=synth.dynamics.clustering)

        # values and bayes factors are on an asv level
        values = pl.variables.summary(PERTURBATION, section=&#39;posterior&#39;, 
            only=[&#39;mean&#39;])[&#39;mean&#39;]
        indicator_trace = ~np.isnan(PERTURBATION.get_trace_from_disk(section=&#39;posterior&#39;))
        bayes_factor = pl.variables.summary(indicator_trace, only=[&#39;mean&#39;])[&#39;mean&#39;]
        bayes_factor = bayes_factor/(1. - bayes_factor)
        bayes_factor = bayes_factor * (PERTURBATION.probability.prior.b.value + 1) / \
            (PERTURBATION.probability.prior.a.value + 1)

        cluster_order = synth.dynamics.clustering.order
        for cidx in range(len(CLUSTERING)):
            
            aidx = list(CLUSTERING.clusters[CLUSTERING.order[cidx]].members)[0]
            if bayes_factor[aidx] &lt; min_bayes_factor:
                perturbation.magnitude.value[cluster_order[cidx]] = 0
                perturbation.indicator.value[cluster_order[cidx]] = False
            else:
                perturbation.magnitude.value[cluster_order[cidx]] = \
                    values[aidx]
                perturbation.indicator.value[cluster_order[cidx]] = True

        _ps.append(perturbation)
    synth.dynamics.perturbations = _ps

    # Set the growth and self-interactions
    # ------------------------------------
    synth.dynamics.growth = pl.variables.summary(GROWTH, section=&#39;posterior&#39;, 
        only=[&#39;mean&#39;])[&#39;mean&#39;]
    synth.dynamics.self_interactions = pl.variables.summary(
        SELF_INTERACTIONS, section=&#39;posterior&#39;, only=[&#39;mean&#39;])[&#39;mean&#39;]

    # Set the timepoints if possible
    # ------------------------------
    if set_times:
        times = []
        for subj in SUBJSET:
            times = np.append(times, subj.times)
        times = np.sort(np.unique(times))
        synth.master_times = times

    # Set the initial distribution
    # ----------------------------
    # values = []
    # for subj in SUBJSET:
    #     if init_dist_timepoint in subj.times:

    #         idx = np.searchsorted(subj.times, init_dist_timepoint)

    #         matrix = subj.matrix()[&#39;abs&#39;]
    #         values = np.append(values, matrix[:,idx].ravel())
    #     else:
    #         logging.warning(&#39;Timepoint `{}` not in subject `{}` ({})&#39;.format(
    #             init_dist_timepoint, subj.name, subj.times))
    # values = np.asarray(values)
    # values = values[values &gt; 0]
    # logvalues = np.log(values)
    synth.init_dist = pl.variables.Uniform(low=init_dist_start, high=init_dist_end)

    return synth</code></pre>
</details>
</dd>
<dt id="mdsine2.synthetic.set_timepoints_without_timeseries"><code class="name flex">
<span>def <span class="ident">set_timepoints_without_timeseries</span></span>(<span>N, T_start, T_end, D, move_timepoints_if_fail=True, essential_timepoints=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Set time points for trajectory.</p>
<p>This is a direct implementation of the algorithm specified in the method
supplement.</p>
<p>We deterministically allocate timepoints such that the density over certain
time periods is proportionally higher than other time periods. The default
denisty is 1. Additionally, we choose timepoints such that:
t1 = set_timepoints_without_timeseries(N, &hellip;)
t2 = set_timepoints_without_timeseries(M, &hellip;)</p>
<pre><code>We have two time-series that have all the same parameters but
M &gt;= N, then t1 is a subset of t2.
</code></pre>
<p>Note that every subsection of the time series must have at least 1 point.</p>
<h2 id="essential-timepoints">Essential Timepoints</h2>
<p>Essential timepoints are timepoints that must be included.</p>
<h2 id="moving-timepoints-if-necessary">Moving Timepoints If Necessary</h2>
<p>Each interval needs to have at least one timepoint allocated to it (more if there
are additional essential timepoints). If by the end of the allocation process there is
no time points in one of the intervals, this will fail. To avoid failing - we can set
the flag <code>move_timepoints_if_fail</code> to True. This moves a timepoint allocated from
the most populated interval to the one/s with 0. If there are multiple timepoints
that have the same number of timepoints, then we use the earliest interval.</p>
<h2 id="example">Example</h2>
<p>D_size = [5,6,2,1,2,3,0] &gt;&gt;&gt; [5,5,2,1,2,3,1]
D_size = [5,6,2,0,2,3,0] &gt;&gt;&gt; [4,5,2,1,2,3,1]</p>
<h2 id="allocating-timepoints-proportionally">Allocating Timepoints Proportionally</h2>
<p>We add the timepoints proportionally to each interval. If there are extra timepoints
at the end of this step then we sequentially add them to the interval based on their
priority. From a high level, the highest density has the highest priority. If multiple
intervals have the same density, then we add the timepoints sequentially from left
to right to only the intervals that have the high priority. Exceptions are: If an
interval has no timepoints, it has the highest priority.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>N</code></strong> :&ensp;<code>int</code></dt>
<dd>Total number of timepoints to allocate</dd>
<dt><strong><code>T_start</code></strong>, <strong><code>T_end</code></strong> :&ensp;<code>float</code></dt>
<dd>These are the start and end times of the time-series, respectivelly.
<code>T_start</code> is inclusive and <code>T_end</code> is not inclusive.</dd>
<dt><strong><code>D</code></strong> :&ensp;<code>list(3-tuple), 3-tuple, None</code></dt>
<dd>These are the densities which we allocate to</dd>
<dt><strong><code>move_timepoints_if_fail</code></strong> :&ensp;<code>bool</code></dt>
<dd>If False, we raise an exception if there is an interval that does not have
a timepoint by the end of allocation. If True, we move around the timepoints
such that None of the intervals have no timepoints. We do this using the
algorithm described above.</dd>
<dt><strong><code>essential_timepoints</code></strong> :&ensp;<code>list, None</code></dt>
<dd>If not None, these are a list of numeric timepoints that we must include.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_timepoints_without_timeseries(N, T_start, T_end, D, move_timepoints_if_fail=True, 
    essential_timepoints=None):
    &#39;&#39;&#39;Set time points for trajectory.

    This is a direct implementation of the algorithm specified in the method
    supplement.

    We deterministically allocate timepoints such that the density over certain
    time periods is proportionally higher than other time periods. The default
    denisty is 1. Additionally, we choose timepoints such that:
        t1 = set_timepoints_without_timeseries(N, ...)
        t2 = set_timepoints_without_timeseries(M, ...)

        We have two time-series that have all the same parameters but
        M &gt;= N, then t1 is a subset of t2.

    Note that every subsection of the time series must have at least 1 point.

    Essential Timepoints
    --------------------
    Essential timepoints are timepoints that must be included.

    Moving timepoints if necessary
    ------------------------------
    Each interval needs to have at least one timepoint allocated to it (more if there
    are additional essential timepoints). If by the end of the allocation process there is 
    no time points in one of the intervals, this will fail. To avoid failing - we can set 
    the flag `move_timepoints_if_fail` to True. This moves a timepoint allocated from
    the most populated interval to the one/s with 0. If there are multiple timepoints 
    that have the same number of timepoints, then we use the earliest interval.
    Example:
        D_size = [5,6,2,1,2,3,0] &gt;&gt;&gt; [5,5,2,1,2,3,1]
        D_size = [5,6,2,0,2,3,0] &gt;&gt;&gt; [4,5,2,1,2,3,1]

    Allocating timepoints proportionally
    ------------------------------------
    We add the timepoints proportionally to each interval. If there are extra timepoints
    at the end of this step then we sequentially add them to the interval based on their
    priority. From a high level, the highest density has the highest priority. If multiple
    intervals have the same density, then we add the timepoints sequentially from left
    to right to only the intervals that have the high priority. Exceptions are: If an
    interval has no timepoints, it has the highest priority.

    Parameters
    ----------
    N : int
        Total number of timepoints to allocate
    T_start, T_end : float
        These are the start and end times of the time-series, respectivelly.
        `T_start` is inclusive and `T_end` is not inclusive.
    D : list(3-tuple), 3-tuple, None
        These are the densities which we allocate to
    move_timepoints_if_fail : bool
        If False, we raise an exception if there is an interval that does not have
        a timepoint by the end of allocation. If True, we move around the timepoints
        such that None of the intervals have no timepoints. We do this using the 
        algorithm described above.
    essential_timepoints : list, None
        If not None, these are a list of numeric timepoints that we must include.
    &#39;&#39;&#39;
    # Type and value checking
    if not pl.isbool(move_timepoints_if_fail):
        raise TypeError(&#39;`move_timepoints_if_fail` ({}) must be a bool&#39;.format( 
            type(move_timepoints_if_fail)))
    if not pl.isint(N):
        raise TypeError(&#39;`N` ({}) must be an int&#39;.format(type(N)))
    if N &lt;= 0:
        raise ValueError(&#39;`N` ({}) must be &gt; 0&#39;.format(N))
    if pl.istuple(D):
        D = [D]
    for ele in D:
        if not pl.istuple(ele):
            raise TypeError(&#39;Every element in `D` ({}) must be a tuple&#39;.format(
                type(ele)))
        if len(ele) != 3:
            raise ValueError(&#39;`Every element in `D` must be a tuple of length 3 ({})&#39;.format( 
                len(ele)))
        start, stop, density = ele
        if not pl.isnumeric(start):
            raise TypeError(&#39;`start` ({}) must be a numeric&#39;.format(type(start)))
        if start &lt; T_start or start &gt; T_end:
            raise ValueError(&#39;`start` ({}) out of range&#39;.format(start))
        if not pl.isnumeric(stop):
            raise TypeError(&#39;`stop` ({}) must be a numeric&#39;.format(type(stop)))
        if stop &lt;= T_start or stop &gt; T_end:
            raise ValueError(&#39;`stop` ({}) out of range&#39;.format(stop))
        if stop &lt;= start:
            raise ValueError(&#39;`stop` ({}) must be &gt; `start` ({})&#39;.format(stop, start))
        if not pl.isnumeric(density):
            raise TypeError(&#39;`density` ({}) must be a numeric&#39;.format(density))
        if density &lt; 1:
            raise ValueError(&#39;`density` ({}) must be &gt;= 1&#39;.format(density))
    
    # Make sure that all of the densities are disjoint to each other and that they
    # span the entire time-series
    start_there = False
    end_there = False
    for (s,e,d) in D:
        if s == T_start:
            start_there = True
        if e == T_end:
            end_there = True
    if not (start_there and end_there):
        raise ValueError(&#39;`D` ({}) does not include the start and end points&#39;.format(D))
    for i, (si,ei,di) in enumerate(D):
        si_there = si == T_start
        ei_there = ei == T_end
        for j, (sj, ej, dj) in enumerate(D):
            if i == j:
                continue
            if (si &gt;= sj and si &lt; ej) or (ei &gt; sj and ei &lt;= ej):
                raise ValueError(&#39;`{}`th interval ({}) is contained in the `{}`th interval ({})&#39;.format( 
                    i, (si,ei,di), j, (sj,ej,dj)))
            if not si_there:
                si_there = si == ej
            if not ei_there:
                ei_there = ei == sj
        if (not si_there) or (not ei_there):
            raise ValueError(&#39;In {}th interval, either `{}` or `{}` was not adjacent to any other &#39; \
                &#39;interval ({})&#39;.format(i, si, ei, D))
    total_length = 0
    for s,e,d in D:
        total_length += (e-s)
    if total_length != (T_end - T_start):
        raise ValueError(&#39;lengths ({}) and ({}) are not the same&#39;.format(total_length, (T_end-T_start)))

    if essential_timepoints is not None:
        if pl.isnumeric(essential_timepoints):
            essential_timepoints = [essential_timepoints]
        if not pl.isarray(essential_timepoints):
            raise TypeError(&#39;`essential_timepoints` ({}) must be an array&#39;.format(
                type(essential_timepoints)))
        for ele in essential_timepoints:
            if not pl.isnumeric(ele):
                raise TypeError(&#39;Each element in `essential_timepoints` ({} must be a numeric&#39;.format(
                    type(ele)))
            if ele &lt; T_start or ele &gt; T_end:
                raise ValueError(&#39;Timepoint ({}) in `essential_timepoints` out of range ({}, {})&#39;.format(
                    ele, T_start, T_end))
        essential_timepoints = np.unique(essential_timepoints)
        if len(essential_timepoints) &gt; N:
            raise ValueError(&#39;There cannot be more essential timepoints ({}) than times to allocate ({})&#39;.format(
                len(essential_timepoints), N))

    # Order D (by the start)
    D_new = []
    while len(D) &gt; 0:
        # Get the smallest start
        min_s = None
        min_i = None
        for i, (s,_,_) in enumerate(D):
            if min_s is None:
                min_s = s
                min_i = 0
                continue
            if s &lt; min_s:
                min_s = s
                min_i = i
        D_new.append(D.pop(min_i))
    D = D_new

    # Perform algorithm
    D_size = []
    for s,e,d in D:
        D_size.append(d * (e - s))
    D_total = np.sum(D_size)
    D_size = [int(s * N / D_total) for s in D_size] # casting to an int rounds down
    D_size = np.asarray(D_size, dtype=int)

    # Set the essential points
    min_D_size = []
    for s,e,_ in D:
        cnt = 1 # Must have at least 1 point
        if essential_timepoints is not None:
            for essential_point in essential_timepoints:
                if essential_point &gt;= s and essential_point &lt; e:
                    cnt += 1
        min_D_size.append(cnt)
    min_D_size = np.asarray(min_D_size, dtype=int)

    # Add in extra points if necessary
    if np.sum(D_size) != N:
        Nextra = N - np.sum(D_size)

        # First we check if there are any intervals that have less than necessary times
        while np.any(D_size &lt; min_D_size):
            if Nextra == 0:
                break
            # Get the first index
            j = np.where(D_size &lt; min_D_size)[0][0]
            D_size[j] += 1
            Nextra -= 1

        if Nextra &gt; 0:
            dmax = np.max([d for (_,_,d) in D])
            I = [] # interval indexes that have the highest density
            for i, (s,e,d) in enumerate(D):
                if d == dmax:
                    I.append(i)
            if len(I) == 1:
                D_size[I[0]] += Nextra
            else:
                j = 0
                while Nextra &gt; 0:
                    i = I[j]
                    D_size[i] += 1
                    Nextra -= 1
                    j += 1
                    if j == len(I):
                        j = 0
    
    # check if every interval has the minimum
    if np.any(D_size &lt; min_D_size):
        if move_timepoints_if_fail:
            while np.any(D_size &lt; min_D_size):
                logging.info(&#39;An interval was flagged to not have a time point allocated. Moved&#39;)
                # find the first instance of the max
                max_size = np.max(D_size)
                index_to_take = np.where(D_size == max_size)[0][0]
                
                # Find the first interval with less than the minimum
                index_to_give = np.where(D_size &lt; min_D_size)[0][0]

                D_size[index_to_give] += 1
                D_size[index_to_take] -= 1

                if D_size[index_to_take] &lt; min_D_size[index_to_take]:
                    print(&#39;D_size&#39;, D_size)
                    print(&#39;min_D_size&#39;, min_D_size)
                    print(&#39;index_to_take&#39;, index_to_take)
                    raise ValueError(&#39;This CAN happen but will only happen when there are a lot of essential&#39; \
                        &#39; timepoints and not a lot of timepoints in total. Increase N.&#39;)

        else:
            raise ValueError(&#39;An interval has less than the minimum required timepoints. &#39;\
                &#39;Must increase N ({}) or set the flag &#39; \
                &#39;`move_timepoints_if_fail` to True. D: {}, D_size: {}, min_D_size: {}&#39;.format(
                    N, D, D_size, min_D_size))

    # Set essential timepoints
    essential_D = [[s] for s,_,_ in D]
    if essential_timepoints is not None:
        for essential_point in essential_timepoints:
            # Find where this essential timepoint goes
            for j, (s,e,_) in enumerate(D):
                if essential_point &gt;= s and essential_point &lt; e:
                    # Check to see if essential_timepoint is == to the start. skip if it is
                    if essential_point not in essential_D[j]:
                        essential_D[j].append(essential_point)


    T_N = []
    for i, (si, ei, di) in enumerate(D):
        n = D_size[i]
        Ti = essential_D[i]

        if n &gt; 1:
            done = False
            while not done:
                Ttemp = []
                for k in range(len(Ti)-1):
                    if D_size[i] == (len(Ti) + len(Ttemp)):
                        break
                    newval = (Ti[k] + Ti[k+1])/2
                    Ttemp = np.append(Ttemp, newval)

                if D_size[i] == (len(Ti) + len(Ttemp)):
                    done = True
                else:
                    newval = (Ti[-1] + ei) / 2
                    Ttemp = np.append(Ttemp, newval)
                Ti = np.sort(np.append(Ti, Ttemp))
        T_N = np.append(T_N, Ti)

    return np.sort(T_N)</code></pre>
</details>
</dd>
<dt id="mdsine2.synthetic.subsample_timepoints"><code class="name flex">
<span>def <span class="ident">subsample_timepoints</span></span>(<span>times, N, required=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Subsample the timepoints <code>times</code> so that it has <code>N</code> timepoints.</p>
<p>If required is not None, it is a list of timepoints that must be
included (start/ends of perturbations, etc.) in the return times.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>times</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>An array of timepoints</dd>
<dt><strong><code>N</code></strong> :&ensp;<code>int</code></dt>
<dd>Total number of timepoints to be remaining</dd>
<dt><strong><code>required</code></strong> :&ensp;<code>None, np.ndarray</code></dt>
<dd>An array of timepoints that need to be included in the return array</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def subsample_timepoints(times, N, required=None):
    &#39;&#39;&#39;Subsample the timepoints `times` so that it has `N` timepoints.

    If required is not None, it is a list of timepoints that must be
    included (start/ends of perturbations, etc.) in the return times.

    Parameters
    ----------
    times : np.ndarray
        An array of timepoints
    N : int
        Total number of timepoints to be remaining
    required : None, np.ndarray
        An array of timepoints that need to be included in the return array
    &#39;&#39;&#39;
    if not pl.isarray(times):
        raise TypeError(&#39;`times` ({}) must be an array&#39;.format(type(times)))
    times = np.sort(np.array(times))

    if not pl.isint(N):
        raise TypeError(&#39;`N` ({}) must be an int&#39;.format(type(N)))
    if N &lt;= 1:
        raise ValueError(&#39;`N` ({}) must be &gt; 1&#39;.format(N))
    if required is not None:
        if not pl.isarray(required):
            raise TypeError(&#39;`required` ({}) must be an array&#39;.format(type(required)))
        for ele in required:
            if ele not in times:
                raise ValueError(&#39;({}) in `required` is not in times: ({})&#39;.format(ele,times))

        if N - len(required) &lt;= 0:
            raise ValueError(&#39;The number of required points ({}) is more than the total number &#39; \
                &#39;of points that need to remain ({})&#39;.format(len(required), N))

        add_at_end = []
        for ele in required:
            # get index at 
            idx = np.searchsorted(times, ele)
            add_at_end.append(ele)
            times = np.delete(times, [idx])
        N -= len(add_at_end)

    l = len(times)
    if N &gt; l/2:
        # These are the indices to take away
        idxs = np.arange(0, l, step=l/(l-N), dtype=int)
        a = np.delete(times, idxs)

    else:
        # These are the indicates to keep
        idxs = np.arange(0, l, step=l/N, dtype=int)
        a = times[idxs]

    if required is not None:
        a = np.sort(np.append(a, add_at_end))
    return a</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="mdsine2.synthetic.SyntheticData"><code class="flex name class">
<span>class <span class="ident">SyntheticData</span></span>
<span>(</span><span>n_days, perturbations_additive)</span>
</code></dt>
<dd>
<div class="desc"><p>This class is used to generate synthetic data using fixed
or sampled topologies. It also gives us the ability to convert
the internal representation into a <code>pylab.base.Study</code> so
we can use it in inference.</p>
<p>We add the parameters for the dynamical system to self.dynamics,
which we assume to be a Discretized Generalized Lotka-Voltera
Dynamics with clustered interactions and perturbations. The
dynamical class is specified in the module <code>model</code>.</p>
<p>NOTE: We cannot specify the dynamical class until we have our <code>asvs</code>
object so we wait to define it then, and it is done automatically
once we call the function <code>set_asvs</code></p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>n_days</code></strong> :&ensp;<code>numeric</code></dt>
<dd>Total length of days</dd>
<dt><strong><code>perturbations_additive</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, model the dynamics as additive. Else model it as multiplicative</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SyntheticData(pl.Saveable):
    &#39;&#39;&#39;This class is used to generate synthetic data using fixed
    or sampled topologies. It also gives us the ability to convert
    the internal representation into a `pylab.base.Study` so
    we can use it in inference.

    We add the parameters for the dynamical system to self.dynamics,
    which we assume to be a Discretized Generalized Lotka-Voltera 
    Dynamics with clustered interactions and perturbations. The 
    dynamical class is specified in the module `model`.

    NOTE: We cannot specify the dynamical class until we have our `asvs` 
    object so we wait to define it then, and it is done automatically 
    once we call the function `set_asvs`

    Parameters
    ----------
    n_days : numeric
        Total length of days
    perturbations_additive : bool
        If True, model the dynamics as additive. Else model it as multiplicative
    &#39;&#39;&#39;
    def __init__(self, n_days, perturbations_additive):
        self.G = pl.graph.Graph(name=&#39;synthetic&#39;) # make local graph
        self.data = []
        self.times = []
        self.dt = None
        self.dynamics = None
        self.n_replicates = 0
        self.perturbations_additive = perturbations_additive

        self.dt = None
        self.n_time_steps = None
        self.n_days = n_days

    @property
    def perturbations(self):
        try:
            return self.dynamics.perturbations
        except:
            raise pl.UndefinedError(&#39;You need to define `dynamics` ({}) first.&#39;.format(
                type(self.dynamics)))

    def get_full_interaction_matrix(self):
        &#39;&#39;&#39;Make the interaction matrix. If `with_interactions` is False,
        you&#39;re effectively only getting the self-interaction terms.
        &#39;&#39;&#39;
        A = self.dynamics.interactions.get_datalevel_value_matrix(set_neg_indicators_to_nan=False)
        for i in range(A.shape[0]):
            A[i,i] = -self.dynamics.self_interactions[i]
        return A

    def set_asvs(self, n_asvs=None, sequences=None, filename=None, asvs=None):
        &#39;&#39;&#39;Sets the ASVset. If you have a list of sequences you want to read in,
        pass in the list of `sequences` and it will create a new ASV for every sequence. 
        If you  just want to specify how many ASVs you want and you dont care about 
        sequences, then specify the number of ASVs with `n_asvs`. If there is an
        ASVSet saved on file, you can load it with the keyword `filename`.

        Defines the dynamics once done.

        Example:
            &gt;&gt;&gt; self.set_asvs(n_asvs=5)
            5 ASVs with random sequences

            &gt;&gt;&gt; seq = [&#39;AAAA&#39;, &#39;TTTT&#39;, &#39;GGGG&#39;, &#39;CCCC&#39;]
            &gt;&gt;&gt; self.set_asvs(sequences=seq)
            3 ASVs with the sequences specified above
            
            &gt;&gt;&gt; self.set_asvs(filename=&#39;pickles/test.pkl&#39;)
            Reads in the ASVSet saved at &#39;pickles/test.pkl&#39;

        Parameters
        ----------
        n_asvs : int, Optional
            How many ASVs you want. This is unnecessary if you specify a list of 
            sequences.
        sequences : array(str), Optional
            The sequence for each ASV. If you do not want any, dont specify anything
            and specify the number of ASVs. If nothing is provided it will create
            a random sequence of sequences for each ASV
        filename : str, Optional
            The filename to lead the ASV
        asvs : pylab.base.ASVSet
            This is an ASVSet object

        Returns
        -------
        pl.ASVSet
            This is the ASVSet that gets created
        &#39;&#39;&#39;
        a = n_asvs is not None
        b = sequences is not None
        c = filename is not None
        d = asvs is not None

        if a + b + c + d != 1:
            raise TypeError(&#39;Only one of `n_asvs` ({}), `sequences` ({}), &#39;\
                &#39;`filename` ({}), or asvs ({})  can be specified&#39;.format(
                type(n_asvs), type(sequences), type(filename), type(asvs)))
        
        if n_asvs is not None:
            if not pl.isint(n_asvs):
                raise TypeError(&#39;`n_asvs` ({}) must be an int&#39;.format(type(n_asvs)))
            self.asvs = pl.ASVSet()
            for i in range(n_asvs):
                name = &#39;ASV_{}&#39;.format(i)
                seq = &#39;&#39;.join(random.choices([&#39;A&#39;,&#39;T&#39;,&#39;G&#39;,&#39;C&#39;,&#39;U&#39;], k=50))
                self.asvs.add_asv(name=name, sequence=seq)
        elif sequences is not None:
            if not pl.isarray(sequences):
                raise TypeError(&#39;`sequences` ({}) must be an array&#39;.format(type(sequences)))
            if not np.all(pl.itercheck(sequences, pl.isstr)):
                raise TypeError(&#39;All elements in `sequences` must be strs: {}&#39;.format(
                    pl.itercheck(sequences, pl.isstr)))
            for i, seq in enumerate(sequences):
                name = &#39;ASV_{}&#39;.format(i)
                self.asvs.add_asv(name=name, sequence=seq)
        elif filename is not None:
            if not pl.isstr(filename):
                raise TypeError(&#39;`filename` ({}) must be a str&#39;.format(type(filename)))
            self.asvs = pl.ASVSet.load(filename)
        else:
            self.asvs = asvs

        self.dynamics = model.gLVDynamicsSingleClustering(asvs=self.asvs, 
            perturbations_additive=self.perturbations_additive)
        
    def set_cluster_assignments(self, clusters=None, n_clusters=None, evenness=None):
        &#39;&#39;&#39;Create clusters for the interactions and perturbations. If you have the cluster
        assignments already, set them with `clusters`. else we can randomly generate them 
        with the parameters `n_clusters` and `evenness`.

        Parameters
        ----------
        clusters : list(list(int))
            These are the cluster assignments of the ASVs
        n_clusters : int
            Number of clusters to create
        evenness : str, 1 or 2-dim array
            How to initialize the clusters. This can be generated automatically or by 
            reading in a similarity matrix.
            If it is a str:
                &#39;even&#39;: Have each cluster have as close to even number of clusters as possible
                &#39;heavy-tail&#39;: TODO : NOT IMPLEMENTED
                &#39;sequence&#39; : TODO : NOT IMPLEMENTED (given the sequences, make an adjacency matrix)
            If it is an array:
                If 1-dim
                    This is how you assign the clusters by index to each cluster. This is the 
                    initialization format for pylab.cluster.Clustering
                If it is 2-dim
                    This is a 2 dimensional DISTANCE matrix. It will build the cluster 
                    assignments given this distance matrix

        Returns
        -------
        pl.cluster.Clustering
            This is the clustering object that gets created

        See also
        --------
        pylab.cluster.Clustering.__init__
        &#39;&#39;&#39;
        if self.asvs is None:
            raise ValueError(&#39;Must specify the ASVSet before by calling `self.set_asvs`&#39;)

        if clusters is None:
            if not pl.isint(n_clusters):
                raise TypeError(&#39;`n_clusters` ({}) must be an int&#39;.format(n_clusters))

            clusters = []
            start = 0
            if pl.isstr(evenness):
                if evenness == &#39;even&#39;:
                    size = int(len(self.asvs)/n_clusters)
                    for _ in range(n_clusters-1):
                        clusters.append(np.arange(start,start+size, dtype=int))
                        start += size
                    clusters.append(np.arange(start, len(self.asvs), dtype=int))
                elif evenness == &#39;sequence&#39;:
                    logging.info(&#39;Making affinity matrix from sequences&#39;)
                    evenness = np.diag(np.ones(len(self.asvs), dtype=float))

                    for i in range(len(self.asvs)):
                        for j in range(len(self.asvs)):
                            if j &lt;= i:
                                continue
                            # Subtract because we want to make a similarity matrix
                            dist = 1-diversity.beta.hamming(
                                list(self.asvs[i].sequence), list(self.asvs[j].sequence))
                            evenness[i,j] = dist
                            evenness[j,i] = dist

                    # print(evenness)

                elif evenness == &#39;heavy-tail&#39;:
                    raise NotImplementedError(&#39;`heavy-tail` not implemented yet&#39;)
                else:
                    raise ValueError(&#39;cluster evenness ({}) not recognized&#39;.format(evenness))
            if pl.isarray(evenness):
                evenness = np.asarray(evenness)
                if evenness.ndim == 1:
                    clusters = evenness.tolist()
                elif evenness.ndim == 2:
                    if evenness.shape[0] != evenness.shape[1]:
                        raise ValueError(&#39;Must be a square matrix&#39;)
                    if evenness.shape[0] != len(self.asvs):
                        raise ValueError(&#39;Length of the side ({}) must be the same as the number of ASVs ({})&#39;.format(
                            evenness.shape[0], len(self.asvs)))
                    
                    c = AgglomerativeClustering(
                        n_clusters=n_clusters,
                        affinity=&#39;precomputed&#39;,
                        linkage=&#39;average&#39;)
                    assignments = c.fit_predict(evenness)
                    clusters = {}
                    for oidx,cidx in enumerate(assignments):
                        if cidx not in clusters:
                            clusters[cidx] = []
                        clusters[cidx].append(oidx)
                    clusters = [val for val in clusters.values()]
                else:
                    raise ValueError(&#39;`evenness` ({}) must be a 1 or 2-dimensional array&#39;.format(
                        evenness.ndim))
            else:
                raise TypeError(&#39;`evenness` ({}) must be either a string or an array&#39;.format(
                    type(evenness)))

        # Initialize clustering object
        logging.info(&#39;cluster assignments: {}&#39;.format(clusters))
        self.dynamics.clustering = pl.Clustering(clusters = clusters, items=self.asvs, G=self.G)
        return self

    def shuffle_cluster_assignments(self, p):
        &#39;&#39;&#39;Shuffle the cluster assignments that were specified in `set_cluster_assignments`.

        `p` indicates what proportion of the ASVs to be reassigned. Example: `p=.1` means
        that you want to shuffle 10% of the ASVs.

        NOTE: THIS SHOULD BE CALLED BEFORE YOU CALL THE FUNCTION `sample_dynamics`.

        Parameters
        ----------
        p : float
            Proportion of the ASVs to be shuffled
        &#39;&#39;&#39;
        if self.dynamics.clustering is None:
            raise ValueError(&#39;Must specfiy the clusters before you call this function&#39;)
        if not pl.isnumeric(p):
            raise TypeError(&#39;`p` ({}) should be a numeric&#39;.format(type(p)))
        if p &lt; 0 or p &gt; 1:
            raise ValueError(&#39;`p` ({}) should be 0 =&lt; p =&lt; 1&#39;.format(p))

        n = int(p*len(self.asvs))
        oidxs = np.random.randint(len(self.asvs), size=n)

        logging.info(&#39;ASV indices to shuffle: {}&#39;.format(oidxs))

        for oidx in oidxs:
            curr_cid = self.dynamics.clustering.idx2cid[oidx]
            assigned_cid = curr_cid
            while assigned_cid == curr_cid:
                assigned_cid = random.choice(self.dynamics.clustering.order)

            self.dynamics.clustering.move_item(idx=oidx, cid=assigned_cid)

        logging.info(&#39;new cluster assignments: {}&#39;.format(self.dynamics.clustering.toarray()))

    def sample_single_perturbation(self, start, end, prob_pos, prob_affect, prob_strength, 
        mean_strength, std_strength):
        &#39;&#39;&#39;Sample a perturbation to add to the system.

        If there are no clusters that are selected, we sample again until we get at least 1.

        Defaults for strength parameters
        --------------------------------
        `prob_strength = [0.2, 0.4, 0.4]`
        `mean_strength   = [0.5, 1.0, 2.0]`
        `std_strength  = 0.1`

        Parameters
        ----------
        start : float
            - Time to start the perturbation.
        end : float
            - Time to end the perturbation
        pob_pos : float, [0,1]
            - This is the probability that the perturbation is going to be positive
            - Sampled from a Bernoulli distribution
        mean_strength : array
            These are the means of the magnitudes of the perturbations to sample around
        prob_strength : array
            These are the probabilities to sample a mean magnitude of the perturbation
        std_strength : float
            This is the standard deviation to sample the magnitude of the perturbation
        prob_affect : float ([0,1]), str
            - This is the probability it will affect a cluster (positive indicator)
            - Sampled from a Bernoulli distribution
            - If str, then only that number of clusters is set.
              Example:
                &#39;1&#39; means only one cluster is set

        Returns
        -------
        pylab.contrib.ClusterPerturbation
            This is the cluster perturbation that was sampled
        &#39;&#39;&#39;
        if self.dynamics.clustering is None:
            raise TypeError(&#39;Clustering module is None, this is likely because you did &#39; \
                &#39;not set the dynamics&#39;)

        # Check variables
        if not pl.isnumeric(start):
            raise TypeError(&#39;`start` ({}) must be a numeric&#39;.format(type(start)))
        if not pl.isnumeric(end):
            raise TypeError(&#39;`end` ({}) must be a numeric&#39;.format(type(end)))
        if end &lt;= start:
            raise ValueError(&#39;`start` ({}) must be strictly smaller than `end` ({})&#39;.format(
                start,end))
        if not pl.isfloat(prob_pos):
            raise TypeError(&#39;`prob_pos` ({}) must be a numeric&#39;.format(type(prob_pos)))
        elif prob_pos &lt; 0 or prob_pos &gt; 1:
            raise ValueError(&#39;`prob_pos` ({}) must be [0,1]&#39;.format(prob_pos))
        if pl.isstr(prob_affect):
            set_num = True
            try:
                pa = int(prob_affect)
            except:
                logging.critical(&#39;Cannot cast `prob_affect` ({}) as an int&#39;.format(prob_affect))
                raise
            if pa &lt; 0:
                raise ValueError(&#39;`prob_affect` ({}) must be &gt; 0&#39;.format(prob_affect))
            if pa &gt; len(self.dynamics.clustering.clusters):
                raise ValueError(&#39;`prob_affect` ({}) must be less than n_clusters&#39;.format(prob_affect))
        else:
            set_num = False
            pa = None
            if not pl.isfloat(prob_affect):
                raise TypeError(&#39;`prob_affect` ({}) must be a numeric&#39;.format(type(prob_affect)))
            elif prob_affect &lt; 0 or prob_affect &gt; 1:
                raise ValueError(&#39;`prob_affect` ({}) must be [0,1]&#39;.format(prob_affect))
        # check the strengths
        if not pl.isarray(prob_strength):
            raise TypeError(&#39;`prob_strength` ({}) be an array&#39;.format(type(prob_strength)))
        for ele in prob_strength:
            if not pl.isfloat(ele):
                raise TypeError(&#39;`every element in `prob_strength` ({}) must be a float&#39;.format( 
                    type(ele)))
            if ele &lt;= 0:
                raise ValueError(&#39;`every probability in `prob_strength` ({}) must be &gt; 0&#39;.format(ele))
        if not pl.isarray(mean_strength):
            raise TypeError(&#39;`mean_strength` ({}) must be an array&#39;.format(type(mean_strength)))
        for ele in mean_strength:
            if not pl.isnumeric(ele):
                raise TypeError(&#39;Every element in `mean_strength` ({}) must be a numeric&#39;.format(ele))
            if ele &lt; 0:
                raise ValueError(&#39;Every element in `mean_strength` ({}) must be positive.&#39; \
                    &#39; If you want a perturbation to have a negative magnitude then set the &#39; \
                    &#39;`prob_pos` parameter to a very low nuber&#39;.format(ele))
        if len(mean_strength) != len(prob_strength):
            raise ValueError(&#39;`mean_strength` ({}) and `prob_strength` ({}) must have the same number&#39; \
                &#39; of elements&#39;.format(len(mean_strength), len(prob_strength)))
        if not pl.isnumeric(std_strength):
            raise TypeError(&#39;`std_strength` ({}) must be a numeric&#39;.format(type(std_strength)))
        if std_strength &lt;= 0:
            raise ValueError(&#39;`std_stregnth` ({}) must be &gt; 0&#39;.format(std_strength))

        # Set indicator at the ASV level
        if set_num:
            # Pick the number of clusters on
            order = copy.deepcopy(self.dynamics.clustering.order)
            order = list(order)
            indicator = np.zeros(len(order), dtype=bool)

            # pick the cids to set to true
            while pa &gt; 0:
                # pick a cluster
                idx = npr.randint(0, len(order))
                indicator[self.dynamics.clustering.cid2cidx[order[idx]]] = True
                order.pop(idx)
                pa -= 1
        else:
            i = 0
            while True:
                indicator = np.zeros(len(order), dtype=bool)
                for cidx in range(len(self.dynamics.clustering.clusters)):
                    indicator[cidx] = bool(pl.random.bernoulli.sample(prob_affect))
                if np.sum(indicator) &gt; 0:
                    break
                if i == 1000:
                    raise ValueError(&#39;Assigning cluster ids failed 1000 times. set `prob_affect` {}&#39; \
                        &#39; to a larger number&#39;.format(prob_affect))
                i += 1

        magnitude = np.zeros(len(self.dynamics.clustering), dtype=float)
        for i,ind in enumerate(indicator):
            if ind:
                # sample the magnitude
                sign = pl.random.bernoulli.sample(prob_pos) * 2 - 1
                mean = np.random.choice(mean_strength, p=prob_strength)
                magnitude[i] = pl.random.normal.sample(mean=mean*sign, 
                    std=std_strength)

        a = pl.contrib.ClusterPerturbation(start=start, end=end, magnitude=magnitude,
            clustering=self.dynamics.clustering, indicator=indicator, G=self.G)
        
        logging.info(&#39;Perturbation:\n\tstart,end ({},{})\n\t&#39; \
            &#39;magnitude {}\n\tindicator {}&#39;.format(start,end,a.cluster_array(),
            indicator))

        if self.dynamics.perturbations is None:
            self.dynamics.perturbations = [a]
        else:
            self.dynamics.perturbations.append(a)
        return a

    def set_single_perturbation(self, start, end, magnitude, indicator):
        &#39;&#39;&#39;Sets a single perturbation - no sampling. This assumes that you have
        initialized the system - (`clustering` is not None)

        Parameters
        ----------
        start, end (float)
            - Time to start/end the perturbation
        magnitude (float)
            - This is the strength of the perturbation
        indicator (int, np.ndarray)
            - If it is an int, this is the cluster id that it is positive for
            - If it is an array, it must either be the length of the
              number of clusters, and must be either a boolean or 1,0s
        &#39;&#39;&#39;
        if self.dynamics.clustering is None:
            raise ValueError(&#39;Must sample the system before you call this function&#39;)
        a = pl.contrib.ClusterPerturbation(start=start, end=end, magnitude=magnitude,
            indicator=indicator, clustering=self.dynamics.clustering, G=self.G)
        if self.dynamics.perturbations is None:
            self.dynamics.perturbations = [a]
        else:
            self.dynamics.perturbations.append(a)
        return a

    def icml_topology(self, n_asvs=13, max_abundance=None):
        &#39;&#39;&#39;Recreate the dynamical system used in the ICML paper [1]. If you use the 
        default parameters you will get the same interaction matrix that was used in [1].

        We rescale the self-interactions and the interactions (and potentially growth) by 150
        so that the time-scales of the trajectories happen over days instead of minutes

        Parameters
        ----------
        n_asvs : int
            These are how many ASVs to include in the system. We will always have 3 clusters and
            the proportion of ASVs in each cluster is as follows:
                cluster 1 - 5/13
                cluster 2 - 6/13
                cluster 3 - 2/13
            We assign each ASV to each cluster the best we can, any extra ASVs we put into cluster 3
        max_abundance : numeric, None
            This is the abundance to set the maximum. All of the other 
            parameters change proportionally. If `None` then we assume no change.
        &#39;&#39;&#39;
        if not pl.isint(n_asvs):
            raise TypeError(&#39;`n_asvs` ({}) must be an int&#39;.format(type(n_asvs)))
        if n_asvs &lt; 3:
            raise ValueError(&#39;`n_asvs` ({}) must be &gt;= 3&#39;.format(n_asvs))
        if max_abundance is not None:
            if not pl.isnumeric(max_abundance):
                raise TypeError(&#39;`max_abundance` ({}) must be a numeric&#39;.format(type(max_abundance)))
            if max_abundance &lt;= 0:
                raise ValueError(&#39;`max_abundance` ({}) must be &gt; 0&#39;.format(max_abundance))
        
        # Generate ASVs
        self.set_asvs(n_asvs)

        # Make cluster assignments with the approximate proportions
        c0size = int(5*n_asvs/13)
        c1size = int(6*n_asvs/13)
        c2size = int(n_asvs - c0size - c1size)

        frac = 150*n_asvs/13

        clusters = [
            np.arange(0, c0size, dtype=int).tolist(), 
            np.arange(c0size, c0size+c1size, dtype=int).tolist(),
            np.arange(c0size+c1size, c0size+c1size+c2size, dtype=int).tolist()]

        self.dynamics.clustering = pl.Clustering(clusters=clusters, items=self.asvs, G=self.G)
        self.dynamics.interactions = pl.Interactions(clustering=self.dynamics.clustering, 
            use_indicators=True, G=self.G)

        c0 = self.dynamics.clustering.order[0]
        c1 = self.dynamics.clustering.order[1]
        c2 = self.dynamics.clustering.order[2]
        for interaction in self.dynamics.interactions:
            if interaction.target_cid == c0 and interaction.source_cid == c1:
                interaction.value = 3/frac
                interaction.indicator = True
            elif interaction.target_cid == c0 and interaction.source_cid == c2:
                interaction.value = -1/frac
                interaction.indicator = True
            elif interaction.target_cid == c2 and interaction.source_cid == c0:
                interaction.value = 2/frac
                interaction.indicator = True
            elif interaction.target_cid == c2 and interaction.source_cid == c1:
                interaction.value = -4/frac
                interaction.indicator = True
            else:
                interaction.value = 0
                interaction.indicator = False
        
        self.dynamics.growth = pl.random.uniform.sample(low=18, high=22, size=n_asvs)/150
        # self.dynamics.growth = pl.random.uniform.sample(.1, 0.6, size=n_asvs)
        # self.dynamics.growth = pl.random.uniform.sample(low=.1 + 0.2, high=math.log(10) - 0.2, size=n_asvs)
        # self.dynamics.growth = pl.random.uniform.sample(low=0.12, high=2, size=n_asvs)
        self.dynamics.self_interactions = np.ones(n_asvs, dtype=float)*(5)/150
        # self.dynamics.self_interactions = pl.random.uniform.sample(-.05/150, -50/150, size=n_asvs)

        if max_abundance is not None:
            # self.dynamics.growth = pl.random.uniform.sample(low=.1, high=math.log(10), size=n_asvs)
            # self.dynamics.growth = pl.random.uniform.sample(low=.21, high=.5, size=n_asvs)/2
            # rescale the abundance such that hte max the max is ~max_abundance
            frac = 25/max_abundance
            self.dynamics.self_interactions *= frac
            for interaction in self.dynamics.interactions:
                if interaction.indicator:
                    interaction.value *= frac
    
    def icml_topology_real(self, n_asvs=13, max_abundance=None, scale_interaction=None):
        &#39;&#39;&#39;Recreate the dynamical system used in the ICML paper [1]. If you use the 
        default parameters you will get the same interaction matrix that was used in [1].

        We rescale the self-interactions and the interactions (and potentially growth) by 150
        so that the time-scales of the trajectories happen over days instead of minutes

        Parameters
        ----------
        n_asvs : int
            These are how many ASVs to include in the system. We will always have 3 clusters and
            the proportion of ASVs in each cluster is as follows:
                cluster 1 - 5/13
                cluster 2 - 6/13
                cluster 3 - 2/13
            We assign each ASV to each cluster the best we can, any extra ASVs we put into cluster 3
        max_abundance : numeric, None
            This is the abundance to set the maximum. All of the other 
            parameters change proportionally. If `None` then we assume no change.
        &#39;&#39;&#39;
        if not pl.isint(n_asvs):
            raise TypeError(&#39;`n_asvs` ({}) must be an int&#39;.format(type(n_asvs)))
        if n_asvs &lt; 3:
            raise ValueError(&#39;`n_asvs` ({}) must be &gt;= 3&#39;.format(n_asvs))
        if max_abundance is not None:
            if not pl.isnumeric(max_abundance):
                raise TypeError(&#39;`max_abundance` ({}) must be a numeric&#39;.format(type(max_abundance)))
            if max_abundance &lt;= 0:
                raise ValueError(&#39;`max_abundance` ({}) must be &gt; 0&#39;.format(max_abundance))
        if scale_interaction is None:
            scale_interaction = 1
        # Generate ASVs
        self.set_asvs(n_asvs)

        # Make cluster assignments with the approximate proportions
        c0size = int(5*n_asvs/13)
        c1size = int(6*n_asvs/13)
        c2size = int(n_asvs - c0size - c1size)

        frac = 150*n_asvs/13

        clusters = [
            np.arange(0, c0size, dtype=int).tolist(), 
            np.arange(c0size, c0size+c1size, dtype=int).tolist(),
            np.arange(c0size+c1size, c0size+c1size+c2size, dtype=int).tolist()]

        self.dynamics.clustering = pl.Clustering(clusters=clusters, items=self.asvs, G=self.G)
        self.dynamics.interactions = pl.Interactions(clustering=self.dynamics.clustering, 
            use_indicators=True, G=self.G)

        c0 = self.dynamics.clustering.order[0]
        c1 = self.dynamics.clustering.order[1]
        c2 = self.dynamics.clustering.order[2]
        for interaction in self.dynamics.interactions:
            if interaction.target_cid == c0 and interaction.source_cid == c1:
                interaction.value = scale_interaction*3/frac
                interaction.indicator = True
            elif interaction.target_cid == c0 and interaction.source_cid == c2:
                interaction.value = scale_interaction*(-1)/frac
                interaction.indicator = True
            elif interaction.target_cid == c2 and interaction.source_cid == c0:
                interaction.value = scale_interaction*2/frac
                interaction.indicator = True
            elif interaction.target_cid == c2 and interaction.source_cid == c1:
                interaction.value = scale_interaction*(-4)/frac
                interaction.indicator = True
            else:
                interaction.value = 0
                interaction.indicator = False
        
        # self.dynamics.growth = pl.random.uniform.sample(low=18, high=22, size=n_asvs)/150
        self.dynamics.growth = pl.random.uniform.sample(0.5, 1.5, size=n_asvs)
        # self.dynamics.growth = pl.random.uniform.sample(low=.1 + 0.2, high=math.log(10) - 0.2, size=n_asvs)
        self.dynamics.self_interactions = np.ones(n_asvs, dtype=float)*(5)/150
        # self.dynamics.self_interactions = pl.random.uniform.sample(-.05/150, -50/150, size=n_asvs)

        if max_abundance is not None:
            # self.dynamics.growth = pl.random.uniform.sample(low=.1, high=math.log(10), size=n_asvs)
            # self.dynamics.growth = pl.random.uniform.sample(low=.21, high=.5, size=n_asvs)/2
            # rescale the abundance such that hte max the max is ~max_abundance
            frac = 25/max_abundance
            self.dynamics.self_interactions *= frac
            for interaction in self.dynamics.interactions:
                if interaction.indicator:
                    interaction.value *= frac

    def icml_perturbations(self, starts, ends):
        &#39;&#39;&#39;Set the perturbations. Made to be informative. The `starts` and
        `end` are starts and ends of 3 perturbations
        &#39;&#39;&#39;
        if self.dynamics.clustering is None:
            raise TypeError(&#39;Clustering module is None, this is likely because you did &#39; \
                &#39;not set the dynamics&#39;)

        if not pl.isarray(starts):
            raise TypeError(&#39;`starts` ({}) must be an array&#39;.format(type(starts)))
        if len(starts) != 3:
            raise ValueError(&#39;`starts` ({}) must be 3 elements long&#39;.format(len(starts)))
        for ele in starts:
            if not pl.isnumeric(ele):
                raise TypeError(&#39;Each element in `starts` ({}) must be an array ({})&#39;.format(
                    ele, starts))
        if not pl.isarray(ends):
            raise TypeError(&#39;`ends` ({}) must be an array&#39;.format(type(ends)))
        if len(ends) != 3:
            raise ValueError(&#39;`ends` ({}) must be 3 elements long&#39;.format(len(ends)))
        for ele in ends:
            if not pl.isnumeric(ele):
                raise TypeError(&#39;Each element in `ends` ({}) must be an array ({})&#39;.format(
                    ele, ends))
        for idx in range(len(starts)):
            start = starts[idx]
            end = ends[idx]

            if end &lt;= start:
                raise ValueError(&#39;`start` ({}) must be smaller than `end` ({})&#39;.format(
                    start,end))

        # Set perturbations
        c0 = self.dynamics.clustering.order[0]
        c1 = self.dynamics.clustering.order[1]
        c2 = self.dynamics.clustering.order[2]

        # Set perturbation 0
        if self.perturbations_additive:
            magnitude0 = np.asarray([
                    # pl.random.normal.sample(mean=-1, std=0.1),
                    0, pl.random.normal.sample(mean=0.5, std=0.1), 0])
        else:
            magnitude0 = np.asarray([
                    # pl.random.normal.sample(mean=-1, std=0.1),
                    0, pl.random.normal.sample(mean=1, std=0.1), 0])
        indicator0 = np.array([False, True, False], dtype=bool)
        p0 = pl.contrib.ClusterPerturbation(start=starts[0], end=ends[0], 
            magnitude=magnitude0, indicator=indicator0, G=self.G, 
            clustering=self.dynamics.clustering)

        # Set perturbation 1
        if self.perturbations_additive:
            magnitude1 = np.asarray([
                    pl.random.normal.sample(mean=1, std=0.1), 0,
                    pl.random.normal.sample(mean=-2, std=0.1)])
        else:
            magnitude1 = np.asarray([
                    pl.random.normal.sample(mean=1, std=0.1), 0,
                    pl.random.normal.sample(mean=-2, std=0.1)])
        indicator1 = np.array([True, False, True], dtype=bool)
        p1 = pl.contrib.ClusterPerturbation(start=starts[1], end=ends[1], 
            magnitude=magnitude1, indicator=indicator1, G=self.G, 
            clustering=self.dynamics.clustering)

        # Set perturbation 2
        if self.perturbations_additive:
            magnitude2 = np.asarray([
                    0, pl.random.normal.sample(mean=-0.5, std=0.1),
                    pl.random.normal.sample(mean=1, std=0.1)])
        else:
            magnitude2 = np.asarray([
                    0, pl.random.normal.sample(mean=-0.5, std=0.1),
                    pl.random.normal.sample(mean=1, std=0.1)])
        indicator2 = np.array([False, True, True], dtype=bool)
        p2 = pl.contrib.ClusterPerturbation(start=starts[2], end=ends[2], 
            magnitude=magnitude2, indicator=indicator2, G=self.G, 
            clustering=self.dynamics.clustering)

        self.dynamics.perturbations = [p0,p1,p2]

    def set_timepoints(self, times):
        &#39;&#39;&#39;Times to set the timepoints

        Parameters
        ----------
        times : np.ndarray
        &#39;&#39;&#39;
        if not pl.isarray(times):
            raise TypeError(&#39;`times` ({}) must be an array&#39;.format(times))
        times = np.sort(np.array(times))
        self.master_times = times
        return self

    def set_times_without_timeseries(self, N, D=&#39;auto&#39;, initial_growth=4, pretransition=1, 
        posttransition=2, transition_density=2, uniform_sampling=False):
        &#39;&#39;&#39;Set the time points for the replicates. 
        
        It is highly recommended that you call this function to set the timepoints
        of the synthetic trajectories instead of directly calling `set_timepoints_without_timeseries` 
        or setting them yourself.

        Types of time setting
        ---------------------
        There are three ways we can set the spacing of the timepoints: (1) We can
        set them with a uniform spacing by setting the parameter `uniform_spacing=True`.
        (2) We can space the timepoints non-uniformly using the algorithm 
        `synthetic.set_timepoints_without_timeseries`. (3) We can manually set the times with the parameter 
        `N` if `N` is an array. In terms of precidence of the parameters:
            (1) If `N` is an array, then we set them according to times
            (2) If `uniform_sampling = True`, then we ignore the parameters for the
                non-uniform
            (3) If `uniform_sampling = False`, then we do non-uniform sampling
 
        Non-uniform sampling
        --------------------
        We set the times according to the denisty intervals sepcified
        in D. If D is auto, we set the density to be `transition_density` 
        more intense than regular time points in the following scenarios:
            - For `initial_growth` days from the start of the trajectory
            - For `pretransition` days before a transition from off- and
              on- a perturbation (starting or ending of a perturbation)
            - For `posttransition` days after a transition from off- and
              on- a perturbation (starting or ending of a perturbation)

        Parameters
        ----------
        N : int, array
            If an int, it represents how many timepoints to allocate. Look at 
            `set_timepoints_without_timeseries` for more information.
            If an array, these are the times to set.
        D : str, 3-tuple, list(3-tuple)
            If this is a string, then we set it according to the densities
            specified above. Otherwise, these are a list of intervals that
            we want specific densities for. If all of the densities are not
            adjacent then we add in intervals in between with a density of 1.
            None of the densities listed in D can overlap. For options on 
        initial_growth : numeric, None
            How many days to double sample during the first `initial_growth` days.
            If None then we do not set the initial growth
        pretransition, posttransition : numeric
            How many days before and after, respectively, of a perturbation 
            transition to have double density. If None then we do not set it.
        transition_density : numeric
            How dense to make the higher densities (initial growth and transition
            times). Must be &gt;= 1

        See also
        --------
        synthetic.set_timepoints_without_timeseries
        &#39;&#39;&#39;
        if pl.isarray(N):
            # Set times manually
            N = np.sort(np.unique(N))
            if np.any(N &lt; 0):
                raise ValueError(&#39;Every value in `times` ({}) must be positive or 0&#39;.format(N))
            if np.any(N &gt; self.n_days):
                raise ValueError(&#39;Values in `N` ({}) out of range&#39;.format(N))
            self.master_times = N
            return
            
        if not pl.isbool(uniform_sampling):
            raise TypeError(&#39;`uniform_sampling` ({}) must be a bool&#39;.format(type(uniform_sampling)))
        
        if uniform_sampling:
            if not pl.isint(N):
                raise TypeError(&#39;`N` ({}) must be an int&#39;.format(type(N)))
            if N &lt;= 0:
                raise ValueError(&#39;`N` ({}) must be &gt; 0&#39;.format(N))
            ts = np.arange(0,self.n_days, step=self.n_days/N)
            for i in range(len(ts)):
                ts[i] = round(ts[i], 2)
            self.master_times = ts
            return

        # Else do non-uniform sampling
        if D == &#39;auto&#39;:
            if not pl.isnumeric(transition_density):
                raise TypeError(&#39;`transition_density` ({}) must be a numeric&#39;.format(
                    type(transition_density)))
            if transition_density &lt; 1:
                raise TypeError(&#39;`transition_density` ({}) must be &gt;= 1&#39;.format(
                    transition_density))
            if initial_growth is not None:
                if not pl.isnumeric(initial_growth):
                    raise TypeError(&#39;`initial_growth` ({}) should be a numeric&#39;.format( 
                        type(initial_growth)))
                if initial_growth &lt; 0:
                    raise ValueError(&#39;`initial_growth` ({}) must be positive&#39;.format(
                        initial_growth))
            if pretransition is not None:
                if not pl.isnumeric(pretransition):
                    raise TypeError(&#39;`pretransition` ({}) should be a numeric&#39;.format( 
                        type(pretransition)))
                if pretransition &lt; 0:
                    raise ValueError(&#39;`pretransition` ({}) must be positive&#39;.format(
                        pretransition))
            if posttransition is not None:
                if not pl.isnumeric(posttransition):
                    raise TypeError(&#39;`posttransition` ({}) should be a numeric&#39;.format( 
                        type(posttransition)))
                if posttransition &lt; 0:
                    raise ValueError(&#39;`posttransition` ({}) must be positive&#39;.format(
                        posttransition))

            # Sort perturbations if there are
            if self.dynamics.perturbations is not None:
                perts = []
                pert_starts = []
                for perturbation in self.dynamics.perturbations:
                    perts.append((perturbation.start, perturbation.end))
                    pert_starts.append(perturbation.start)
                
                idxs = np.argsort(pert_starts)
                temp = []
                for idx in idxs:
                    temp.append(perts[idx])
                perts = temp

                # fail if the start or end of a perturbation is greater than n_days
                for s,e in perts:
                    if s &gt;= self.n_days or e &gt; self.n_days:
                        raise ValueError(&#39;Perturbation start and end (`{}`,`{}`) &#39; \
                            &#39; is out of range for the number of days `{}`&#39;.format( 
                                s,e,self.n_days))
            else:
                perts = None

            # Add the densities in order
            D = []
            
            # Set initial growth
            l = np.min([initial_growth, self.n_days])
            D.append((0, l, transition_density))

            # Set for each perturbation:
            if perts is not None:
                for s, e in perts:
                    D.append((s-pretransition, s+posttransition, transition_density))
                    l = np.min([e+posttransition,self.n_days])
                    D.append((e-pretransition, l, transition_density))
        else:
            # check D
            if pl.istuple(D):
                D = [D]
            if not pl.isarray(D):
                raise TypeError(&#39;`D` ({}) must be an array&#39;.format(type(D)))
            for ele in D:
                if not pl.istuple(ele):
                    raise ValueError(&#39;Each element in D ({}) must be a tuple&#39;.format(
                        type(ele)))
                if len(ele) != 3:
                    raise ValueError(&#39;Each element in D must have 3 elements ({})&#39; \
                        &#39;&#39;.format(len(ele)))
                
                s,e,d = ele
                if not np.all(pl.itercheck([s,e,d], pl.isnumeric)):
                    raise TypeError(&#39;All values in ({},{},{}) must be numerics&#39;.format( 
                        type(s), type(e), type(d)))
                if s &lt; 0 or s &gt;= self.n_days or s &gt;= e:
                    raise ValueError(&#39;`start` ({}) out of range&#39;.format(s))
                if e &gt; self.n_days:
                    raise ValueError(&#39;`end` ({}) out of range&#39;.format(e))
                if d &lt; 1:
                    raise ValueError(&#39;`density` ({}) must be &gt;= 1&#39;.format(d))
            
            # Order by the start
            starts = [s for (s,e,d) in D]
            idxs = np.argsort(starts)
            temp = []
            for idx in idxs:
                temp.append(D[idx])
            D = temp

            # Check if any of the intervals overlap
            # If they overlap and they have different densities then 
            # throw an error
            for i in range(len(D)-1):
                si, ei, di = D[i]
                sj, ej, dj = D[i+1]

                if si == sj:
                    raise ValueError(&#39;Two intervals ({} and {}) cannot have the &#39; \
                        &#39;same start point&#39;.format(D[i], D[i+1]))
                if ei &gt; sj:
                    if di != dj:
                        raise ValueError(&#39;Intervals {} and {} overlap and have &#39; \
                            &#39;different densities&#39;.format(D[i], D[i+1]))

        # Merge any overlapping time periods. If the time periods are not 
        # overlapping then add in extra intervals with the background density
        # 1
        D_new = [D[0]]
        i = 1
        while True:
            if i == len(D):
                break

            # Compare the last element in D_new to the ith element of D
            si, ei, di = D_new[-1]
            sj, ej, dj = D[i]

            if ei &gt; sj and ei &lt; ej:
                # they overlap, merge
                if di != dj:
                    raise ValueError(&#39;Something went wrong. D_new: {}, D: {}, &#39; \
                        &#39;i: {}&#39;.format(D_new, D, i))
                D[-1] = (si, ej, di)
            else:
                if ei &lt; sj:
                    # These do not overlap, add an intermediate
                    D_new.append((ei, sj, 1))
                elif ei != sj:
                    raise ValueError(&#39;Something went wrong. D_new: {}, D: {}, &#39; \
                        &#39;i: {}&#39;.format(D_new, D, i))
                D_new.append(D[i])
            i += 1
        D = D_new

        # check if the last timepoint goes up to the last time
        s,e,d = D[-1]
        if e &lt; self.n_days:
            D.append((e,self.n_days,1))
        if e &gt; self.n_days:
            raise ValueError(&#39;Last interval {} ends after the set number of days {}&#39; \
                &#39;&#39;.format(D[-1], self.n_days))

        # Set the days that the perturbations start and end as essential
        essential_timepoints = None
        if self.dynamics.perturbations is not None:
            essential_timepoints = []
            for perturbation in self.dynamics.perturbations:
                essential_timepoints += [perturbation.start, perturbation.end]

        # Set the timepoints for each replicate
        self.master_times = set_timepoints_without_timeseries(N=N, T_start=0, T_end=self.n_days, D=D,
            move_timepoints_if_fail=True, essential_timepoints=essential_timepoints)

    def generate_trajectories(self, dt, init_dist=None, processvar=None):
        &#39;&#39;&#39;Generate gLV dynamics given the dynamics sampled or set

        Parameters
        ----------
        init_dist : pylab.variables.RandomVariable
            This is the distribution that we are sampling the initial 
            condition from
        n_replicates : int
            How many replicates to make
        dt : float
            This is the sampling rate to generate the trajectories
        - processvar : pl.dynamics.BaseProcessVariance
            Must be a subset of process variance class
        &#39;&#39;&#39;
        if processvar is not None:
            if not pl.isprocessvariance(processvar):
                raise TypeError(&#39;`processvar` ({}) not recognized&#39;.format(
                    type(processvar)))
        if init_dist is not None:
            self.init_dist = init_dist
        self.processvar = processvar
        self.n_replicates += 1
        
        self.dt = dt
        n_valid = 1

        while n_valid &gt; 0:
            init_abundance = self.init_dist.sample(size=len(self.asvs)).reshape(-1,1)
            d = self.dynamics.integrate(
                initial_conditions=init_abundance,
                dt=self.dt, processvar=processvar, subsample=True, 
                times=self.master_times, n_days=self.n_days)
            if not np.any(np.isnan(d[&#39;X&#39;])):
                # valid, we dont have to resample
                self.data.append(d[&#39;X&#39;])
                self.times.append(d[&#39;times&#39;])
                n_valid -= 1
            else:
                logging.info(&#39;resampling&#39;)
                print(d[&#39;X&#39;])

        return self
    
    def stability(self):
        return self.dynamics.stability()

    def simulateRealRegressionDataDMD(self, subjset, alpha, qpcr_scale=None):
        &#39;&#39;&#39;This function converts the synthetic trajectories into &#34;real&#34; data
        by simulating read counts and qPCR measurements. We base the sampling
        on parameters that we learn from the real data (MouseSet `subjset`). We assume
        that the real data has already been filtered.

        This uses a dirichlet multinomial to simulate the reads

        Simulating qPCR measurements
        ----------------------------
        We simulate the qPCR measurements with a lognormal distribution that has
        the standard deviation `qpcr_scale`. If `qpcr_scale` is not given, then we fit
        fit the qPCR measurements in `subjset` and use that value.

        Simulating count data
        ---------------------
        We first simulate the read depth for each day from a negative binomial
        that was fitted using the read depth of our data from `subjset` (using function 
        minimization). To generate the indivual counts for each ASV, we then &#39;sample&#39; 
        from a dirichlet multinomial using the sample read depth as our counts and the 
        `alpha` parameter as the multiplicative concentration for our relative abundances, where
            $\mathbf{\alpha}_i = \alpha * r_i$,
            $\mathbf{\alpha}_i$ is the concentration parameter for ASV $i$ to sample from a dirichlet
            distribution,
            $r_i$ is the relative abundance for ASV $i$,
            $\alpha$ is the input concentration
        To emulate a dirichlet multinomial distribution, we first sample the relative abundances 
        of each of the ASVs from a dirichlet distribution using our alpha term as sepcified above, 
        and then sample from a multinomial distribution given our sampled a read depth and our 
        probabilities sampled concentrations for each ASV from the dirichlet distribution.

        Parameters
        ----------
        subjset : pl.Study
            This is the real data that we are fitting to
        alpha : numeric, Optional
            This is a parameter for how much noise you have in sampling the individual reads.
            The larger the number, the less noise there is in the system. The smaller the number,
            the more noise there is in sampling the relative abundances
        qpcr_scale : numeric, None, Opional
            This is the scale of the lognormal distribution used to generate the qPCR measurements
            of the simulated data

        Returns
        -------
        pl.Study
            This is the subject set that contains the data in terms of counts and absolute abundance
        &#39;&#39;&#39;
        logging.info(&#39;Fitting real data&#39;)
        # load Study and filter
        if type(subjset) == str:
            subjset = pl.Study.load(subjset)
        elif not pl.isstudy(subjset):
            raise ValueError(&#39;`subjset` ({}) must eb a pl.Study object&#39;.format(
                type(subjset)))
        if not pl.isnumeric(alpha):
            raise ValueError(&#39;`alpha` ({}) must either be a float or an int&#39;.format(type(alpha)))
        elif alpha &lt; 0:
            raise ValueError(&#39;`alpha` ({}) must be greater than 0&#39;.format(alpha))
        if qpcr_scale is not None:
            if not pl.isnumeric(qpcr_scale):
                raise ValueError(&#39;`qpcr_scale` ({}) must either be a float or an int&#39;.format(
                    type(qpcr_scale)))
            elif qpcr_scale &lt; 0:
                raise ValueError(&#39;`qpcr_scale` ({}) must be greater than 0&#39;.format(
                    qpcr_scale))
        
        # Fit qPCR with lognormal if necessary
        if qpcr_scale is not None:
            data = []
            for subj in subjset:
                for t, qpcr in subj.qpcr.items():
                    d = np.log(qpcr.data)
                    data = np.append(data, d - np.mean(d))
            qpcr_scale = np.std(data)
            logging.info(&#39;qpcr fitted scale: {}&#39;.format(qpcr_scale))

        # Fit read depth with negative binomial
        read_depths = np.asarray([])
        for subj in subjset:
            read_depths = np.append(read_depths, subj.read_depth())
        n_pred, p_pred = _fit_nbinom(read_depths)
        logging.info(&#39;read depth negbin n: {}, p: {}&#39;.format(n_pred, p_pred))
        readdepth_negbin = scipy.stats.nbinom(n_pred, p_pred)
        
        # Make data, record the data with noise
        ret_subjset = pl.Study(asvs=self.asvs)
        for ridx in range(self.n_replicates):
            mid = str(ridx)
            ret_subjset.add(name=mid)

            for tidx in range(len(self.times[ridx])):
                # make time id
                t = self.times[ridx][tidx]

                # Sample counts
                sum_abund = np.sum(self.data[ridx][:,tidx])
                rel = self.data[ridx][:,tidx] / sum_abund
                probs = (scipy.stats.dirichlet.rvs(rel*alpha)).flatten()
                read_depth = readdepth_negbin.rvs()
                ret_subjset[mid].reads[t] = scipy.stats.multinomial.rvs(read_depth, probs).ravel()

                # Sample qPCR
                triplicates = np.exp(np.log(sum_abund) + qpcr_scale * npr.normal(size=3))
                ret_subjset[mid].qpcr[t] = pl.qPCRdata(cfus=triplicates, mass=1., 
                    dilution_factor=1.)

            ret_subjset[mid].times = self.times[ridx]
        
        # Add in the perturbations
        if self.dynamics.perturbations is not None:
            for perturbation in self.dynamics.perturbations:
                ret_subjset.add_perturbation(perturbation.start, perturbation.end)
        return ret_subjset

    def simulate_reads(self, a0, a1, read_depth):
        &#39;&#39;&#39;This function converts the synthetic trajectories into &#34;real&#34; data
        by simulating read counts and qPCR measurements. We base the sampling
        on parameters that we learn from the real data (MouseSet `subjset`). We assume
        that the real data has already been filtered.

        Simulating count data
        ---------------------
        We first fit the read depth for each day (`r_k`) from a negative binomial from the 
        read depth of our data (`subjset`) using function minimization. We then use `r_k`, 
        `a_0`, and `a_1` with the relative abundances to sample from a negative binomial 
        distirbution. We then use the relative abundances from this sample as the concentrations
        for a multinomial distribution with read depth `r_k`.

        Parameters
        ----------
        subjset : pl.Study
            This is the real data that we are fitting to
        a0, a1 : numeric
            These are the negative binomial dispersion parameters that we are using to 
            simulate the data
        &#39;&#39;&#39;
        if not np.all(pl.itercheck([a0, a1], pl.isnumeric)):
            raise TypeError(&#39;`a0` ({}) and `a1` ({}) must be numerics&#39;.format(
                type(a0), type(a1)))
        if a0 &lt; 0 or a1 &lt; 0:
            raise ValueError(&#39;`a0` ({}) and `a1` ({}) must be &gt; 0&#39;.format(a0, a1))
        
        # Make data, record the data with noise
        n_time_points = 0
        for times in self.times:
            n_time_points += len(times)

        ret_subjset = pl.Study(asvs=self.asvs)
        data = self.data
        times = self.times
        for ridx in np.arange(self.n_replicates):
            mid = str(ridx)
            ret_subjset.add(name=mid)
            # self.data_w_noise.append(np.zeros(shape=data[ridx].shape,dtype=float))

            for tidx in range(len(times[ridx])):
                # make time id
                t = times[ridx][tidx]

                # Sample counts
                sum_abund = np.sum(data[ridx][:,tidx])
                rel = data[ridx][:,tidx] / sum_abund

                phi = read_depth * rel
                eps = a0 / rel + a1

                reads = pl.random.negative_binomial.sample(phi, eps)
                # concentration = reads/np.sum(reads)
                # ret_subjset[mid].reads[t] = scipy.stats.multinomial.rvs(
                #     r_k, concentration).ravel()
                ret_subjset[mid].reads[t] = reads

            ret_subjset[mid].times = times[ridx]
        
        # Add in the perturbations
        if self.dynamics.perturbations is not None:
            for perturbation in self.dynamics.perturbations:
                ret_subjset.add_perturbation(perturbation.start, perturbation.end)
        self.subjset_with_noise = ret_subjset

    def simulate_qpcr(self, qpcr_noise_scale):
        &#39;&#39;&#39;This function converts the synthetic trajectories into &#34;real&#34; data
        by simulating read counts and qPCR measurements. We base the sampling
        on parameters that we learn from the real data (MouseSet `subjset`). We assume
        that the real data has already been filtered.

        We assume that the function `simulate_reads`

        Simulating qPCR measurements
        ----------------------------
        We simulate the qPCR measurements with a lognormal distribution that was
        fitted using the data from `subjset`. We use this parameterization to sample
        a qPCR measurement with mean from the total biomass of the simulated data.

        Parameters
        ----------
        qpcr_noise_scale : numeric
            This is the parameter to scale the `s` parameter learned by the lognormal
            distribution.
        &#39;&#39;&#39;
        if not pl.isnumeric(qpcr_noise_scale):
            raise TypeError(&#39;`qpcr_noise_scale` ({}) must either be a float or an int&#39;.format(
                type(qpcr_noise_scale)))
        elif qpcr_noise_scale &lt; 0:
            raise ValueError(&#39;`qpcr_noise_scale` ({}) must be greater than 0&#39;.format(
                qpcr_noise_scale))

        for ridx in np.arange(self.n_replicates):
            mid = str(ridx)
            subj = self.subjset_with_noise[mid]

            for tidx, t in enumerate(self.times[ridx]):
                
                # Sample qPCR
                sum_abund = np.sum(self.data[ridx][:,tidx])
                triplicates = np.exp(np.log(sum_abund) + qpcr_noise_scale * pl.random.normal.sample(size=3))
                subj.qpcr[t] = pl.qPCRdata(
                    cfus=triplicates, mass=1., dilution_factor=1.)

    def get_subjset(self):
        &#39;&#39;&#39;Return the subjectset with the noise

        Returns
        -------
        pylab.base.Study
        &#39;&#39;&#39;
        return self.subjset_with_noise

    def simulateRealRegressionDataNegBinMD(self, a0, a1, qpcr_noise_scale, subjset, replicates=&#39;all&#39;, read_depth=None):
        &#39;&#39;&#39;This function converts the synthetic trajectories into &#34;real&#34; data
        by simulating read counts and qPCR measurements. We base the sampling
        on parameters that we learn from the real data (MouseSet `subjset`). We assume
        that the real data has already been filtered.

        This uses a negative binomial distribution to simiulate the reads.

        Simulating qPCR measurements
        ----------------------------
        We simulate the qPCR measurements with a lognormal distribution that was
        fitted using the data from `subjset`. We use this parameterization to sample
        a qPCR measurement with mean from the total biomass of the simulated data.

        Simulating count data
        ---------------------
        We first fit the read depth for each day (`r_k`) from a negative binomial from the 
        read depth of our data (`subjset`) using function minimization. We then use `r_k`, 
        `a_0`, and `a_1` with the relative abundances to sample from a negative binomial 
        distirbution. We then use the relative abundances from this sample as the concentrations
        for a multinomial distribution with read depth `r_k`.

        Parameters
        ----------
        subjset : pl.Study
            This is the real data that we are fitting to
        a0, a1 : numeric
            These are the negative binomial dispersion parameters that we are using to 
            simulate the data
        qpcr_noise_scale : numeric
            This is the parameter to scale the `s` parameter learned by the lognormal
            distribution.
        replicates : str, array, int
            Which replicates to make the data for. If `replicates=&#39;all&#39;`, then we make
            it for all of the replicates. If it is an array, we assume it is an array of
            replicate indices of the replicates that you want.

        Returns
        -------
        pl.Study
            This is the subject set that contains the data in terms of counts and absolute abundance
        &#39;&#39;&#39;
        logging.info(&#39;Fitting real data&#39;)
        if pl.isstr(subjset):
            subjset = pl.base.Study.load(subjset)
        elif not pl.isstudy(subjset):
            raise TypeError(&#39;`subjset` ({}) must be a pylab.base.SubjsetSet&#39;.format( 
                type(subjset)))
        if not pl.isnumeric(qpcr_noise_scale):
            raise TypeError(&#39;`qpcr_noise_scale` ({}) must either be a float or an int&#39;.format(
                type(qpcr_noise_scale)))
        elif qpcr_noise_scale &lt; 0:
            raise ValueError(&#39;`qpcr_noise_scale` ({}) must be greater than 0&#39;.format(
                qpcr_noise_scale))
        if pl.isstr(replicates):
            if replicates == &#39;all&#39;:
                replicates = np.arange(len(self.data))
            else:
                raise ValueError(&#39;`replicates` ({}) not recognized&#39;.format(replicates))
        elif pl.isint(replicates):
            replicates = [replicates]
        if pl.isarray(replicates):
            for i in replicates:
                if not pl.isint(i):
                    raise TypeError(&#39;`replicates` ({}) must be ints&#39;.format(type(i)))
                if i &gt;= len(self.data):
                    raise IndexError(&#39;`replicates` ({}) out of range ({})&#39;.format(
                        i, len(self.data)))
        else:
            raise TypeError(&#39;`replicates` ({}) type not recognized&#39;.format(type(replicates)))


        # # Fit qPCR with lognormal
        # data = []
        # for subj in subjset:
        #     for t, qpcr in subj.qpcr.items():
        #         if np.any(np.isnan(qpcr.data)):
        #             continue
        #         data.append(qpcr.data)
        # std_biomass = _fit_qpcr(data) * self.qpcr_noise_scale
        std_biomass = qpcr_noise_scale
        logging.info(&#39;lognormal s: {}&#39;.format(std_biomass))

        # Fit read depth with negative binomial
        if read_depth is None:
            read_depths = np.asarray([])
            for subj in subjset:
                read_depths = np.append(read_depths, subj.read_depth())
            n_pred, p_pred = _fit_nbinom(read_depths)
            logging.info(&#39;negbin n: {}, p: {}&#39;.format(n_pred, p_pred))
            negbin_read_depth = scipy.stats.nbinom(n_pred, p_pred)

        # Make data, record the data with noise
        n_time_points = 0
        for times in self.times:
            n_time_points += len(times)

        ret_subjset = pl.Study(asvs=self.asvs)
        data = self.data
        times = self.times
        for ridx in replicates:
            mid = str(ridx)
            ret_subjset.add(name=mid)
            # self.data_w_noise.append(np.zeros(shape=data[ridx].shape,dtype=float))

            for tidx in range(len(times[ridx])):
                # make time id
                t = times[ridx][tidx]

                # Sample counts
                sum_abund = np.sum(data[ridx][:,tidx])
                rel = data[ridx][:,tidx] / sum_abund
                if read_depth is None:
                    r_k = negbin_read_depth.rvs()
                else:
                    r_k = read_depth
                read_depth = 75000

                phi = r_k * rel
                eps = a0 / rel + a1

                reads = pl.random.negative_binomial.sample(phi, eps)
                # concentration = reads/np.sum(reads)
                # ret_subjset[mid].reads[t] = scipy.stats.multinomial.rvs(
                #     r_k, concentration).ravel()
                ret_subjset[mid].reads[t] = reads

                # Sample qPCR
                triplicates = np.exp(np.log(sum_abund) + std_biomass * npr.normal(size=3))
                ret_subjset[mid].qpcr[t] = pl.qPCRdata(
                    cfus=triplicates, mass=1., dilution_factor=1.)

            ret_subjset[mid].times = times[ridx]
        
        # Add in the perturbations
        if self.dynamics.perturbations is not None:
            for perturbation in self.dynamics.perturbations:
                ret_subjset.add_perturbation(perturbation.start, perturbation.end)
        return ret_subjset

    def simulateExactSubjset(self):
        &#39;&#39;&#39;This function is effectively the same as `simulateRealRegressionData*` but 
        we add an :math:`\epsilon` amount of measurement noise.

        Returns
        -------
        pylab.Base.Study
        &#39;&#39;&#39;
        subjset = pl.Study(asvs=self.asvs)
        data = self.data
        times = self.times

        for ridx in range(len(data)):
            mid = str(ridx)
            subjset.add(name=mid)

            for tidx in range(len(times[ridx])):
                t = times[ridx][tidx]

                # Make &#34;exact&#34; counts we read depth at ~1000000
                total_abund = np.sum(data[ridx][:,tidx])
                rel = data[ridx][:,tidx]/total_abund
                counts = np.asarray(rel * 1000000, dtype=int)
                subjset[mid].reads[t] = counts

                # make &#34;exact&#34; qpcr by having very little qpcr noise
                subjset[mid].qpcr[t] = pl.qPCRdata( 
                    cfus=pl.random.normal.sample(mean=total_abund, std=1e-10, size=3), 
                    mass=1., dilution_factor=1.)
            subjset[mid].times = times[ridx]

        # Add in the perturbations
        if self.dynamics.perturbations is not None:
            for perturbation in self.dynamics.perturbations:
                subjset.add_perturbation(perturbation.start, perturbation.end, 
                    name=perturbation.name)
        
        return subjset</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mdsine2.pylab.base.Saveable" href="pylab/base.html#mdsine2.pylab.base.Saveable">Saveable</a></li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="mdsine2.synthetic.SyntheticData.perturbations"><code class="name">var <span class="ident">perturbations</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def perturbations(self):
    try:
        return self.dynamics.perturbations
    except:
        raise pl.UndefinedError(&#39;You need to define `dynamics` ({}) first.&#39;.format(
            type(self.dynamics)))</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="mdsine2.synthetic.SyntheticData.generate_trajectories"><code class="name flex">
<span>def <span class="ident">generate_trajectories</span></span>(<span>self, dt, init_dist=None, processvar=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Generate gLV dynamics given the dynamics sampled or set</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>init_dist</code></strong> :&ensp;<code>pylab.variables.RandomVariable</code></dt>
<dd>This is the distribution that we are sampling the initial
condition from</dd>
<dt><strong><code>n_replicates</code></strong> :&ensp;<code>int</code></dt>
<dd>How many replicates to make</dd>
<dt><strong><code>dt</code></strong> :&ensp;<code>float</code></dt>
<dd>
<p>&nbsp;</p>
<p>This is the sampling rate to generate the trajectories
- processvar : pl.dynamics.BaseProcessVariance
Must be a subset of process variance class</p>
</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_trajectories(self, dt, init_dist=None, processvar=None):
    &#39;&#39;&#39;Generate gLV dynamics given the dynamics sampled or set

    Parameters
    ----------
    init_dist : pylab.variables.RandomVariable
        This is the distribution that we are sampling the initial 
        condition from
    n_replicates : int
        How many replicates to make
    dt : float
        This is the sampling rate to generate the trajectories
    - processvar : pl.dynamics.BaseProcessVariance
        Must be a subset of process variance class
    &#39;&#39;&#39;
    if processvar is not None:
        if not pl.isprocessvariance(processvar):
            raise TypeError(&#39;`processvar` ({}) not recognized&#39;.format(
                type(processvar)))
    if init_dist is not None:
        self.init_dist = init_dist
    self.processvar = processvar
    self.n_replicates += 1
    
    self.dt = dt
    n_valid = 1

    while n_valid &gt; 0:
        init_abundance = self.init_dist.sample(size=len(self.asvs)).reshape(-1,1)
        d = self.dynamics.integrate(
            initial_conditions=init_abundance,
            dt=self.dt, processvar=processvar, subsample=True, 
            times=self.master_times, n_days=self.n_days)
        if not np.any(np.isnan(d[&#39;X&#39;])):
            # valid, we dont have to resample
            self.data.append(d[&#39;X&#39;])
            self.times.append(d[&#39;times&#39;])
            n_valid -= 1
        else:
            logging.info(&#39;resampling&#39;)
            print(d[&#39;X&#39;])

    return self</code></pre>
</details>
</dd>
<dt id="mdsine2.synthetic.SyntheticData.get_full_interaction_matrix"><code class="name flex">
<span>def <span class="ident">get_full_interaction_matrix</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Make the interaction matrix. If <code>with_interactions</code> is False,
you're effectively only getting the self-interaction terms.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_full_interaction_matrix(self):
    &#39;&#39;&#39;Make the interaction matrix. If `with_interactions` is False,
    you&#39;re effectively only getting the self-interaction terms.
    &#39;&#39;&#39;
    A = self.dynamics.interactions.get_datalevel_value_matrix(set_neg_indicators_to_nan=False)
    for i in range(A.shape[0]):
        A[i,i] = -self.dynamics.self_interactions[i]
    return A</code></pre>
</details>
</dd>
<dt id="mdsine2.synthetic.SyntheticData.get_subjset"><code class="name flex">
<span>def <span class="ident">get_subjset</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Return the subjectset with the noise</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pylab.base.Study</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_subjset(self):
    &#39;&#39;&#39;Return the subjectset with the noise

    Returns
    -------
    pylab.base.Study
    &#39;&#39;&#39;
    return self.subjset_with_noise</code></pre>
</details>
</dd>
<dt id="mdsine2.synthetic.SyntheticData.icml_perturbations"><code class="name flex">
<span>def <span class="ident">icml_perturbations</span></span>(<span>self, starts, ends)</span>
</code></dt>
<dd>
<div class="desc"><p>Set the perturbations. Made to be informative. The <code>starts</code> and
<code>end</code> are starts and ends of 3 perturbations</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def icml_perturbations(self, starts, ends):
    &#39;&#39;&#39;Set the perturbations. Made to be informative. The `starts` and
    `end` are starts and ends of 3 perturbations
    &#39;&#39;&#39;
    if self.dynamics.clustering is None:
        raise TypeError(&#39;Clustering module is None, this is likely because you did &#39; \
            &#39;not set the dynamics&#39;)

    if not pl.isarray(starts):
        raise TypeError(&#39;`starts` ({}) must be an array&#39;.format(type(starts)))
    if len(starts) != 3:
        raise ValueError(&#39;`starts` ({}) must be 3 elements long&#39;.format(len(starts)))
    for ele in starts:
        if not pl.isnumeric(ele):
            raise TypeError(&#39;Each element in `starts` ({}) must be an array ({})&#39;.format(
                ele, starts))
    if not pl.isarray(ends):
        raise TypeError(&#39;`ends` ({}) must be an array&#39;.format(type(ends)))
    if len(ends) != 3:
        raise ValueError(&#39;`ends` ({}) must be 3 elements long&#39;.format(len(ends)))
    for ele in ends:
        if not pl.isnumeric(ele):
            raise TypeError(&#39;Each element in `ends` ({}) must be an array ({})&#39;.format(
                ele, ends))
    for idx in range(len(starts)):
        start = starts[idx]
        end = ends[idx]

        if end &lt;= start:
            raise ValueError(&#39;`start` ({}) must be smaller than `end` ({})&#39;.format(
                start,end))

    # Set perturbations
    c0 = self.dynamics.clustering.order[0]
    c1 = self.dynamics.clustering.order[1]
    c2 = self.dynamics.clustering.order[2]

    # Set perturbation 0
    if self.perturbations_additive:
        magnitude0 = np.asarray([
                # pl.random.normal.sample(mean=-1, std=0.1),
                0, pl.random.normal.sample(mean=0.5, std=0.1), 0])
    else:
        magnitude0 = np.asarray([
                # pl.random.normal.sample(mean=-1, std=0.1),
                0, pl.random.normal.sample(mean=1, std=0.1), 0])
    indicator0 = np.array([False, True, False], dtype=bool)
    p0 = pl.contrib.ClusterPerturbation(start=starts[0], end=ends[0], 
        magnitude=magnitude0, indicator=indicator0, G=self.G, 
        clustering=self.dynamics.clustering)

    # Set perturbation 1
    if self.perturbations_additive:
        magnitude1 = np.asarray([
                pl.random.normal.sample(mean=1, std=0.1), 0,
                pl.random.normal.sample(mean=-2, std=0.1)])
    else:
        magnitude1 = np.asarray([
                pl.random.normal.sample(mean=1, std=0.1), 0,
                pl.random.normal.sample(mean=-2, std=0.1)])
    indicator1 = np.array([True, False, True], dtype=bool)
    p1 = pl.contrib.ClusterPerturbation(start=starts[1], end=ends[1], 
        magnitude=magnitude1, indicator=indicator1, G=self.G, 
        clustering=self.dynamics.clustering)

    # Set perturbation 2
    if self.perturbations_additive:
        magnitude2 = np.asarray([
                0, pl.random.normal.sample(mean=-0.5, std=0.1),
                pl.random.normal.sample(mean=1, std=0.1)])
    else:
        magnitude2 = np.asarray([
                0, pl.random.normal.sample(mean=-0.5, std=0.1),
                pl.random.normal.sample(mean=1, std=0.1)])
    indicator2 = np.array([False, True, True], dtype=bool)
    p2 = pl.contrib.ClusterPerturbation(start=starts[2], end=ends[2], 
        magnitude=magnitude2, indicator=indicator2, G=self.G, 
        clustering=self.dynamics.clustering)

    self.dynamics.perturbations = [p0,p1,p2]</code></pre>
</details>
</dd>
<dt id="mdsine2.synthetic.SyntheticData.icml_topology"><code class="name flex">
<span>def <span class="ident">icml_topology</span></span>(<span>self, n_asvs=13, max_abundance=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Recreate the dynamical system used in the ICML paper [1]. If you use the
default parameters you will get the same interaction matrix that was used in [1].</p>
<p>We rescale the self-interactions and the interactions (and potentially growth) by 150
so that the time-scales of the trajectories happen over days instead of minutes</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>n_asvs</code></strong> :&ensp;<code>int</code></dt>
<dd>These are how many ASVs to include in the system. We will always have 3 clusters and
the proportion of ASVs in each cluster is as follows:
cluster 1 - 5/13
cluster 2 - 6/13
cluster 3 - 2/13
We assign each ASV to each cluster the best we can, any extra ASVs we put into cluster 3</dd>
<dt><strong><code>max_abundance</code></strong> :&ensp;<code>numeric, None</code></dt>
<dd>This is the abundance to set the maximum. All of the other
parameters change proportionally. If <code>None</code> then we assume no change.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def icml_topology(self, n_asvs=13, max_abundance=None):
    &#39;&#39;&#39;Recreate the dynamical system used in the ICML paper [1]. If you use the 
    default parameters you will get the same interaction matrix that was used in [1].

    We rescale the self-interactions and the interactions (and potentially growth) by 150
    so that the time-scales of the trajectories happen over days instead of minutes

    Parameters
    ----------
    n_asvs : int
        These are how many ASVs to include in the system. We will always have 3 clusters and
        the proportion of ASVs in each cluster is as follows:
            cluster 1 - 5/13
            cluster 2 - 6/13
            cluster 3 - 2/13
        We assign each ASV to each cluster the best we can, any extra ASVs we put into cluster 3
    max_abundance : numeric, None
        This is the abundance to set the maximum. All of the other 
        parameters change proportionally. If `None` then we assume no change.
    &#39;&#39;&#39;
    if not pl.isint(n_asvs):
        raise TypeError(&#39;`n_asvs` ({}) must be an int&#39;.format(type(n_asvs)))
    if n_asvs &lt; 3:
        raise ValueError(&#39;`n_asvs` ({}) must be &gt;= 3&#39;.format(n_asvs))
    if max_abundance is not None:
        if not pl.isnumeric(max_abundance):
            raise TypeError(&#39;`max_abundance` ({}) must be a numeric&#39;.format(type(max_abundance)))
        if max_abundance &lt;= 0:
            raise ValueError(&#39;`max_abundance` ({}) must be &gt; 0&#39;.format(max_abundance))
    
    # Generate ASVs
    self.set_asvs(n_asvs)

    # Make cluster assignments with the approximate proportions
    c0size = int(5*n_asvs/13)
    c1size = int(6*n_asvs/13)
    c2size = int(n_asvs - c0size - c1size)

    frac = 150*n_asvs/13

    clusters = [
        np.arange(0, c0size, dtype=int).tolist(), 
        np.arange(c0size, c0size+c1size, dtype=int).tolist(),
        np.arange(c0size+c1size, c0size+c1size+c2size, dtype=int).tolist()]

    self.dynamics.clustering = pl.Clustering(clusters=clusters, items=self.asvs, G=self.G)
    self.dynamics.interactions = pl.Interactions(clustering=self.dynamics.clustering, 
        use_indicators=True, G=self.G)

    c0 = self.dynamics.clustering.order[0]
    c1 = self.dynamics.clustering.order[1]
    c2 = self.dynamics.clustering.order[2]
    for interaction in self.dynamics.interactions:
        if interaction.target_cid == c0 and interaction.source_cid == c1:
            interaction.value = 3/frac
            interaction.indicator = True
        elif interaction.target_cid == c0 and interaction.source_cid == c2:
            interaction.value = -1/frac
            interaction.indicator = True
        elif interaction.target_cid == c2 and interaction.source_cid == c0:
            interaction.value = 2/frac
            interaction.indicator = True
        elif interaction.target_cid == c2 and interaction.source_cid == c1:
            interaction.value = -4/frac
            interaction.indicator = True
        else:
            interaction.value = 0
            interaction.indicator = False
    
    self.dynamics.growth = pl.random.uniform.sample(low=18, high=22, size=n_asvs)/150
    # self.dynamics.growth = pl.random.uniform.sample(.1, 0.6, size=n_asvs)
    # self.dynamics.growth = pl.random.uniform.sample(low=.1 + 0.2, high=math.log(10) - 0.2, size=n_asvs)
    # self.dynamics.growth = pl.random.uniform.sample(low=0.12, high=2, size=n_asvs)
    self.dynamics.self_interactions = np.ones(n_asvs, dtype=float)*(5)/150
    # self.dynamics.self_interactions = pl.random.uniform.sample(-.05/150, -50/150, size=n_asvs)

    if max_abundance is not None:
        # self.dynamics.growth = pl.random.uniform.sample(low=.1, high=math.log(10), size=n_asvs)
        # self.dynamics.growth = pl.random.uniform.sample(low=.21, high=.5, size=n_asvs)/2
        # rescale the abundance such that hte max the max is ~max_abundance
        frac = 25/max_abundance
        self.dynamics.self_interactions *= frac
        for interaction in self.dynamics.interactions:
            if interaction.indicator:
                interaction.value *= frac</code></pre>
</details>
</dd>
<dt id="mdsine2.synthetic.SyntheticData.icml_topology_real"><code class="name flex">
<span>def <span class="ident">icml_topology_real</span></span>(<span>self, n_asvs=13, max_abundance=None, scale_interaction=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Recreate the dynamical system used in the ICML paper [1]. If you use the
default parameters you will get the same interaction matrix that was used in [1].</p>
<p>We rescale the self-interactions and the interactions (and potentially growth) by 150
so that the time-scales of the trajectories happen over days instead of minutes</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>n_asvs</code></strong> :&ensp;<code>int</code></dt>
<dd>These are how many ASVs to include in the system. We will always have 3 clusters and
the proportion of ASVs in each cluster is as follows:
cluster 1 - 5/13
cluster 2 - 6/13
cluster 3 - 2/13
We assign each ASV to each cluster the best we can, any extra ASVs we put into cluster 3</dd>
<dt><strong><code>max_abundance</code></strong> :&ensp;<code>numeric, None</code></dt>
<dd>This is the abundance to set the maximum. All of the other
parameters change proportionally. If <code>None</code> then we assume no change.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def icml_topology_real(self, n_asvs=13, max_abundance=None, scale_interaction=None):
    &#39;&#39;&#39;Recreate the dynamical system used in the ICML paper [1]. If you use the 
    default parameters you will get the same interaction matrix that was used in [1].

    We rescale the self-interactions and the interactions (and potentially growth) by 150
    so that the time-scales of the trajectories happen over days instead of minutes

    Parameters
    ----------
    n_asvs : int
        These are how many ASVs to include in the system. We will always have 3 clusters and
        the proportion of ASVs in each cluster is as follows:
            cluster 1 - 5/13
            cluster 2 - 6/13
            cluster 3 - 2/13
        We assign each ASV to each cluster the best we can, any extra ASVs we put into cluster 3
    max_abundance : numeric, None
        This is the abundance to set the maximum. All of the other 
        parameters change proportionally. If `None` then we assume no change.
    &#39;&#39;&#39;
    if not pl.isint(n_asvs):
        raise TypeError(&#39;`n_asvs` ({}) must be an int&#39;.format(type(n_asvs)))
    if n_asvs &lt; 3:
        raise ValueError(&#39;`n_asvs` ({}) must be &gt;= 3&#39;.format(n_asvs))
    if max_abundance is not None:
        if not pl.isnumeric(max_abundance):
            raise TypeError(&#39;`max_abundance` ({}) must be a numeric&#39;.format(type(max_abundance)))
        if max_abundance &lt;= 0:
            raise ValueError(&#39;`max_abundance` ({}) must be &gt; 0&#39;.format(max_abundance))
    if scale_interaction is None:
        scale_interaction = 1
    # Generate ASVs
    self.set_asvs(n_asvs)

    # Make cluster assignments with the approximate proportions
    c0size = int(5*n_asvs/13)
    c1size = int(6*n_asvs/13)
    c2size = int(n_asvs - c0size - c1size)

    frac = 150*n_asvs/13

    clusters = [
        np.arange(0, c0size, dtype=int).tolist(), 
        np.arange(c0size, c0size+c1size, dtype=int).tolist(),
        np.arange(c0size+c1size, c0size+c1size+c2size, dtype=int).tolist()]

    self.dynamics.clustering = pl.Clustering(clusters=clusters, items=self.asvs, G=self.G)
    self.dynamics.interactions = pl.Interactions(clustering=self.dynamics.clustering, 
        use_indicators=True, G=self.G)

    c0 = self.dynamics.clustering.order[0]
    c1 = self.dynamics.clustering.order[1]
    c2 = self.dynamics.clustering.order[2]
    for interaction in self.dynamics.interactions:
        if interaction.target_cid == c0 and interaction.source_cid == c1:
            interaction.value = scale_interaction*3/frac
            interaction.indicator = True
        elif interaction.target_cid == c0 and interaction.source_cid == c2:
            interaction.value = scale_interaction*(-1)/frac
            interaction.indicator = True
        elif interaction.target_cid == c2 and interaction.source_cid == c0:
            interaction.value = scale_interaction*2/frac
            interaction.indicator = True
        elif interaction.target_cid == c2 and interaction.source_cid == c1:
            interaction.value = scale_interaction*(-4)/frac
            interaction.indicator = True
        else:
            interaction.value = 0
            interaction.indicator = False
    
    # self.dynamics.growth = pl.random.uniform.sample(low=18, high=22, size=n_asvs)/150
    self.dynamics.growth = pl.random.uniform.sample(0.5, 1.5, size=n_asvs)
    # self.dynamics.growth = pl.random.uniform.sample(low=.1 + 0.2, high=math.log(10) - 0.2, size=n_asvs)
    self.dynamics.self_interactions = np.ones(n_asvs, dtype=float)*(5)/150
    # self.dynamics.self_interactions = pl.random.uniform.sample(-.05/150, -50/150, size=n_asvs)

    if max_abundance is not None:
        # self.dynamics.growth = pl.random.uniform.sample(low=.1, high=math.log(10), size=n_asvs)
        # self.dynamics.growth = pl.random.uniform.sample(low=.21, high=.5, size=n_asvs)/2
        # rescale the abundance such that hte max the max is ~max_abundance
        frac = 25/max_abundance
        self.dynamics.self_interactions *= frac
        for interaction in self.dynamics.interactions:
            if interaction.indicator:
                interaction.value *= frac</code></pre>
</details>
</dd>
<dt id="mdsine2.synthetic.SyntheticData.sample_single_perturbation"><code class="name flex">
<span>def <span class="ident">sample_single_perturbation</span></span>(<span>self, start, end, prob_pos, prob_affect, prob_strength, mean_strength, std_strength)</span>
</code></dt>
<dd>
<div class="desc"><p>Sample a perturbation to add to the system.</p>
<p>If there are no clusters that are selected, we sample again until we get at least 1.</p>
<h2 id="defaults-for-strength-parameters">Defaults For Strength Parameters</h2>
<p><code>prob_strength = [0.2, 0.4, 0.4]</code>
<code>mean_strength
= [0.5, 1.0, 2.0]</code>
<code>std_strength
= 0.1</code></p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>start</code></strong> :&ensp;<code>float</code></dt>
<dd>
<ul>
<li>Time to start the perturbation.</li>
</ul>
</dd>
<dt><strong><code>end</code></strong> :&ensp;<code>float</code></dt>
<dd>
<ul>
<li>Time to end the perturbation</li>
</ul>
</dd>
<dt><strong><code>pob_pos</code></strong> :&ensp;<code>float, [0,1]</code></dt>
<dd>
<ul>
<li>This is the probability that the perturbation is going to be positive</li>
<li>Sampled from a Bernoulli distribution</li>
</ul>
</dd>
<dt><strong><code>mean_strength</code></strong> :&ensp;<code>array</code></dt>
<dd>These are the means of the magnitudes of the perturbations to sample around</dd>
<dt><strong><code>prob_strength</code></strong> :&ensp;<code>array</code></dt>
<dd>These are the probabilities to sample a mean magnitude of the perturbation</dd>
<dt><strong><code>std_strength</code></strong> :&ensp;<code>float</code></dt>
<dd>This is the standard deviation to sample the magnitude of the perturbation</dd>
<dt><strong><code>prob_affect</code></strong> :&ensp;<code>float ([0,1]), str</code></dt>
<dd>
<ul>
<li>This is the probability it will affect a cluster (positive indicator)</li>
<li>Sampled from a Bernoulli distribution</li>
<li>If str, then only that number of clusters is set.
Example:
'1' means only one cluster is set</li>
</ul>
</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pylab.contrib.ClusterPerturbation</code></dt>
<dd>This is the cluster perturbation that was sampled</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sample_single_perturbation(self, start, end, prob_pos, prob_affect, prob_strength, 
    mean_strength, std_strength):
    &#39;&#39;&#39;Sample a perturbation to add to the system.

    If there are no clusters that are selected, we sample again until we get at least 1.

    Defaults for strength parameters
    --------------------------------
    `prob_strength = [0.2, 0.4, 0.4]`
    `mean_strength   = [0.5, 1.0, 2.0]`
    `std_strength  = 0.1`

    Parameters
    ----------
    start : float
        - Time to start the perturbation.
    end : float
        - Time to end the perturbation
    pob_pos : float, [0,1]
        - This is the probability that the perturbation is going to be positive
        - Sampled from a Bernoulli distribution
    mean_strength : array
        These are the means of the magnitudes of the perturbations to sample around
    prob_strength : array
        These are the probabilities to sample a mean magnitude of the perturbation
    std_strength : float
        This is the standard deviation to sample the magnitude of the perturbation
    prob_affect : float ([0,1]), str
        - This is the probability it will affect a cluster (positive indicator)
        - Sampled from a Bernoulli distribution
        - If str, then only that number of clusters is set.
          Example:
            &#39;1&#39; means only one cluster is set

    Returns
    -------
    pylab.contrib.ClusterPerturbation
        This is the cluster perturbation that was sampled
    &#39;&#39;&#39;
    if self.dynamics.clustering is None:
        raise TypeError(&#39;Clustering module is None, this is likely because you did &#39; \
            &#39;not set the dynamics&#39;)

    # Check variables
    if not pl.isnumeric(start):
        raise TypeError(&#39;`start` ({}) must be a numeric&#39;.format(type(start)))
    if not pl.isnumeric(end):
        raise TypeError(&#39;`end` ({}) must be a numeric&#39;.format(type(end)))
    if end &lt;= start:
        raise ValueError(&#39;`start` ({}) must be strictly smaller than `end` ({})&#39;.format(
            start,end))
    if not pl.isfloat(prob_pos):
        raise TypeError(&#39;`prob_pos` ({}) must be a numeric&#39;.format(type(prob_pos)))
    elif prob_pos &lt; 0 or prob_pos &gt; 1:
        raise ValueError(&#39;`prob_pos` ({}) must be [0,1]&#39;.format(prob_pos))
    if pl.isstr(prob_affect):
        set_num = True
        try:
            pa = int(prob_affect)
        except:
            logging.critical(&#39;Cannot cast `prob_affect` ({}) as an int&#39;.format(prob_affect))
            raise
        if pa &lt; 0:
            raise ValueError(&#39;`prob_affect` ({}) must be &gt; 0&#39;.format(prob_affect))
        if pa &gt; len(self.dynamics.clustering.clusters):
            raise ValueError(&#39;`prob_affect` ({}) must be less than n_clusters&#39;.format(prob_affect))
    else:
        set_num = False
        pa = None
        if not pl.isfloat(prob_affect):
            raise TypeError(&#39;`prob_affect` ({}) must be a numeric&#39;.format(type(prob_affect)))
        elif prob_affect &lt; 0 or prob_affect &gt; 1:
            raise ValueError(&#39;`prob_affect` ({}) must be [0,1]&#39;.format(prob_affect))
    # check the strengths
    if not pl.isarray(prob_strength):
        raise TypeError(&#39;`prob_strength` ({}) be an array&#39;.format(type(prob_strength)))
    for ele in prob_strength:
        if not pl.isfloat(ele):
            raise TypeError(&#39;`every element in `prob_strength` ({}) must be a float&#39;.format( 
                type(ele)))
        if ele &lt;= 0:
            raise ValueError(&#39;`every probability in `prob_strength` ({}) must be &gt; 0&#39;.format(ele))
    if not pl.isarray(mean_strength):
        raise TypeError(&#39;`mean_strength` ({}) must be an array&#39;.format(type(mean_strength)))
    for ele in mean_strength:
        if not pl.isnumeric(ele):
            raise TypeError(&#39;Every element in `mean_strength` ({}) must be a numeric&#39;.format(ele))
        if ele &lt; 0:
            raise ValueError(&#39;Every element in `mean_strength` ({}) must be positive.&#39; \
                &#39; If you want a perturbation to have a negative magnitude then set the &#39; \
                &#39;`prob_pos` parameter to a very low nuber&#39;.format(ele))
    if len(mean_strength) != len(prob_strength):
        raise ValueError(&#39;`mean_strength` ({}) and `prob_strength` ({}) must have the same number&#39; \
            &#39; of elements&#39;.format(len(mean_strength), len(prob_strength)))
    if not pl.isnumeric(std_strength):
        raise TypeError(&#39;`std_strength` ({}) must be a numeric&#39;.format(type(std_strength)))
    if std_strength &lt;= 0:
        raise ValueError(&#39;`std_stregnth` ({}) must be &gt; 0&#39;.format(std_strength))

    # Set indicator at the ASV level
    if set_num:
        # Pick the number of clusters on
        order = copy.deepcopy(self.dynamics.clustering.order)
        order = list(order)
        indicator = np.zeros(len(order), dtype=bool)

        # pick the cids to set to true
        while pa &gt; 0:
            # pick a cluster
            idx = npr.randint(0, len(order))
            indicator[self.dynamics.clustering.cid2cidx[order[idx]]] = True
            order.pop(idx)
            pa -= 1
    else:
        i = 0
        while True:
            indicator = np.zeros(len(order), dtype=bool)
            for cidx in range(len(self.dynamics.clustering.clusters)):
                indicator[cidx] = bool(pl.random.bernoulli.sample(prob_affect))
            if np.sum(indicator) &gt; 0:
                break
            if i == 1000:
                raise ValueError(&#39;Assigning cluster ids failed 1000 times. set `prob_affect` {}&#39; \
                    &#39; to a larger number&#39;.format(prob_affect))
            i += 1

    magnitude = np.zeros(len(self.dynamics.clustering), dtype=float)
    for i,ind in enumerate(indicator):
        if ind:
            # sample the magnitude
            sign = pl.random.bernoulli.sample(prob_pos) * 2 - 1
            mean = np.random.choice(mean_strength, p=prob_strength)
            magnitude[i] = pl.random.normal.sample(mean=mean*sign, 
                std=std_strength)

    a = pl.contrib.ClusterPerturbation(start=start, end=end, magnitude=magnitude,
        clustering=self.dynamics.clustering, indicator=indicator, G=self.G)
    
    logging.info(&#39;Perturbation:\n\tstart,end ({},{})\n\t&#39; \
        &#39;magnitude {}\n\tindicator {}&#39;.format(start,end,a.cluster_array(),
        indicator))

    if self.dynamics.perturbations is None:
        self.dynamics.perturbations = [a]
    else:
        self.dynamics.perturbations.append(a)
    return a</code></pre>
</details>
</dd>
<dt id="mdsine2.synthetic.SyntheticData.set_asvs"><code class="name flex">
<span>def <span class="ident">set_asvs</span></span>(<span>self, n_asvs=None, sequences=None, filename=None, asvs=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Sets the ASVset. If you have a list of sequences you want to read in,
pass in the list of <code>sequences</code> and it will create a new ASV for every sequence.
If you
just want to specify how many ASVs you want and you dont care about
sequences, then specify the number of ASVs with <code>n_asvs</code>. If there is an
ASVSet saved on file, you can load it with the keyword <code>filename</code>.</p>
<p>Defines the dynamics once done.</p>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; self.set_asvs(n_asvs=5)
5 ASVs with random sequences
</code></pre>
<pre><code class="language-python-repl">&gt;&gt;&gt; seq = ['AAAA', 'TTTT', 'GGGG', 'CCCC']
&gt;&gt;&gt; self.set_asvs(sequences=seq)
3 ASVs with the sequences specified above
</code></pre>
<pre><code class="language-python-repl">&gt;&gt;&gt; self.set_asvs(filename='pickles/test.pkl')
Reads in the ASVSet saved at 'pickles/test.pkl'
</code></pre>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>n_asvs</code></strong> :&ensp;<code>int, Optional</code></dt>
<dd>How many ASVs you want. This is unnecessary if you specify a list of
sequences.</dd>
<dt><strong><code>sequences</code></strong> :&ensp;<code>array(str), Optional</code></dt>
<dd>The sequence for each ASV. If you do not want any, dont specify anything
and specify the number of ASVs. If nothing is provided it will create
a random sequence of sequences for each ASV</dd>
<dt><strong><code>filename</code></strong> :&ensp;<code>str, Optional</code></dt>
<dd>The filename to lead the ASV</dd>
<dt><strong><code>asvs</code></strong> :&ensp;<code>pylab.base.ASVSet</code></dt>
<dd>This is an ASVSet object</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pl.ASVSet</code></dt>
<dd>This is the ASVSet that gets created</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_asvs(self, n_asvs=None, sequences=None, filename=None, asvs=None):
    &#39;&#39;&#39;Sets the ASVset. If you have a list of sequences you want to read in,
    pass in the list of `sequences` and it will create a new ASV for every sequence. 
    If you  just want to specify how many ASVs you want and you dont care about 
    sequences, then specify the number of ASVs with `n_asvs`. If there is an
    ASVSet saved on file, you can load it with the keyword `filename`.

    Defines the dynamics once done.

    Example:
        &gt;&gt;&gt; self.set_asvs(n_asvs=5)
        5 ASVs with random sequences

        &gt;&gt;&gt; seq = [&#39;AAAA&#39;, &#39;TTTT&#39;, &#39;GGGG&#39;, &#39;CCCC&#39;]
        &gt;&gt;&gt; self.set_asvs(sequences=seq)
        3 ASVs with the sequences specified above
        
        &gt;&gt;&gt; self.set_asvs(filename=&#39;pickles/test.pkl&#39;)
        Reads in the ASVSet saved at &#39;pickles/test.pkl&#39;

    Parameters
    ----------
    n_asvs : int, Optional
        How many ASVs you want. This is unnecessary if you specify a list of 
        sequences.
    sequences : array(str), Optional
        The sequence for each ASV. If you do not want any, dont specify anything
        and specify the number of ASVs. If nothing is provided it will create
        a random sequence of sequences for each ASV
    filename : str, Optional
        The filename to lead the ASV
    asvs : pylab.base.ASVSet
        This is an ASVSet object

    Returns
    -------
    pl.ASVSet
        This is the ASVSet that gets created
    &#39;&#39;&#39;
    a = n_asvs is not None
    b = sequences is not None
    c = filename is not None
    d = asvs is not None

    if a + b + c + d != 1:
        raise TypeError(&#39;Only one of `n_asvs` ({}), `sequences` ({}), &#39;\
            &#39;`filename` ({}), or asvs ({})  can be specified&#39;.format(
            type(n_asvs), type(sequences), type(filename), type(asvs)))
    
    if n_asvs is not None:
        if not pl.isint(n_asvs):
            raise TypeError(&#39;`n_asvs` ({}) must be an int&#39;.format(type(n_asvs)))
        self.asvs = pl.ASVSet()
        for i in range(n_asvs):
            name = &#39;ASV_{}&#39;.format(i)
            seq = &#39;&#39;.join(random.choices([&#39;A&#39;,&#39;T&#39;,&#39;G&#39;,&#39;C&#39;,&#39;U&#39;], k=50))
            self.asvs.add_asv(name=name, sequence=seq)
    elif sequences is not None:
        if not pl.isarray(sequences):
            raise TypeError(&#39;`sequences` ({}) must be an array&#39;.format(type(sequences)))
        if not np.all(pl.itercheck(sequences, pl.isstr)):
            raise TypeError(&#39;All elements in `sequences` must be strs: {}&#39;.format(
                pl.itercheck(sequences, pl.isstr)))
        for i, seq in enumerate(sequences):
            name = &#39;ASV_{}&#39;.format(i)
            self.asvs.add_asv(name=name, sequence=seq)
    elif filename is not None:
        if not pl.isstr(filename):
            raise TypeError(&#39;`filename` ({}) must be a str&#39;.format(type(filename)))
        self.asvs = pl.ASVSet.load(filename)
    else:
        self.asvs = asvs

    self.dynamics = model.gLVDynamicsSingleClustering(asvs=self.asvs, 
        perturbations_additive=self.perturbations_additive)</code></pre>
</details>
</dd>
<dt id="mdsine2.synthetic.SyntheticData.set_cluster_assignments"><code class="name flex">
<span>def <span class="ident">set_cluster_assignments</span></span>(<span>self, clusters=None, n_clusters=None, evenness=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Create clusters for the interactions and perturbations. If you have the cluster
assignments already, set them with <code>clusters</code>. else we can randomly generate them
with the parameters <code>n_clusters</code> and <code>evenness</code>.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>clusters</code></strong> :&ensp;<code>list(list(int))</code></dt>
<dd>These are the cluster assignments of the ASVs</dd>
<dt><strong><code>n_clusters</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of clusters to create</dd>
<dt><strong><code>evenness</code></strong> :&ensp;<code>str, 1</code> or <code>2-dim array</code></dt>
<dd>How to initialize the clusters. This can be generated automatically or by
reading in a similarity matrix.
If it is a str:
'even': Have each cluster have as close to even number of clusters as possible
'heavy-tail': TODO : NOT IMPLEMENTED
'sequence' : TODO : NOT IMPLEMENTED (given the sequences, make an adjacency matrix)
If it is an array:
If 1-dim
This is how you assign the clusters by index to each cluster. This is the
initialization format for pylab.cluster.Clustering
If it is 2-dim
This is a 2 dimensional DISTANCE matrix. It will build the cluster
assignments given this distance matrix</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pl.cluster.Clustering</code></dt>
<dd>This is the clustering object that gets created</dd>
</dl>
<h2 id="see-also">See Also</h2>
<p><code>pylab.cluster.Clustering.__init__</code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_cluster_assignments(self, clusters=None, n_clusters=None, evenness=None):
    &#39;&#39;&#39;Create clusters for the interactions and perturbations. If you have the cluster
    assignments already, set them with `clusters`. else we can randomly generate them 
    with the parameters `n_clusters` and `evenness`.

    Parameters
    ----------
    clusters : list(list(int))
        These are the cluster assignments of the ASVs
    n_clusters : int
        Number of clusters to create
    evenness : str, 1 or 2-dim array
        How to initialize the clusters. This can be generated automatically or by 
        reading in a similarity matrix.
        If it is a str:
            &#39;even&#39;: Have each cluster have as close to even number of clusters as possible
            &#39;heavy-tail&#39;: TODO : NOT IMPLEMENTED
            &#39;sequence&#39; : TODO : NOT IMPLEMENTED (given the sequences, make an adjacency matrix)
        If it is an array:
            If 1-dim
                This is how you assign the clusters by index to each cluster. This is the 
                initialization format for pylab.cluster.Clustering
            If it is 2-dim
                This is a 2 dimensional DISTANCE matrix. It will build the cluster 
                assignments given this distance matrix

    Returns
    -------
    pl.cluster.Clustering
        This is the clustering object that gets created

    See also
    --------
    pylab.cluster.Clustering.__init__
    &#39;&#39;&#39;
    if self.asvs is None:
        raise ValueError(&#39;Must specify the ASVSet before by calling `self.set_asvs`&#39;)

    if clusters is None:
        if not pl.isint(n_clusters):
            raise TypeError(&#39;`n_clusters` ({}) must be an int&#39;.format(n_clusters))

        clusters = []
        start = 0
        if pl.isstr(evenness):
            if evenness == &#39;even&#39;:
                size = int(len(self.asvs)/n_clusters)
                for _ in range(n_clusters-1):
                    clusters.append(np.arange(start,start+size, dtype=int))
                    start += size
                clusters.append(np.arange(start, len(self.asvs), dtype=int))
            elif evenness == &#39;sequence&#39;:
                logging.info(&#39;Making affinity matrix from sequences&#39;)
                evenness = np.diag(np.ones(len(self.asvs), dtype=float))

                for i in range(len(self.asvs)):
                    for j in range(len(self.asvs)):
                        if j &lt;= i:
                            continue
                        # Subtract because we want to make a similarity matrix
                        dist = 1-diversity.beta.hamming(
                            list(self.asvs[i].sequence), list(self.asvs[j].sequence))
                        evenness[i,j] = dist
                        evenness[j,i] = dist

                # print(evenness)

            elif evenness == &#39;heavy-tail&#39;:
                raise NotImplementedError(&#39;`heavy-tail` not implemented yet&#39;)
            else:
                raise ValueError(&#39;cluster evenness ({}) not recognized&#39;.format(evenness))
        if pl.isarray(evenness):
            evenness = np.asarray(evenness)
            if evenness.ndim == 1:
                clusters = evenness.tolist()
            elif evenness.ndim == 2:
                if evenness.shape[0] != evenness.shape[1]:
                    raise ValueError(&#39;Must be a square matrix&#39;)
                if evenness.shape[0] != len(self.asvs):
                    raise ValueError(&#39;Length of the side ({}) must be the same as the number of ASVs ({})&#39;.format(
                        evenness.shape[0], len(self.asvs)))
                
                c = AgglomerativeClustering(
                    n_clusters=n_clusters,
                    affinity=&#39;precomputed&#39;,
                    linkage=&#39;average&#39;)
                assignments = c.fit_predict(evenness)
                clusters = {}
                for oidx,cidx in enumerate(assignments):
                    if cidx not in clusters:
                        clusters[cidx] = []
                    clusters[cidx].append(oidx)
                clusters = [val for val in clusters.values()]
            else:
                raise ValueError(&#39;`evenness` ({}) must be a 1 or 2-dimensional array&#39;.format(
                    evenness.ndim))
        else:
            raise TypeError(&#39;`evenness` ({}) must be either a string or an array&#39;.format(
                type(evenness)))

    # Initialize clustering object
    logging.info(&#39;cluster assignments: {}&#39;.format(clusters))
    self.dynamics.clustering = pl.Clustering(clusters = clusters, items=self.asvs, G=self.G)
    return self</code></pre>
</details>
</dd>
<dt id="mdsine2.synthetic.SyntheticData.set_single_perturbation"><code class="name flex">
<span>def <span class="ident">set_single_perturbation</span></span>(<span>self, start, end, magnitude, indicator)</span>
</code></dt>
<dd>
<div class="desc"><p>Sets a single perturbation - no sampling. This assumes that you have
initialized the system - (<code>clustering</code> is not None)</p>
<h2 id="parameters">Parameters</h2>
<p>start, end (float)
- Time to start/end the perturbation
magnitude (float)
- This is the strength of the perturbation
indicator (int, np.ndarray)
- If it is an int, this is the cluster id that it is positive for
- If it is an array, it must either be the length of the
number of clusters, and must be either a boolean or 1,0s</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_single_perturbation(self, start, end, magnitude, indicator):
    &#39;&#39;&#39;Sets a single perturbation - no sampling. This assumes that you have
    initialized the system - (`clustering` is not None)

    Parameters
    ----------
    start, end (float)
        - Time to start/end the perturbation
    magnitude (float)
        - This is the strength of the perturbation
    indicator (int, np.ndarray)
        - If it is an int, this is the cluster id that it is positive for
        - If it is an array, it must either be the length of the
          number of clusters, and must be either a boolean or 1,0s
    &#39;&#39;&#39;
    if self.dynamics.clustering is None:
        raise ValueError(&#39;Must sample the system before you call this function&#39;)
    a = pl.contrib.ClusterPerturbation(start=start, end=end, magnitude=magnitude,
        indicator=indicator, clustering=self.dynamics.clustering, G=self.G)
    if self.dynamics.perturbations is None:
        self.dynamics.perturbations = [a]
    else:
        self.dynamics.perturbations.append(a)
    return a</code></pre>
</details>
</dd>
<dt id="mdsine2.synthetic.SyntheticData.set_timepoints"><code class="name flex">
<span>def <span class="ident">set_timepoints</span></span>(<span>self, times)</span>
</code></dt>
<dd>
<div class="desc"><p>Times to set the timepoints</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>times</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_timepoints(self, times):
    &#39;&#39;&#39;Times to set the timepoints

    Parameters
    ----------
    times : np.ndarray
    &#39;&#39;&#39;
    if not pl.isarray(times):
        raise TypeError(&#39;`times` ({}) must be an array&#39;.format(times))
    times = np.sort(np.array(times))
    self.master_times = times
    return self</code></pre>
</details>
</dd>
<dt id="mdsine2.synthetic.SyntheticData.set_times_without_timeseries"><code class="name flex">
<span>def <span class="ident">set_times_without_timeseries</span></span>(<span>self, N, D='auto', initial_growth=4, pretransition=1, posttransition=2, transition_density=2, uniform_sampling=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Set the time points for the replicates. </p>
<p>It is highly recommended that you call this function to set the timepoints
of the synthetic trajectories instead of directly calling <code><a title="mdsine2.synthetic.set_timepoints_without_timeseries" href="#mdsine2.synthetic.set_timepoints_without_timeseries">set_timepoints_without_timeseries()</a></code>
or setting them yourself.</p>
<h2 id="types-of-time-setting">Types Of Time Setting</h2>
<p>There are three ways we can set the spacing of the timepoints: (1) We can
set them with a uniform spacing by setting the parameter <code>uniform_spacing=True</code>.
(2) We can space the timepoints non-uniformly using the algorithm
<code>synthetic.set_timepoints_without_timeseries</code>. (3) We can manually set the times with the parameter
<code>N</code> if <code>N</code> is an array. In terms of precidence of the parameters:
(1) If <code>N</code> is an array, then we set them according to times
(2) If <code>uniform_sampling = True</code>, then we ignore the parameters for the
non-uniform
(3) If <code>uniform_sampling = False</code>, then we do non-uniform sampling</p>
<h2 id="non-uniform-sampling">Non-uniform sampling</h2>
<p>We set the times according to the denisty intervals sepcified
in D. If D is auto, we set the density to be <code>transition_density</code>
more intense than regular time points in the following scenarios:
- For <code>initial_growth</code> days from the start of the trajectory
- For <code>pretransition</code> days before a transition from off- and
on- a perturbation (starting or ending of a perturbation)
- For <code>posttransition</code> days after a transition from off- and
on- a perturbation (starting or ending of a perturbation)</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>N</code></strong> :&ensp;<code>int, array</code></dt>
<dd>If an int, it represents how many timepoints to allocate. Look at
<code><a title="mdsine2.synthetic.set_timepoints_without_timeseries" href="#mdsine2.synthetic.set_timepoints_without_timeseries">set_timepoints_without_timeseries()</a></code> for more information.
If an array, these are the times to set.</dd>
<dt><strong><code>D</code></strong> :&ensp;<code>str, 3-tuple, list(3-tuple)</code></dt>
<dd>If this is a string, then we set it according to the densities
specified above. Otherwise, these are a list of intervals that
we want specific densities for. If all of the densities are not
adjacent then we add in intervals in between with a density of 1.
None of the densities listed in D can overlap. For options on</dd>
<dt><strong><code>initial_growth</code></strong> :&ensp;<code>numeric, None</code></dt>
<dd>How many days to double sample during the first <code>initial_growth</code> days.
If None then we do not set the initial growth</dd>
<dt><strong><code>pretransition</code></strong>, <strong><code>posttransition</code></strong> :&ensp;<code>numeric</code></dt>
<dd>How many days before and after, respectively, of a perturbation
transition to have double density. If None then we do not set it.</dd>
<dt><strong><code>transition_density</code></strong> :&ensp;<code>numeric</code></dt>
<dd>How dense to make the higher densities (initial growth and transition
times). Must be &gt;= 1</dd>
</dl>
<h2 id="see-also">See Also</h2>
<p><code>synthetic.set_timepoints_without_timeseries</code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_times_without_timeseries(self, N, D=&#39;auto&#39;, initial_growth=4, pretransition=1, 
    posttransition=2, transition_density=2, uniform_sampling=False):
    &#39;&#39;&#39;Set the time points for the replicates. 
    
    It is highly recommended that you call this function to set the timepoints
    of the synthetic trajectories instead of directly calling `set_timepoints_without_timeseries` 
    or setting them yourself.

    Types of time setting
    ---------------------
    There are three ways we can set the spacing of the timepoints: (1) We can
    set them with a uniform spacing by setting the parameter `uniform_spacing=True`.
    (2) We can space the timepoints non-uniformly using the algorithm 
    `synthetic.set_timepoints_without_timeseries`. (3) We can manually set the times with the parameter 
    `N` if `N` is an array. In terms of precidence of the parameters:
        (1) If `N` is an array, then we set them according to times
        (2) If `uniform_sampling = True`, then we ignore the parameters for the
            non-uniform
        (3) If `uniform_sampling = False`, then we do non-uniform sampling

    Non-uniform sampling
    --------------------
    We set the times according to the denisty intervals sepcified
    in D. If D is auto, we set the density to be `transition_density` 
    more intense than regular time points in the following scenarios:
        - For `initial_growth` days from the start of the trajectory
        - For `pretransition` days before a transition from off- and
          on- a perturbation (starting or ending of a perturbation)
        - For `posttransition` days after a transition from off- and
          on- a perturbation (starting or ending of a perturbation)

    Parameters
    ----------
    N : int, array
        If an int, it represents how many timepoints to allocate. Look at 
        `set_timepoints_without_timeseries` for more information.
        If an array, these are the times to set.
    D : str, 3-tuple, list(3-tuple)
        If this is a string, then we set it according to the densities
        specified above. Otherwise, these are a list of intervals that
        we want specific densities for. If all of the densities are not
        adjacent then we add in intervals in between with a density of 1.
        None of the densities listed in D can overlap. For options on 
    initial_growth : numeric, None
        How many days to double sample during the first `initial_growth` days.
        If None then we do not set the initial growth
    pretransition, posttransition : numeric
        How many days before and after, respectively, of a perturbation 
        transition to have double density. If None then we do not set it.
    transition_density : numeric
        How dense to make the higher densities (initial growth and transition
        times). Must be &gt;= 1

    See also
    --------
    synthetic.set_timepoints_without_timeseries
    &#39;&#39;&#39;
    if pl.isarray(N):
        # Set times manually
        N = np.sort(np.unique(N))
        if np.any(N &lt; 0):
            raise ValueError(&#39;Every value in `times` ({}) must be positive or 0&#39;.format(N))
        if np.any(N &gt; self.n_days):
            raise ValueError(&#39;Values in `N` ({}) out of range&#39;.format(N))
        self.master_times = N
        return
        
    if not pl.isbool(uniform_sampling):
        raise TypeError(&#39;`uniform_sampling` ({}) must be a bool&#39;.format(type(uniform_sampling)))
    
    if uniform_sampling:
        if not pl.isint(N):
            raise TypeError(&#39;`N` ({}) must be an int&#39;.format(type(N)))
        if N &lt;= 0:
            raise ValueError(&#39;`N` ({}) must be &gt; 0&#39;.format(N))
        ts = np.arange(0,self.n_days, step=self.n_days/N)
        for i in range(len(ts)):
            ts[i] = round(ts[i], 2)
        self.master_times = ts
        return

    # Else do non-uniform sampling
    if D == &#39;auto&#39;:
        if not pl.isnumeric(transition_density):
            raise TypeError(&#39;`transition_density` ({}) must be a numeric&#39;.format(
                type(transition_density)))
        if transition_density &lt; 1:
            raise TypeError(&#39;`transition_density` ({}) must be &gt;= 1&#39;.format(
                transition_density))
        if initial_growth is not None:
            if not pl.isnumeric(initial_growth):
                raise TypeError(&#39;`initial_growth` ({}) should be a numeric&#39;.format( 
                    type(initial_growth)))
            if initial_growth &lt; 0:
                raise ValueError(&#39;`initial_growth` ({}) must be positive&#39;.format(
                    initial_growth))
        if pretransition is not None:
            if not pl.isnumeric(pretransition):
                raise TypeError(&#39;`pretransition` ({}) should be a numeric&#39;.format( 
                    type(pretransition)))
            if pretransition &lt; 0:
                raise ValueError(&#39;`pretransition` ({}) must be positive&#39;.format(
                    pretransition))
        if posttransition is not None:
            if not pl.isnumeric(posttransition):
                raise TypeError(&#39;`posttransition` ({}) should be a numeric&#39;.format( 
                    type(posttransition)))
            if posttransition &lt; 0:
                raise ValueError(&#39;`posttransition` ({}) must be positive&#39;.format(
                    posttransition))

        # Sort perturbations if there are
        if self.dynamics.perturbations is not None:
            perts = []
            pert_starts = []
            for perturbation in self.dynamics.perturbations:
                perts.append((perturbation.start, perturbation.end))
                pert_starts.append(perturbation.start)
            
            idxs = np.argsort(pert_starts)
            temp = []
            for idx in idxs:
                temp.append(perts[idx])
            perts = temp

            # fail if the start or end of a perturbation is greater than n_days
            for s,e in perts:
                if s &gt;= self.n_days or e &gt; self.n_days:
                    raise ValueError(&#39;Perturbation start and end (`{}`,`{}`) &#39; \
                        &#39; is out of range for the number of days `{}`&#39;.format( 
                            s,e,self.n_days))
        else:
            perts = None

        # Add the densities in order
        D = []
        
        # Set initial growth
        l = np.min([initial_growth, self.n_days])
        D.append((0, l, transition_density))

        # Set for each perturbation:
        if perts is not None:
            for s, e in perts:
                D.append((s-pretransition, s+posttransition, transition_density))
                l = np.min([e+posttransition,self.n_days])
                D.append((e-pretransition, l, transition_density))
    else:
        # check D
        if pl.istuple(D):
            D = [D]
        if not pl.isarray(D):
            raise TypeError(&#39;`D` ({}) must be an array&#39;.format(type(D)))
        for ele in D:
            if not pl.istuple(ele):
                raise ValueError(&#39;Each element in D ({}) must be a tuple&#39;.format(
                    type(ele)))
            if len(ele) != 3:
                raise ValueError(&#39;Each element in D must have 3 elements ({})&#39; \
                    &#39;&#39;.format(len(ele)))
            
            s,e,d = ele
            if not np.all(pl.itercheck([s,e,d], pl.isnumeric)):
                raise TypeError(&#39;All values in ({},{},{}) must be numerics&#39;.format( 
                    type(s), type(e), type(d)))
            if s &lt; 0 or s &gt;= self.n_days or s &gt;= e:
                raise ValueError(&#39;`start` ({}) out of range&#39;.format(s))
            if e &gt; self.n_days:
                raise ValueError(&#39;`end` ({}) out of range&#39;.format(e))
            if d &lt; 1:
                raise ValueError(&#39;`density` ({}) must be &gt;= 1&#39;.format(d))
        
        # Order by the start
        starts = [s for (s,e,d) in D]
        idxs = np.argsort(starts)
        temp = []
        for idx in idxs:
            temp.append(D[idx])
        D = temp

        # Check if any of the intervals overlap
        # If they overlap and they have different densities then 
        # throw an error
        for i in range(len(D)-1):
            si, ei, di = D[i]
            sj, ej, dj = D[i+1]

            if si == sj:
                raise ValueError(&#39;Two intervals ({} and {}) cannot have the &#39; \
                    &#39;same start point&#39;.format(D[i], D[i+1]))
            if ei &gt; sj:
                if di != dj:
                    raise ValueError(&#39;Intervals {} and {} overlap and have &#39; \
                        &#39;different densities&#39;.format(D[i], D[i+1]))

    # Merge any overlapping time periods. If the time periods are not 
    # overlapping then add in extra intervals with the background density
    # 1
    D_new = [D[0]]
    i = 1
    while True:
        if i == len(D):
            break

        # Compare the last element in D_new to the ith element of D
        si, ei, di = D_new[-1]
        sj, ej, dj = D[i]

        if ei &gt; sj and ei &lt; ej:
            # they overlap, merge
            if di != dj:
                raise ValueError(&#39;Something went wrong. D_new: {}, D: {}, &#39; \
                    &#39;i: {}&#39;.format(D_new, D, i))
            D[-1] = (si, ej, di)
        else:
            if ei &lt; sj:
                # These do not overlap, add an intermediate
                D_new.append((ei, sj, 1))
            elif ei != sj:
                raise ValueError(&#39;Something went wrong. D_new: {}, D: {}, &#39; \
                    &#39;i: {}&#39;.format(D_new, D, i))
            D_new.append(D[i])
        i += 1
    D = D_new

    # check if the last timepoint goes up to the last time
    s,e,d = D[-1]
    if e &lt; self.n_days:
        D.append((e,self.n_days,1))
    if e &gt; self.n_days:
        raise ValueError(&#39;Last interval {} ends after the set number of days {}&#39; \
            &#39;&#39;.format(D[-1], self.n_days))

    # Set the days that the perturbations start and end as essential
    essential_timepoints = None
    if self.dynamics.perturbations is not None:
        essential_timepoints = []
        for perturbation in self.dynamics.perturbations:
            essential_timepoints += [perturbation.start, perturbation.end]

    # Set the timepoints for each replicate
    self.master_times = set_timepoints_without_timeseries(N=N, T_start=0, T_end=self.n_days, D=D,
        move_timepoints_if_fail=True, essential_timepoints=essential_timepoints)</code></pre>
</details>
</dd>
<dt id="mdsine2.synthetic.SyntheticData.shuffle_cluster_assignments"><code class="name flex">
<span>def <span class="ident">shuffle_cluster_assignments</span></span>(<span>self, p)</span>
</code></dt>
<dd>
<div class="desc"><p>Shuffle the cluster assignments that were specified in <code>set_cluster_assignments</code>.</p>
<p><code>p</code> indicates what proportion of the ASVs to be reassigned. Example: <code>p=.1</code> means
that you want to shuffle 10% of the ASVs.</p>
<p>NOTE: THIS SHOULD BE CALLED BEFORE YOU CALL THE FUNCTION <code>sample_dynamics</code>.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>p</code></strong> :&ensp;<code>float</code></dt>
<dd>Proportion of the ASVs to be shuffled</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def shuffle_cluster_assignments(self, p):
    &#39;&#39;&#39;Shuffle the cluster assignments that were specified in `set_cluster_assignments`.

    `p` indicates what proportion of the ASVs to be reassigned. Example: `p=.1` means
    that you want to shuffle 10% of the ASVs.

    NOTE: THIS SHOULD BE CALLED BEFORE YOU CALL THE FUNCTION `sample_dynamics`.

    Parameters
    ----------
    p : float
        Proportion of the ASVs to be shuffled
    &#39;&#39;&#39;
    if self.dynamics.clustering is None:
        raise ValueError(&#39;Must specfiy the clusters before you call this function&#39;)
    if not pl.isnumeric(p):
        raise TypeError(&#39;`p` ({}) should be a numeric&#39;.format(type(p)))
    if p &lt; 0 or p &gt; 1:
        raise ValueError(&#39;`p` ({}) should be 0 =&lt; p =&lt; 1&#39;.format(p))

    n = int(p*len(self.asvs))
    oidxs = np.random.randint(len(self.asvs), size=n)

    logging.info(&#39;ASV indices to shuffle: {}&#39;.format(oidxs))

    for oidx in oidxs:
        curr_cid = self.dynamics.clustering.idx2cid[oidx]
        assigned_cid = curr_cid
        while assigned_cid == curr_cid:
            assigned_cid = random.choice(self.dynamics.clustering.order)

        self.dynamics.clustering.move_item(idx=oidx, cid=assigned_cid)

    logging.info(&#39;new cluster assignments: {}&#39;.format(self.dynamics.clustering.toarray()))</code></pre>
</details>
</dd>
<dt id="mdsine2.synthetic.SyntheticData.simulateExactSubjset"><code class="name flex">
<span>def <span class="ident">simulateExactSubjset</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>This function is effectively the same as <code>simulateRealRegressionData*</code> but
we add an :math:<code>\epsilon</code> amount of measurement noise.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pylab.Base.Study</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def simulateExactSubjset(self):
    &#39;&#39;&#39;This function is effectively the same as `simulateRealRegressionData*` but 
    we add an :math:`\epsilon` amount of measurement noise.

    Returns
    -------
    pylab.Base.Study
    &#39;&#39;&#39;
    subjset = pl.Study(asvs=self.asvs)
    data = self.data
    times = self.times

    for ridx in range(len(data)):
        mid = str(ridx)
        subjset.add(name=mid)

        for tidx in range(len(times[ridx])):
            t = times[ridx][tidx]

            # Make &#34;exact&#34; counts we read depth at ~1000000
            total_abund = np.sum(data[ridx][:,tidx])
            rel = data[ridx][:,tidx]/total_abund
            counts = np.asarray(rel * 1000000, dtype=int)
            subjset[mid].reads[t] = counts

            # make &#34;exact&#34; qpcr by having very little qpcr noise
            subjset[mid].qpcr[t] = pl.qPCRdata( 
                cfus=pl.random.normal.sample(mean=total_abund, std=1e-10, size=3), 
                mass=1., dilution_factor=1.)
        subjset[mid].times = times[ridx]

    # Add in the perturbations
    if self.dynamics.perturbations is not None:
        for perturbation in self.dynamics.perturbations:
            subjset.add_perturbation(perturbation.start, perturbation.end, 
                name=perturbation.name)
    
    return subjset</code></pre>
</details>
</dd>
<dt id="mdsine2.synthetic.SyntheticData.simulateRealRegressionDataDMD"><code class="name flex">
<span>def <span class="ident">simulateRealRegressionDataDMD</span></span>(<span>self, subjset, alpha, qpcr_scale=None)</span>
</code></dt>
<dd>
<div class="desc"><p>This function converts the synthetic trajectories into "real" data
by simulating read counts and qPCR measurements. We base the sampling
on parameters that we learn from the real data (MouseSet <code>subjset</code>). We assume
that the real data has already been filtered.</p>
<p>This uses a dirichlet multinomial to simulate the reads</p>
<h2 id="simulating-qpcr-measurements">Simulating Qpcr Measurements</h2>
<p>We simulate the qPCR measurements with a lognormal distribution that has
the standard deviation <code>qpcr_scale</code>. If <code>qpcr_scale</code> is not given, then we fit
fit the qPCR measurements in <code>subjset</code> and use that value.</p>
<h2 id="simulating-count-data">Simulating Count Data</h2>
<p>We first simulate the read depth for each day from a negative binomial
that was fitted using the read depth of our data from <code>subjset</code> (using function
minimization). To generate the indivual counts for each ASV, we then 'sample'
from a dirichlet multinomial using the sample read depth as our counts and the
<code>alpha</code> parameter as the multiplicative concentration for our relative abundances, where
$\mathbf{lpha}_i = lpha * r_i$,
$\mathbf{lpha}_i$ is the concentration parameter for ASV $i$ to sample from a dirichlet
distribution,
$r_i$ is the relative abundance for ASV $i$,
$lpha$ is the input concentration
To emulate a dirichlet multinomial distribution, we first sample the relative abundances
of each of the ASVs from a dirichlet distribution using our alpha term as sepcified above,
and then sample from a multinomial distribution given our sampled a read depth and our
probabilities sampled concentrations for each ASV from the dirichlet distribution.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>subjset</code></strong> :&ensp;<code>pl.Study</code></dt>
<dd>This is the real data that we are fitting to</dd>
<dt><strong><code>alpha</code></strong> :&ensp;<code>numeric, Optional</code></dt>
<dd>This is a parameter for how much noise you have in sampling the individual reads.
The larger the number, the less noise there is in the system. The smaller the number,
the more noise there is in sampling the relative abundances</dd>
<dt><strong><code>qpcr_scale</code></strong> :&ensp;<code>numeric, None, Opional</code></dt>
<dd>This is the scale of the lognormal distribution used to generate the qPCR measurements
of the simulated data</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pl.Study</code></dt>
<dd>This is the subject set that contains the data in terms of counts and absolute abundance</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def simulateRealRegressionDataDMD(self, subjset, alpha, qpcr_scale=None):
    &#39;&#39;&#39;This function converts the synthetic trajectories into &#34;real&#34; data
    by simulating read counts and qPCR measurements. We base the sampling
    on parameters that we learn from the real data (MouseSet `subjset`). We assume
    that the real data has already been filtered.

    This uses a dirichlet multinomial to simulate the reads

    Simulating qPCR measurements
    ----------------------------
    We simulate the qPCR measurements with a lognormal distribution that has
    the standard deviation `qpcr_scale`. If `qpcr_scale` is not given, then we fit
    fit the qPCR measurements in `subjset` and use that value.

    Simulating count data
    ---------------------
    We first simulate the read depth for each day from a negative binomial
    that was fitted using the read depth of our data from `subjset` (using function 
    minimization). To generate the indivual counts for each ASV, we then &#39;sample&#39; 
    from a dirichlet multinomial using the sample read depth as our counts and the 
    `alpha` parameter as the multiplicative concentration for our relative abundances, where
        $\mathbf{\alpha}_i = \alpha * r_i$,
        $\mathbf{\alpha}_i$ is the concentration parameter for ASV $i$ to sample from a dirichlet
        distribution,
        $r_i$ is the relative abundance for ASV $i$,
        $\alpha$ is the input concentration
    To emulate a dirichlet multinomial distribution, we first sample the relative abundances 
    of each of the ASVs from a dirichlet distribution using our alpha term as sepcified above, 
    and then sample from a multinomial distribution given our sampled a read depth and our 
    probabilities sampled concentrations for each ASV from the dirichlet distribution.

    Parameters
    ----------
    subjset : pl.Study
        This is the real data that we are fitting to
    alpha : numeric, Optional
        This is a parameter for how much noise you have in sampling the individual reads.
        The larger the number, the less noise there is in the system. The smaller the number,
        the more noise there is in sampling the relative abundances
    qpcr_scale : numeric, None, Opional
        This is the scale of the lognormal distribution used to generate the qPCR measurements
        of the simulated data

    Returns
    -------
    pl.Study
        This is the subject set that contains the data in terms of counts and absolute abundance
    &#39;&#39;&#39;
    logging.info(&#39;Fitting real data&#39;)
    # load Study and filter
    if type(subjset) == str:
        subjset = pl.Study.load(subjset)
    elif not pl.isstudy(subjset):
        raise ValueError(&#39;`subjset` ({}) must eb a pl.Study object&#39;.format(
            type(subjset)))
    if not pl.isnumeric(alpha):
        raise ValueError(&#39;`alpha` ({}) must either be a float or an int&#39;.format(type(alpha)))
    elif alpha &lt; 0:
        raise ValueError(&#39;`alpha` ({}) must be greater than 0&#39;.format(alpha))
    if qpcr_scale is not None:
        if not pl.isnumeric(qpcr_scale):
            raise ValueError(&#39;`qpcr_scale` ({}) must either be a float or an int&#39;.format(
                type(qpcr_scale)))
        elif qpcr_scale &lt; 0:
            raise ValueError(&#39;`qpcr_scale` ({}) must be greater than 0&#39;.format(
                qpcr_scale))
    
    # Fit qPCR with lognormal if necessary
    if qpcr_scale is not None:
        data = []
        for subj in subjset:
            for t, qpcr in subj.qpcr.items():
                d = np.log(qpcr.data)
                data = np.append(data, d - np.mean(d))
        qpcr_scale = np.std(data)
        logging.info(&#39;qpcr fitted scale: {}&#39;.format(qpcr_scale))

    # Fit read depth with negative binomial
    read_depths = np.asarray([])
    for subj in subjset:
        read_depths = np.append(read_depths, subj.read_depth())
    n_pred, p_pred = _fit_nbinom(read_depths)
    logging.info(&#39;read depth negbin n: {}, p: {}&#39;.format(n_pred, p_pred))
    readdepth_negbin = scipy.stats.nbinom(n_pred, p_pred)
    
    # Make data, record the data with noise
    ret_subjset = pl.Study(asvs=self.asvs)
    for ridx in range(self.n_replicates):
        mid = str(ridx)
        ret_subjset.add(name=mid)

        for tidx in range(len(self.times[ridx])):
            # make time id
            t = self.times[ridx][tidx]

            # Sample counts
            sum_abund = np.sum(self.data[ridx][:,tidx])
            rel = self.data[ridx][:,tidx] / sum_abund
            probs = (scipy.stats.dirichlet.rvs(rel*alpha)).flatten()
            read_depth = readdepth_negbin.rvs()
            ret_subjset[mid].reads[t] = scipy.stats.multinomial.rvs(read_depth, probs).ravel()

            # Sample qPCR
            triplicates = np.exp(np.log(sum_abund) + qpcr_scale * npr.normal(size=3))
            ret_subjset[mid].qpcr[t] = pl.qPCRdata(cfus=triplicates, mass=1., 
                dilution_factor=1.)

        ret_subjset[mid].times = self.times[ridx]
    
    # Add in the perturbations
    if self.dynamics.perturbations is not None:
        for perturbation in self.dynamics.perturbations:
            ret_subjset.add_perturbation(perturbation.start, perturbation.end)
    return ret_subjset</code></pre>
</details>
</dd>
<dt id="mdsine2.synthetic.SyntheticData.simulateRealRegressionDataNegBinMD"><code class="name flex">
<span>def <span class="ident">simulateRealRegressionDataNegBinMD</span></span>(<span>self, a0, a1, qpcr_noise_scale, subjset, replicates='all', read_depth=None)</span>
</code></dt>
<dd>
<div class="desc"><p>This function converts the synthetic trajectories into "real" data
by simulating read counts and qPCR measurements. We base the sampling
on parameters that we learn from the real data (MouseSet <code>subjset</code>). We assume
that the real data has already been filtered.</p>
<p>This uses a negative binomial distribution to simiulate the reads.</p>
<h2 id="simulating-qpcr-measurements">Simulating Qpcr Measurements</h2>
<p>We simulate the qPCR measurements with a lognormal distribution that was
fitted using the data from <code>subjset</code>. We use this parameterization to sample
a qPCR measurement with mean from the total biomass of the simulated data.</p>
<h2 id="simulating-count-data">Simulating Count Data</h2>
<p>We first fit the read depth for each day (<code>r_k</code>) from a negative binomial from the
read depth of our data (<code>subjset</code>) using function minimization. We then use <code>r_k</code>,
<code>a_0</code>, and <code>a_1</code> with the relative abundances to sample from a negative binomial
distirbution. We then use the relative abundances from this sample as the concentrations
for a multinomial distribution with read depth <code>r_k</code>.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>subjset</code></strong> :&ensp;<code>pl.Study</code></dt>
<dd>This is the real data that we are fitting to</dd>
<dt><strong><code>a0</code></strong>, <strong><code>a1</code></strong> :&ensp;<code>numeric</code></dt>
<dd>These are the negative binomial dispersion parameters that we are using to
simulate the data</dd>
<dt><strong><code>qpcr_noise_scale</code></strong> :&ensp;<code>numeric</code></dt>
<dd>This is the parameter to scale the <code>s</code> parameter learned by the lognormal
distribution.</dd>
<dt><strong><code>replicates</code></strong> :&ensp;<code>str, array, int</code></dt>
<dd>Which replicates to make the data for. If <code>replicates='all'</code>, then we make
it for all of the replicates. If it is an array, we assume it is an array of
replicate indices of the replicates that you want.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pl.Study</code></dt>
<dd>This is the subject set that contains the data in terms of counts and absolute abundance</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def simulateRealRegressionDataNegBinMD(self, a0, a1, qpcr_noise_scale, subjset, replicates=&#39;all&#39;, read_depth=None):
    &#39;&#39;&#39;This function converts the synthetic trajectories into &#34;real&#34; data
    by simulating read counts and qPCR measurements. We base the sampling
    on parameters that we learn from the real data (MouseSet `subjset`). We assume
    that the real data has already been filtered.

    This uses a negative binomial distribution to simiulate the reads.

    Simulating qPCR measurements
    ----------------------------
    We simulate the qPCR measurements with a lognormal distribution that was
    fitted using the data from `subjset`. We use this parameterization to sample
    a qPCR measurement with mean from the total biomass of the simulated data.

    Simulating count data
    ---------------------
    We first fit the read depth for each day (`r_k`) from a negative binomial from the 
    read depth of our data (`subjset`) using function minimization. We then use `r_k`, 
    `a_0`, and `a_1` with the relative abundances to sample from a negative binomial 
    distirbution. We then use the relative abundances from this sample as the concentrations
    for a multinomial distribution with read depth `r_k`.

    Parameters
    ----------
    subjset : pl.Study
        This is the real data that we are fitting to
    a0, a1 : numeric
        These are the negative binomial dispersion parameters that we are using to 
        simulate the data
    qpcr_noise_scale : numeric
        This is the parameter to scale the `s` parameter learned by the lognormal
        distribution.
    replicates : str, array, int
        Which replicates to make the data for. If `replicates=&#39;all&#39;`, then we make
        it for all of the replicates. If it is an array, we assume it is an array of
        replicate indices of the replicates that you want.

    Returns
    -------
    pl.Study
        This is the subject set that contains the data in terms of counts and absolute abundance
    &#39;&#39;&#39;
    logging.info(&#39;Fitting real data&#39;)
    if pl.isstr(subjset):
        subjset = pl.base.Study.load(subjset)
    elif not pl.isstudy(subjset):
        raise TypeError(&#39;`subjset` ({}) must be a pylab.base.SubjsetSet&#39;.format( 
            type(subjset)))
    if not pl.isnumeric(qpcr_noise_scale):
        raise TypeError(&#39;`qpcr_noise_scale` ({}) must either be a float or an int&#39;.format(
            type(qpcr_noise_scale)))
    elif qpcr_noise_scale &lt; 0:
        raise ValueError(&#39;`qpcr_noise_scale` ({}) must be greater than 0&#39;.format(
            qpcr_noise_scale))
    if pl.isstr(replicates):
        if replicates == &#39;all&#39;:
            replicates = np.arange(len(self.data))
        else:
            raise ValueError(&#39;`replicates` ({}) not recognized&#39;.format(replicates))
    elif pl.isint(replicates):
        replicates = [replicates]
    if pl.isarray(replicates):
        for i in replicates:
            if not pl.isint(i):
                raise TypeError(&#39;`replicates` ({}) must be ints&#39;.format(type(i)))
            if i &gt;= len(self.data):
                raise IndexError(&#39;`replicates` ({}) out of range ({})&#39;.format(
                    i, len(self.data)))
    else:
        raise TypeError(&#39;`replicates` ({}) type not recognized&#39;.format(type(replicates)))


    # # Fit qPCR with lognormal
    # data = []
    # for subj in subjset:
    #     for t, qpcr in subj.qpcr.items():
    #         if np.any(np.isnan(qpcr.data)):
    #             continue
    #         data.append(qpcr.data)
    # std_biomass = _fit_qpcr(data) * self.qpcr_noise_scale
    std_biomass = qpcr_noise_scale
    logging.info(&#39;lognormal s: {}&#39;.format(std_biomass))

    # Fit read depth with negative binomial
    if read_depth is None:
        read_depths = np.asarray([])
        for subj in subjset:
            read_depths = np.append(read_depths, subj.read_depth())
        n_pred, p_pred = _fit_nbinom(read_depths)
        logging.info(&#39;negbin n: {}, p: {}&#39;.format(n_pred, p_pred))
        negbin_read_depth = scipy.stats.nbinom(n_pred, p_pred)

    # Make data, record the data with noise
    n_time_points = 0
    for times in self.times:
        n_time_points += len(times)

    ret_subjset = pl.Study(asvs=self.asvs)
    data = self.data
    times = self.times
    for ridx in replicates:
        mid = str(ridx)
        ret_subjset.add(name=mid)
        # self.data_w_noise.append(np.zeros(shape=data[ridx].shape,dtype=float))

        for tidx in range(len(times[ridx])):
            # make time id
            t = times[ridx][tidx]

            # Sample counts
            sum_abund = np.sum(data[ridx][:,tidx])
            rel = data[ridx][:,tidx] / sum_abund
            if read_depth is None:
                r_k = negbin_read_depth.rvs()
            else:
                r_k = read_depth
            read_depth = 75000

            phi = r_k * rel
            eps = a0 / rel + a1

            reads = pl.random.negative_binomial.sample(phi, eps)
            # concentration = reads/np.sum(reads)
            # ret_subjset[mid].reads[t] = scipy.stats.multinomial.rvs(
            #     r_k, concentration).ravel()
            ret_subjset[mid].reads[t] = reads

            # Sample qPCR
            triplicates = np.exp(np.log(sum_abund) + std_biomass * npr.normal(size=3))
            ret_subjset[mid].qpcr[t] = pl.qPCRdata(
                cfus=triplicates, mass=1., dilution_factor=1.)

        ret_subjset[mid].times = times[ridx]
    
    # Add in the perturbations
    if self.dynamics.perturbations is not None:
        for perturbation in self.dynamics.perturbations:
            ret_subjset.add_perturbation(perturbation.start, perturbation.end)
    return ret_subjset</code></pre>
</details>
</dd>
<dt id="mdsine2.synthetic.SyntheticData.simulate_qpcr"><code class="name flex">
<span>def <span class="ident">simulate_qpcr</span></span>(<span>self, qpcr_noise_scale)</span>
</code></dt>
<dd>
<div class="desc"><p>This function converts the synthetic trajectories into "real" data
by simulating read counts and qPCR measurements. We base the sampling
on parameters that we learn from the real data (MouseSet <code>subjset</code>). We assume
that the real data has already been filtered.</p>
<p>We assume that the function <code>simulate_reads</code></p>
<h2 id="simulating-qpcr-measurements">Simulating Qpcr Measurements</h2>
<p>We simulate the qPCR measurements with a lognormal distribution that was
fitted using the data from <code>subjset</code>. We use this parameterization to sample
a qPCR measurement with mean from the total biomass of the simulated data.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>qpcr_noise_scale</code></strong> :&ensp;<code>numeric</code></dt>
<dd>This is the parameter to scale the <code>s</code> parameter learned by the lognormal
distribution.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def simulate_qpcr(self, qpcr_noise_scale):
    &#39;&#39;&#39;This function converts the synthetic trajectories into &#34;real&#34; data
    by simulating read counts and qPCR measurements. We base the sampling
    on parameters that we learn from the real data (MouseSet `subjset`). We assume
    that the real data has already been filtered.

    We assume that the function `simulate_reads`

    Simulating qPCR measurements
    ----------------------------
    We simulate the qPCR measurements with a lognormal distribution that was
    fitted using the data from `subjset`. We use this parameterization to sample
    a qPCR measurement with mean from the total biomass of the simulated data.

    Parameters
    ----------
    qpcr_noise_scale : numeric
        This is the parameter to scale the `s` parameter learned by the lognormal
        distribution.
    &#39;&#39;&#39;
    if not pl.isnumeric(qpcr_noise_scale):
        raise TypeError(&#39;`qpcr_noise_scale` ({}) must either be a float or an int&#39;.format(
            type(qpcr_noise_scale)))
    elif qpcr_noise_scale &lt; 0:
        raise ValueError(&#39;`qpcr_noise_scale` ({}) must be greater than 0&#39;.format(
            qpcr_noise_scale))

    for ridx in np.arange(self.n_replicates):
        mid = str(ridx)
        subj = self.subjset_with_noise[mid]

        for tidx, t in enumerate(self.times[ridx]):
            
            # Sample qPCR
            sum_abund = np.sum(self.data[ridx][:,tidx])
            triplicates = np.exp(np.log(sum_abund) + qpcr_noise_scale * pl.random.normal.sample(size=3))
            subj.qpcr[t] = pl.qPCRdata(
                cfus=triplicates, mass=1., dilution_factor=1.)</code></pre>
</details>
</dd>
<dt id="mdsine2.synthetic.SyntheticData.simulate_reads"><code class="name flex">
<span>def <span class="ident">simulate_reads</span></span>(<span>self, a0, a1, read_depth)</span>
</code></dt>
<dd>
<div class="desc"><p>This function converts the synthetic trajectories into "real" data
by simulating read counts and qPCR measurements. We base the sampling
on parameters that we learn from the real data (MouseSet <code>subjset</code>). We assume
that the real data has already been filtered.</p>
<h2 id="simulating-count-data">Simulating Count Data</h2>
<p>We first fit the read depth for each day (<code>r_k</code>) from a negative binomial from the
read depth of our data (<code>subjset</code>) using function minimization. We then use <code>r_k</code>,
<code>a_0</code>, and <code>a_1</code> with the relative abundances to sample from a negative binomial
distirbution. We then use the relative abundances from this sample as the concentrations
for a multinomial distribution with read depth <code>r_k</code>.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>subjset</code></strong> :&ensp;<code>pl.Study</code></dt>
<dd>This is the real data that we are fitting to</dd>
<dt><strong><code>a0</code></strong>, <strong><code>a1</code></strong> :&ensp;<code>numeric</code></dt>
<dd>These are the negative binomial dispersion parameters that we are using to
simulate the data</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def simulate_reads(self, a0, a1, read_depth):
    &#39;&#39;&#39;This function converts the synthetic trajectories into &#34;real&#34; data
    by simulating read counts and qPCR measurements. We base the sampling
    on parameters that we learn from the real data (MouseSet `subjset`). We assume
    that the real data has already been filtered.

    Simulating count data
    ---------------------
    We first fit the read depth for each day (`r_k`) from a negative binomial from the 
    read depth of our data (`subjset`) using function minimization. We then use `r_k`, 
    `a_0`, and `a_1` with the relative abundances to sample from a negative binomial 
    distirbution. We then use the relative abundances from this sample as the concentrations
    for a multinomial distribution with read depth `r_k`.

    Parameters
    ----------
    subjset : pl.Study
        This is the real data that we are fitting to
    a0, a1 : numeric
        These are the negative binomial dispersion parameters that we are using to 
        simulate the data
    &#39;&#39;&#39;
    if not np.all(pl.itercheck([a0, a1], pl.isnumeric)):
        raise TypeError(&#39;`a0` ({}) and `a1` ({}) must be numerics&#39;.format(
            type(a0), type(a1)))
    if a0 &lt; 0 or a1 &lt; 0:
        raise ValueError(&#39;`a0` ({}) and `a1` ({}) must be &gt; 0&#39;.format(a0, a1))
    
    # Make data, record the data with noise
    n_time_points = 0
    for times in self.times:
        n_time_points += len(times)

    ret_subjset = pl.Study(asvs=self.asvs)
    data = self.data
    times = self.times
    for ridx in np.arange(self.n_replicates):
        mid = str(ridx)
        ret_subjset.add(name=mid)
        # self.data_w_noise.append(np.zeros(shape=data[ridx].shape,dtype=float))

        for tidx in range(len(times[ridx])):
            # make time id
            t = times[ridx][tidx]

            # Sample counts
            sum_abund = np.sum(data[ridx][:,tidx])
            rel = data[ridx][:,tidx] / sum_abund

            phi = read_depth * rel
            eps = a0 / rel + a1

            reads = pl.random.negative_binomial.sample(phi, eps)
            # concentration = reads/np.sum(reads)
            # ret_subjset[mid].reads[t] = scipy.stats.multinomial.rvs(
            #     r_k, concentration).ravel()
            ret_subjset[mid].reads[t] = reads

        ret_subjset[mid].times = times[ridx]
    
    # Add in the perturbations
    if self.dynamics.perturbations is not None:
        for perturbation in self.dynamics.perturbations:
            ret_subjset.add_perturbation(perturbation.start, perturbation.end)
    self.subjset_with_noise = ret_subjset</code></pre>
</details>
</dd>
<dt id="mdsine2.synthetic.SyntheticData.stability"><code class="name flex">
<span>def <span class="ident">stability</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def stability(self):
    return self.dynamics.stability()</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="mdsine2.pylab.base.Saveable" href="pylab/base.html#mdsine2.pylab.base.Saveable">Saveable</a></b></code>:
<ul class="hlist">
<li><code><a title="mdsine2.pylab.base.Saveable.load" href="pylab/base.html#mdsine2.pylab.base.Saveable.load">load</a></code></li>
<li><code><a title="mdsine2.pylab.base.Saveable.save" href="pylab/base.html#mdsine2.pylab.base.Saveable.save">save</a></code></li>
<li><code><a title="mdsine2.pylab.base.Saveable.set_save_location" href="pylab/base.html#mdsine2.pylab.base.Saveable.set_save_location">set_save_location</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul>
<li><a href="#references">References</a></li>
</ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="mdsine2" href="index.html">mdsine2</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="mdsine2.synthetic.issynthetic" href="#mdsine2.synthetic.issynthetic">issynthetic</a></code></li>
<li><code><a title="mdsine2.synthetic.make_semisynthetic" href="#mdsine2.synthetic.make_semisynthetic">make_semisynthetic</a></code></li>
<li><code><a title="mdsine2.synthetic.set_timepoints_without_timeseries" href="#mdsine2.synthetic.set_timepoints_without_timeseries">set_timepoints_without_timeseries</a></code></li>
<li><code><a title="mdsine2.synthetic.subsample_timepoints" href="#mdsine2.synthetic.subsample_timepoints">subsample_timepoints</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="mdsine2.synthetic.SyntheticData" href="#mdsine2.synthetic.SyntheticData">SyntheticData</a></code></h4>
<ul class="">
<li><code><a title="mdsine2.synthetic.SyntheticData.generate_trajectories" href="#mdsine2.synthetic.SyntheticData.generate_trajectories">generate_trajectories</a></code></li>
<li><code><a title="mdsine2.synthetic.SyntheticData.get_full_interaction_matrix" href="#mdsine2.synthetic.SyntheticData.get_full_interaction_matrix">get_full_interaction_matrix</a></code></li>
<li><code><a title="mdsine2.synthetic.SyntheticData.get_subjset" href="#mdsine2.synthetic.SyntheticData.get_subjset">get_subjset</a></code></li>
<li><code><a title="mdsine2.synthetic.SyntheticData.icml_perturbations" href="#mdsine2.synthetic.SyntheticData.icml_perturbations">icml_perturbations</a></code></li>
<li><code><a title="mdsine2.synthetic.SyntheticData.icml_topology" href="#mdsine2.synthetic.SyntheticData.icml_topology">icml_topology</a></code></li>
<li><code><a title="mdsine2.synthetic.SyntheticData.icml_topology_real" href="#mdsine2.synthetic.SyntheticData.icml_topology_real">icml_topology_real</a></code></li>
<li><code><a title="mdsine2.synthetic.SyntheticData.perturbations" href="#mdsine2.synthetic.SyntheticData.perturbations">perturbations</a></code></li>
<li><code><a title="mdsine2.synthetic.SyntheticData.sample_single_perturbation" href="#mdsine2.synthetic.SyntheticData.sample_single_perturbation">sample_single_perturbation</a></code></li>
<li><code><a title="mdsine2.synthetic.SyntheticData.set_asvs" href="#mdsine2.synthetic.SyntheticData.set_asvs">set_asvs</a></code></li>
<li><code><a title="mdsine2.synthetic.SyntheticData.set_cluster_assignments" href="#mdsine2.synthetic.SyntheticData.set_cluster_assignments">set_cluster_assignments</a></code></li>
<li><code><a title="mdsine2.synthetic.SyntheticData.set_single_perturbation" href="#mdsine2.synthetic.SyntheticData.set_single_perturbation">set_single_perturbation</a></code></li>
<li><code><a title="mdsine2.synthetic.SyntheticData.set_timepoints" href="#mdsine2.synthetic.SyntheticData.set_timepoints">set_timepoints</a></code></li>
<li><code><a title="mdsine2.synthetic.SyntheticData.set_times_without_timeseries" href="#mdsine2.synthetic.SyntheticData.set_times_without_timeseries">set_times_without_timeseries</a></code></li>
<li><code><a title="mdsine2.synthetic.SyntheticData.shuffle_cluster_assignments" href="#mdsine2.synthetic.SyntheticData.shuffle_cluster_assignments">shuffle_cluster_assignments</a></code></li>
<li><code><a title="mdsine2.synthetic.SyntheticData.simulateExactSubjset" href="#mdsine2.synthetic.SyntheticData.simulateExactSubjset">simulateExactSubjset</a></code></li>
<li><code><a title="mdsine2.synthetic.SyntheticData.simulateRealRegressionDataDMD" href="#mdsine2.synthetic.SyntheticData.simulateRealRegressionDataDMD">simulateRealRegressionDataDMD</a></code></li>
<li><code><a title="mdsine2.synthetic.SyntheticData.simulateRealRegressionDataNegBinMD" href="#mdsine2.synthetic.SyntheticData.simulateRealRegressionDataNegBinMD">simulateRealRegressionDataNegBinMD</a></code></li>
<li><code><a title="mdsine2.synthetic.SyntheticData.simulate_qpcr" href="#mdsine2.synthetic.SyntheticData.simulate_qpcr">simulate_qpcr</a></code></li>
<li><code><a title="mdsine2.synthetic.SyntheticData.simulate_reads" href="#mdsine2.synthetic.SyntheticData.simulate_reads">simulate_reads</a></code></li>
<li><code><a title="mdsine2.synthetic.SyntheticData.stability" href="#mdsine2.synthetic.SyntheticData.stability">stability</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.1</a>.</p>
</footer>
</body>
</html>