<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.1" />
<title>mdsine2.filtering API documentation</title>
<meta name="description" content="Filtering parameters for the posterior" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>mdsine2.filtering</code></h1>
</header>
<section id="section-intro">
<p>Filtering parameters for the posterior</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#39;&#39;&#39;Filtering parameters for the posterior
&#39;&#39;&#39;
import logging
import time

import numpy as np
import numpy.random as npr
import math

import matplotlib.pyplot as plt

from .util import negbin_loglikelihood_MH_condensed, \
    negbin_loglikelihood, build_prior_covariance, build_prior_mean, \
    prod_gaussians, negbin_loglikelihood_MH_condensed_not_fast, Loess
from .names import STRNAMES, REPRNAMES

from . import pylab as pl

_LOG_INV_SQRT_2PI = np.log(1/np.sqrt(2*math.pi))
def _normal_logpdf(value, mean, std):
    &#39;&#39;&#39;We use this function if `pylab.random.normal.logpdf` fails to compile,
    which can happen when running jobs on the cluster.
    &#39;&#39;&#39;
    return _LOG_INV_SQRT_2PI + (-0.5*((value-mean)/std)**2) - np.log(std)

class TrajectorySet(pl.graph.Node):
    &#39;&#39;&#39;This aggregates a set of trajectories from each set

    Parameters
    ----------
    name : str
        Name of the object
    G : pylab.graph.Graph
        Graph object to attach it to
    &#39;&#39;&#39;
    def __init__(self, name, G, **kwargs):
        pl.graph.Node.__init__(self, name=name, G=G)
        self.value = []
        n_asvs = self.G.data.n_asvs

        for ridx in range(self.G.data.n_replicates):
            n_timepoints = self.G.data.n_timepoints_for_replicate[ridx]

            # initialize values to zeros for initialization
            self.value.append(pl.variables.Variable(
                name=name+&#39;_ridx{}&#39;.format(ridx), G=G, shape=(n_asvs, n_timepoints),
                value=np.zeros((n_asvs, n_timepoints), dtype=float), **kwargs))
        prior = pl.variables.Normal(
            mean=pl.variables.Constant(name=self.name+&#39;_prior_mean&#39;, value=0, G=self.G),
            var=pl.variables.Constant(name=self.name+&#39;_prior_var&#39;, value=1, G=self.G),
            name=self.name+&#39;_prior&#39;, G=self.G)
        self.add_prior(prior)

    def __getitem__(self, ridx):
        return self.value[ridx]

    @property
    def sample_iter(self):
        return self.value[0].sample_iter

    def reset_value_size(self):
        &#39;&#39;&#39;Change the size of the trajectory when we set the intermediate timepoints
        &#39;&#39;&#39;
        n_asvs = self.G.data.n_asvs
        for ridx in range(len(self.value)):
            n_timepoints = self.G.data.n_timepoints_for_replicate[ridx]
            self.value[ridx].value = np.zeros((n_asvs, n_timepoints),dtype=float)
            self.value[ridx].set_value_shape(self.value[ridx].value.shape)

    def _vectorize(self):
        &#39;&#39;&#39;Get all the data in vector form
        &#39;&#39;&#39;
        vals = np.array([])
        for data in self.value:
            vals = np.append(vals, data.value)
        return vals

    def mean(self):
        return np.mean(self._vectorize())

    def var(self):
        return np.var(self._vectorize())

    def iter_indices(self):
        &#39;&#39;&#39;Iterate through the indices and the values
        &#39;&#39;&#39;
        for ridx in range(self.G.data.n_replicates):
            for tidx in range(self.G.data.n_timepoints_for_replicate[ridx]):
                for oidx in range(self.G.data.asvs.n_asvs):
                    yield (ridx, tidx, oidx)

    def set_trace(self, *args, **kwargs):
        for ridx in range(len(self.value)):
            self.value[ridx].set_trace(*args, **kwargs)

    def add_trace(self):
        for ridx in range(len(self.value)):
            # Set the zero inflation values to nans
            self.value[ridx].value[~self.G[REPRNAMES.ZERO_INFLATION].value[ridx]] = np.nan
            self.value[ridx].add_trace()

    def add_init_value(self):
        for ridx in range(len(self.value)):
            self.value[ridx].add_init_value()


class FilteringLogMP(pl.graph.Node):
    &#39;&#39;&#39;This is the posterior for the latent trajectory that are
    sampled using a standard normal Metropolis-Hastings proposal.

    This is the multiprocessing version of the class. All of the computation is
    done on the subject level in parallel.

    Parallelization Modes
    ---------------------
    &#39;debug&#39;
        If this is selected, then we dont actually parallelize, but we go in
        order of the objects in sequential order. We would do this if we want
        to benchmark within each processor or do easier print statements
    &#39;full&#39;
        This is where each subject gets their own process

    This assumes that we are using the log model
    &#39;&#39;&#39;
    def __init__(self, mp, zero_inflation_transition_policy,**kwargs):
        &#39;&#39;&#39;
        Parameters
        ----------
        mp : str
            &#39;debug&#39;
                Does not actually parallelize, does it serially - we do this in case we
                want to debug and/or benchmark
            &#39;full&#39;
                Send each replicate to a processor each
        zero_inflation_transition_policy : None, str
            Type of zero inflation to do. If None then there is no zero inflation
        &#39;&#39;&#39;
        kwargs[&#39;name&#39;] = STRNAMES.FILTERING
        pl.graph.Node.__init__(self, **kwargs)
        self.x = TrajectorySet(name=STRNAMES.LATENT_TRAJECTORY, G=self.G)
        self.mp = mp
        self.zero_inflation_transition_policy = zero_inflation_transition_policy

        self.print_vals = False
        self._strr = &#39;parallel&#39;

    def __str__(self):
        return self._strr

    @property
    def sample_iter(self):
        # It doesnt matter if we chose q or x because they are both the same
        return self.x.sample_iter

    def initialize(self, x_value_option, a0, a1, v1, v2, essential_timepoints, tune, 
        proposal_init_scale, intermediate_step, intermediate_interpolation=None, 
        delay=0, bandwidth=None, window=None, target_acceptance_rate=0.44, 
        calculate_qpcr_loglik=True):
        &#39;&#39;&#39;Initialize the values of the error model (values for the
        latent and the auxiliary trajectory). Additionally this sets
        the intermediate time points

        Initialize the values of the prior.

        Parameters
        ----------
        x_value_option : str
            Option to initialize the value of the latent trajectory.
            Options
                &#39;coupling&#39;
                    Sample the values around the data with extremely low variance.
                    This also truncates the data so that it stays &gt; 0.
                &#39;moving-avg&#39;
                    Initialize the values using a moving average around the points.
                    The bandwidth of the filter is by number of days, not the order
                    of timepoints. You must also provide the argument `bandwidth`.
                &#39;loess&#39;, &#39;auto&#39;
                    Implements the initialization of the values using LOESS (Locally
                    Estimated Scatterplot Smoothing) algorithm. You must also provide
                    the `window` parameter
        tune : tuple(int, int)
            This is how often to tune the individual covariances
            The first element indicates which MCMC sample to stop the tuning
            The second element is how often to update the proposal covariance
        a0, a1 : float, str
            These are the hyperparameters to calculate the dispersion of the
            negative binomial.
        v1, v2 : float, int, str
            These are the values used to calulcate the coupling variance between
            x and q
        intermediate_step : tuple(str, args), array, None
            This is the type of interemediate timestep to intialize and the arguments
            for them. If this is None, then we do no intermediate timesteps.
            Options:
                &#39;step&#39;
                    args: (stride (numeric), eps (numeric))
                    We simulate at each timepoint every `stride` days.
                    We do not set an intermediate time point if it is within `eps`
                    days of a given data point.
                &#39;preserve-density&#39;
                    args: (n (int), eps (numeric))
                    We preserve the denisty of the given data by only simulating data
                    `n` times between each essential datapoint. If a timepoint is within
                    `eps` days of a given timepoint then we do not make an intermediate
                    point there.
                &#39;manual;
                    args: np.ndarray
                    These are the points that we want to set. If these are not given times
                    then we set them as timepoints
        intermediate_interpolation : str
            This is the type of interpolation to perform on the intermediate timepoints.
            Options:
                &#39;linear-interpolation&#39;, &#39;auto&#39;
                    Perform linear interpolation between the two closest given timepoints
        essential_timepoints : np.ndarray, str, None
            These are the timepoints that must be included in each subject. If one of the
            subjects has a missing timepoint there then we use an intermediate time point
            that this timepoint. It is initialized with linear interpolation. If all of the
            timepoints specified in this vector are included in a subject then nothing is
            done. If it is a str:
                &#39;union&#39;, &#39;auto&#39;
                    We take a union of all the timepoints in each subject and make sure
                    that all of the subjects have all those points.
        bandwidth : float
            This is the day bandwidth of the filter if the initialization method is
            done with &#39;moving-avg&#39;.
        window : int
            This is the window term for the LOESS initialization scheme. This is
            only used if value_initialization is done with &#39;loess&#39;
        target_acceptance_rate : numeric
            This is the target acceptance rate for each time point individually
        calculate_qpcr_loglik : bool
            If True, calculate the loglikelihood of the qPCR measurements during the 
            proposal
        &#39;&#39;&#39;
        if not pl.isint(delay):
            raise TypeError(&#39;`delay` ({}) must be an int&#39;.format(type(delay)))
        if delay &lt; 0:
            raise ValueError(&#39;`delay` ({}) must be &gt;= 0&#39;.format(delay))
        self.delay = delay
        self._there_are_perturbations = self.G.perturbations is not None

        # Set the hyperparameters
        if not pl.isfloat(target_acceptance_rate):
            raise TypeError(&#39;`target_acceptance_rate` must be a float&#39;.format(
                type(target_acceptance_rate)))
        if target_acceptance_rate &lt; 0 or target_acceptance_rate &gt; 1:
            raise ValueError(&#39;`target_acceptance_rate` ({}) must be in (0,1)&#39;.format(
                target_acceptance_rate))
        if not pl.istuple(tune):
            raise TypeError(&#39;`tune` ({}) must be a tuple&#39;.format(type(tune)))
        if len(tune) != 2:
            raise ValueError(&#39;`tune` ({}) must have 2 elements&#39;.format(len(tune)))
        if not pl.isint(tune[0]):
            raise TypeError(&#39;`tune` ({}) 1st parameter must be an int&#39;.format(type(tune[0])))
        if tune[0] &lt; 0:
            raise ValueError(&#39;`tune` ({}) 1st parameter must be &gt; 0&#39;.format(tune[0]))
        if not pl.isint(tune[1]):
            raise TypeError(&#39;`tune` ({}) 2nd parameter must be an int&#39;.format(type(tune[1])))
        if tune[1] &lt; 0:
            raise ValueError(&#39;`tune` ({}) 2nd parameter must be &gt; 0&#39;.format(tune[1]))
        
        if not pl.isnumeric(a0):
            raise TypeError(&#39;`a0` ({}) must be a numeric type&#39;.format(type(a0)))
        elif a0 &lt;= 0:
            raise ValueError(&#39;`a0` ({}) must be &gt; 0&#39;.format(a0))
        if not pl.isnumeric(a1):
            raise TypeError(&#39;`a1` ({}) must be a numeric type&#39;.format(type(a1)))
        elif a1 &lt;= 0:
            raise ValueError(&#39;`a1` ({}) must be &gt; 0&#39;.format(a1))

        if not pl.isnumeric(proposal_init_scale):
            raise TypeError(&#39;`proposal_init_scale` ({}) must be a numeric type (int, float)&#39;.format(
                type(proposal_init_scale)))
        if proposal_init_scale &lt; 0:
            raise ValueError(&#39;`proposal_init_scale` ({}) must be positive&#39;.format(
                proposal_init_scale))

        self.tune = tune
        self.a0 = a0
        self.a1 = a1
        self.target_acceptance_rate = target_acceptance_rate
        self.proposal_init_scale = proposal_init_scale
        self.v1 = v1
        self.v2 = v2

        # Set the essential timepoints (check to see if there is any missing data)
        if essential_timepoints is not None:
            logging.info(&#39;Setting up the essential timepoints&#39;)
            if pl.isstr(essential_timepoints):
                if essential_timepoints in [&#39;auto&#39;, &#39;union&#39;]:
                    essential_timepoints = set()
                    for ts in self.G.data.times:
                        essential_timepoints = essential_timepoints.union(set(list(ts)))
                    essential_timepoints = np.sort(list(essential_timepoints))
                else:
                    raise ValueError(&#39;`essential_timepoints` ({}) not recognized&#39;.format(
                        essential_timepoints))
            elif not pl.isarray(essential_timepoints):
                raise TypeError(&#39;`essential_timepoints` ({}) must be a str or an array&#39;.format(
                    type(essential_timepoints)))
            logging.info(&#39;Essential timepoints: {}&#39;.format(essential_timepoints))
            self.G.data.set_timepoints(times=essential_timepoints, eps=None, reset_timepoints=True)
            self.x.reset_value_size()

        # Set the intermediate timepoints if necessary
        if intermediate_step is not None:
            # Set the intermediate timepoints in the data
            if not pl.istuple(intermediate_step):
                raise TypeError(&#39;`intermediate_step` ({}) must be a tuple&#39;.format(
                    type(intermediate_step)))
            if len(intermediate_step) != 2:
                raise ValueError(&#39;`intermediate_step` ({}) must be length 2&#39;.format(
                    len(intermediate_step)))
            f, args = intermediate_step
            if not pl.isstr(f):
                raise TypeError(&#39;intermediate_step type ({}) must be a str&#39;.format(type(f)))
            if f == &#39;step&#39;:
                if not pl.istuple(args):
                    raise TypeError(&#39;`args` ({}) must be a tuple&#39;.format(type(args)))
                if len(args) != 2:
                    raise TypeError(&#39;`args` ({}) must have 2 arguments&#39;.format(len(args)))
                step, eps = args
                self.G.data.set_timepoints(timestep=step, eps=eps, reset_timepoints=False)
            elif f == &#39;preserve-density&#39;:
                if not pl.istuple(args):
                    raise TypeError(&#39;`args` ({}) must be a tuple&#39;.format(type(args)))
                if len(args) != 2:
                    raise TypeError(&#39;`args` ({}) must have 2 arguments&#39;.format(len(args)))
                n, eps = args
                if not pl.isint(n):
                    raise TypeError(&#39;`n` ({}) must be an int&#39;.format(type(n)))

                # For each timepoint, add `n` intermediate timepoints
                for ridx in range(self.G.data.n_replicates):
                    times = []
                    for i in range(len(self.G.data.times[ridx])-1):
                        t0 = self.G.data.times[ridx][i]
                        t1 = self.G.data.times[ridx][i+1]
                        step = (t1-t0)/(n+1)
                        times = np.append(times, np.arange(t0,t1,step=step))
                    times = np.sort(np.unique(times))
                    # print(&#39;\n\ntimes to put in&#39;, times)
                    self.G.data.set_timepoints(times=times, eps=eps, ridx=ridx, reset_timepoints=False)
                    # print(&#39;times for ridx {}&#39;.format(self.G.data.times[ridx]))
                    # print(&#39;len times&#39;, len(self.G.data.times[ridx]))
                    # print(&#39;data shape&#39;, self.G.data.data[ridx].shape)

                # sys.exit()
            elif f == &#39;manual&#39;:
                raise NotImplementedError(&#39;Not Implemented&#39;)
            else:
                raise ValueError(&#39;`intermediate_step type ({}) not recognized&#39;.format(f))
            self.x.reset_value_size()

        if intermediate_interpolation is not None:
            if intermediate_interpolation in [&#39;linear-interpolation&#39;, &#39;auto&#39;]:
                for ridx in range(self.G.data.n_replicates):
                    for tidx in range(self.G.data.n_timepoints_for_replicate[ridx]):
                        if tidx not in self.G.data.given_timeindices[ridx]:
                            # We need to interpolate this time point
                            # get the previous given and next given timepoint
                            prev_tidx = None
                            for ii in range(tidx-1,-1,-1):
                                if ii in self.G.data.given_timeindices[ridx]:
                                    prev_tidx = ii
                                    break
                            if prev_tidx is None:
                                # Set to the same as the closest forward timepoint then continue
                                next_idx = None
                                for ii in range(tidx+1, self.G.data.n_timepoints_for_replicate[ridx]):
                                    if ii in self.G.data.given_timeindices[ridx]:
                                        next_idx = ii
                                        break
                                self.G.data.data[ridx][:,tidx] = self.G.data.data[ridx][:,next_idx]
                                continue

                            next_tidx = None
                            for ii in range(tidx+1, self.G.data.n_timepoints_for_replicate[ridx]):
                                if ii in self.G.data.given_timeindices[ridx]:
                                    next_tidx = ii
                                    break
                            if next_tidx is None:
                                # Set to the previous timepoint then continue
                                self.G.data.data[ridx][:,tidx] = self.G.data.data[ridx][:,prev_tidx]
                                continue

                            # Interpolate from prev_tidx to next_tidx
                            x = self.G.data.times[ridx][tidx]
                            x0 = self.G.data.times[ridx][prev_tidx]
                            y0 = self.G.data.data[ridx][:,prev_tidx]
                            x1 = self.G.data.times[ridx][next_tidx]
                            y1 = self.G.data.data[ridx][:,next_tidx]
                            self.G.data.data[ridx][:,tidx] = y0 * (1-((x-x0)/(x1-x0))) + y1 * (1-((x1-x)/(x1-x0)))
            else:
                raise ValueError(&#39;`intermediate_interpolation` ({}) not recognized&#39;.format(intermediate_interpolation))

        # Initialize the latent trajectory
        if not pl.isstr(x_value_option):
            raise TypeError(&#39;`x_value_option` ({}) is not a str&#39;.format(type(x_value_option)))
        if x_value_option == &#39;coupling&#39;:
            self._init_coupling()
        elif x_value_option == &#39;moving-avg&#39;:
            if not pl.isnumeric(bandwidth):
                raise TypeError(&#39;`bandwidth` ({}) must be a numeric&#39;.format(type(bandwidth)))
            if bandwidth &lt;= 0:
                raise ValueError(&#39;`bandwidth` ({}) must be positive&#39;.format(bandwidth))
            self.bandwidth = bandwidth
            self._init_moving_avg()
        elif x_value_option in [&#39;loess&#39;, &#39;auto&#39;]:
            if window is None:
                raise TypeError(&#39;If `value_option` is loess, then `window` must be specified&#39;)
            if not pl.isint(window):
                raise TypeError(&#39;`window` ({}) must be an int&#39;.format(type(window)))
            if window &lt;= 0:
                raise ValueError(&#39;`window` ({}) must be &gt; 0&#39;.format(window))
            self.window = window
            self._init_loess()
        else:
            raise ValueError(&#39;`x_value_option` ({}) not recognized&#39;.format(x_value_option))

        # Get necessary data and set the parallel objects
        if self._there_are_perturbations:
            pert_starts = []
            pert_ends = []
            for perturbation in self.G.perturbations:
                pert_starts.append(perturbation.start)
                pert_ends.append(perturbation.end)
        else:
            pert_starts = None
            pert_ends = None

        if self.mp is None:
            self.mp = &#39;debug&#39;
        if not pl.isstr(self.mp):
            raise TypeError(&#39;`mp` ({}) must either be a string or None&#39;.format(type(self.mp)))
        if self.mp == &#39;debug&#39;:
            self.pool = []
        elif self.mp == &#39;full&#39;:
            self.pool = pl.multiprocessing.PersistentPool(G=self.G, ptype=&#39;sadw&#39;)
            self.worker_pids = []
        else:
            raise ValueError(&#39;`mp` ({}) not recognized&#39;.format(self.mp))

        for ridx in range(self.G.data.n_replicates):
            # Set up qPCR measurements and reads to send
            qpcr_log_measurements = {}
            for t in self.G.data.given_timepoints[ridx]:
                qpcr_log_measurements[t] = self.G.data.qpcr[ridx][t].log_data
            reads = self.G.data.subjects.iloc(ridx).reads

            worker = SubjectLogTrajectorySetMP()
            worker.initialize(
                zero_inflation_transition_policy=self.zero_inflation_transition_policy,
                times=self.G.data.times[ridx],
                qpcr_log_measurements=qpcr_log_measurements,
                reads=reads,
                there_are_intermediate_timepoints=True,
                there_are_perturbations=self._there_are_perturbations,
                pv_global=self.G[REPRNAMES.PROCESSVAR].global_variance,
                x_prior_mean=np.log(1e7),
                x_prior_std=1e10,
                tune=tune[1],
                delay=delay,
                end_iter=tune[0],
                proposal_init_scale=proposal_init_scale,
                a0=a0,
                a1=a1,
                x=self.x[ridx].value,
                pert_starts=np.asarray(pert_starts),
                pert_ends=np.asarray(pert_ends),
                ridx=ridx,
                calculate_qpcr_loglik=calculate_qpcr_loglik,
                h5py_xname=self.x[ridx].name,
                target_acceptance_rate=self.target_acceptance_rate)
            if self.mp == &#39;debug&#39;:
                self.pool.append(worker)
            elif self.mp == &#39;full&#39;:
                pid = self.pool.add_worker(worker)
                self.worker_pids.append(pid)

        # Set the data to the latent values
        self.set_latent_as_data(update_values=False)

        self.total_n_datapoints = 0
        for ridx in range(self.G.data.n_replicates):
            self.total_n_datapoints += self.x[ridx].value.shape[0] * self.x[ridx].value.shape[1]

    def _init_coupling(self):
        &#39;&#39;&#39;Initialize `x` by sampling around the data using a small
        variance using a truncated normal distribution
        &#39;&#39;&#39;
        for ridx, tidx, oidx in self.x.iter_indices():
            val = self.G.data.data[ridx][oidx,tidx]
            self.x[ridx][oidx,tidx] = pl.random.truncnormal.sample(
                mean=val,
                std=math.sqrt(self.v1 * (val ** 2) + self.v2),
                low=0, high=float(&#39;inf&#39;))

    def _init_moving_avg(self):
        &#39;&#39;&#39;Initializes `x` by using a moving
        average over the data - using `self.bandwidth` as the bandwidth
        of number of days - it then samples around that point using the
        coupling variance.

        If there are no other points within the bandwidth around the point,
        then it just samples around the current timepoint with the coupling
        variance.
        &#39;&#39;&#39;
        for ridx in range(self.G.data.n_replicates):
            for tidx in range(self.G.data.n_timepoints_for_replicate[ridx]):
                tidx_low = np.searchsorted(
                    self.G.data.times[ridx], self.G.data.times[ridx][tidx]-self.bandwidth)
                tidx_high = np.searchsorted(
                    self.G.data.times[ridx], self.G.data.times[ridx][tidx]+self.bandwidth)

                for oidx in range(len(self.G.data.asvs)):
                    val = np.mean(self.G.data.data[ridx][oidx, tidx_low: tidx_high])
                    self.x[ridx][oidx,tidx] = pl.random.truncnormal.sample(
                        mean=val,
                        std=math.sqrt(self.v1 * (val ** 2) + self.v2),
                        low=0, high=float(&#39;inf&#39;))

    def _init_loess(self):
        &#39;&#39;&#39;Initialize the data using LOESS algorithm and then samples around that
        the coupling variance we implement the LOESS algorithm in the module
        `fit_loess.py`
        &#39;&#39;&#39;
        for ridx in range(self.G.data.n_replicates):
            xx = self.G.data.times[ridx]
            for oidx in range(len(self.G.data.asvs)):
                yy = self.G.data.data[ridx][oidx, :]
                loess = Loess(xx, yy)

                for tidx, t in enumerate(self.G.data.times[ridx]):
                    val = loess.estimate(t, window=self.window)
                    self.x[ridx][oidx,tidx] = pl.random.truncnormal.sample(
                        mean=val,
                        std=math.sqrt(self.v1 * (val ** 2) + self.v2),
                        low=0, high=float(&#39;inf&#39;))

                    if np.isnan(self.x[ridx][oidx, tidx]):
                        print(&#39;crashed here&#39;, ridx, tidx, oidx)
                        print(&#39;mean&#39;, val)
                        print(&#39;t&#39;, t)
                        print(&#39;yy&#39;, yy)
                        print(&#39;std&#39;, math.sqrt(self.v1 * (val ** 2) + self.v2))
                        raise ValueError(&#39;&#39;)

    def set_latent_as_data(self, update_values=True):
        &#39;&#39;&#39;Change the values in the data matrix so that it is the latent variables
        &#39;&#39;&#39;
        data = []
        for obj in self.x.value:
            data.append(obj.value)
        self.G.data.data = data
        if update_values:
            self.G.data.update_values()

    def add_trace(self):
        self.x.add_trace()

    def add_init_value(self):
        self.x.add_init_value()

    def set_trace(self, *args, **kwargs):
        self.x.set_trace(*args, **kwargs)

    def kill(self):
        if self.mp == &#39;full&#39;:
            self.pool.kill()

    def update(self):
        &#39;&#39;&#39;Send out to each parallel object
        &#39;&#39;&#39;
        if self.sample_iter &lt; self.delay:
            return
        start_time = time.time()

        growth = self.G[REPRNAMES.GROWTH_VALUE].value.ravel()
        self_interactions = self.G[REPRNAMES.SELF_INTERACTION_VALUE].value.ravel()
        pv = self.G[REPRNAMES.PROCESSVAR].value
        interactions = self.G[REPRNAMES.INTERACTIONS_OBJ].get_datalevel_value_matrix(
            set_neg_indicators_to_nan=False)
        perts = None
        if self._there_are_perturbations:
            perts = []
            for perturbation in self.G.perturbations:
                perts.append(perturbation.item_array().reshape(-1,1))
            perts = np.hstack(perts)

        # zero_inflation = [self.G[REPRNAMES.ZERO_INFLATION].value[ridx] for ridx in range(self.G.data.n_replicates)]
        qpcr_vars = []
        for aaa in self.G[REPRNAMES.QPCR_VARIANCES].value:
            qpcr_vars.append(aaa.value)
        
        
        kwargs = {&#39;growth&#39;:growth, &#39;self_interactions&#39;:self_interactions,
            &#39;pv&#39;:pv, &#39;interactions&#39;:interactions, &#39;perturbations&#39;:perts, 
            &#39;zero_inflation_data&#39;: None, &#39;qpcr_variances&#39;:qpcr_vars}

        str_acc = [None]*self.G.data.n_replicates
        if self.mp == &#39;debug&#39;:

            for ridx in range(self.G.data.n_replicates):
                _, x, acc_rate = self.pool[ridx].persistent_run(**kwargs)
                self.x[ridx].value = x
                str_acc[ridx] = &#39;{:.3f}&#39;.format(acc_rate)

        else:
            # raise NotImplementedError(&#39;Multiprocessing for filtering with zero inflation &#39; \
            #     &#39;is not implemented&#39;)
            ret = self.pool.map(func=&#39;persistent_run&#39;, args=kwargs)
            for ridx, x, acc_rate in ret:
                self.x[ridx].value = x
                str_acc[ridx] = &#39;{:.3f}&#39;.format(acc_rate)

        self.set_latent_as_data()

        t = time.time() - start_time
        try:
            self._strr = &#39;Time: {:.4f}, Acc: {}, data/sec: {:.2f}&#39;.format(t,
                str(str_acc).replace(&#34;&#39;&#34;,&#39;&#39;), self.total_n_datapoints/t)
        except:
            self._strr = &#39;NA&#39;


class SubjectLogTrajectorySetMP(pl.multiprocessing.PersistentWorker):
    &#39;&#39;&#39;This performs filtering on a multiprocessing level. We send the
    other parameters of the model and return the filtered `x` and values.
    With multiprocessing, this class has ~91% efficiency. Additionally, the code
    in this class is optimized to be ~20X faster than the code in `Filtering`. This
    assumes we have the log model.

    It might seem unneccessary to have so many local attributes, but it speeds up
    the inference considerably if we index a value from an array once and store it
    as a float instead of repeatedly indexing the array - the difference in
    reality is super small but we do this so often that it adds up to ~40% speedup
    as supposed to not doing it - so we do this as often as possible - This speedup
    is even greater for indexing keys of dictionaries and getting parameters of objects.

    General efficiency speedups
    ---------------------------
    All of these are done relative to a non-optimized filtering implementation
        - Specialized sampling and logpdf functions. About 95% faster than
          scipy or numpy functions. All of these add up to a ~35% speed up
        - Explicit function definitions:
          instead of doing `self.prior.logpdf(...)`, we do
          `pl.random.normal.logpdf(...)`, about a ~10% overall speedup
        - Precomputation of values so that the least amout of computation
          is done on a data level - All of these add up to a ~25% speed up
    Benchmarked on a MacPro

    Non/trivial efficiency speedups w.r.t. non-multiprocessed filtering
    -------------------------------------------------------------------
    All of the speed ups are done relative to the current implementation
    in non-multiprocessed filtering.
        - Whenever possible we replace a 2D variable like `self.x` with a
          `curr_x`, which is 1D, because indexing a 1D array is 10-20% faster
          than a 2D array. All of these add up to a ~8% speed up
        - We only every compute the forward dynamics (not the reverse), because
          we can use the forward of the previous timepoint as the reverse for
          the next timepoint. This is about 45% faster and adds up to a ~40%
          speedup.
        - Whenever possible, we replace a dictionary like `read_depths`
          with a float because indexing a dict is 12-20% slower than a 1D
          array. All these add up to a ~7% speed up
        - We precompute AS MUCH AS POSSIBLE in `update` and in `initialize`,
          even simple this as `self.curr_tidx_minus_1`: all of these add up
          to about ~5% speedup
        - If an attribute of a class is being referenced more than once in
          a subroutine, we &#34;get&#34; it by making it a local variable. Example:
          `tidx = self.tidx`. This has about a 5% speed up PER ADDITIONAL
          CALL within the subroutine. All of these add up to ~2.5% speed up.
        - If an indexed value gets indexed more than once within a subroutine,
          we &#34;get&#34; the value by making it a local variable. All of these
          add up to ~4% speed up.
        - We &#34;get&#34; all of the means and stds of the qPCR data-structures so we
          do not reference an object. This is about 22% faster and adds up
          to a ~3% speed up.
    Benchmarked on a MacPro
    &#39;&#39;&#39;
    def __init__(self):
        &#39;&#39;&#39;Set all local variables to None
        &#39;&#39;&#39;
        return

    def initialize(self, times, qpcr_log_measurements, reads, there_are_intermediate_timepoints,
        there_are_perturbations, pv_global, x_prior_mean,
        x_prior_std, tune, delay, end_iter, proposal_init_scale, a0, a1, x, calculate_qpcr_loglik,
        pert_starts, pert_ends, ridx, h5py_xname, target_acceptance_rate,
        zero_inflation_transition_policy):
        &#39;&#39;&#39;Initialize the object at the beginning of the inference

        n_o = Number of ASVs
        n_gT = Number of given time points
        n_T = Total number of time points, including intermediate
        n_P = Number of Perturbations

        Parameters
        ----------
        times : np.array((n_T, ))
            Times for each of the time points
        qpcr_log_measurements : dict(t -&gt; np.ndarray(float))
            These are the qPCR observations for every timepoint in log space.
        reads : dict (float -&gt; np.ndarray((n_o, )))
            The counts for each of the given timepoints. Each value is an
            array for the counts for each of the ASVs
        there_are_intermediate_timepoints : bool
            If True, then there are intermediate timepoints, else there are only
            given timepoints
        there_are_perturbations : bool
            If True, that means there are perturbations, else there are no
            perturbations
        pv_global : bool
            If True, it means the process variance is is global for each ASV. If
            False it means that there is a separate `pv` for each ASV
        pv : float, np.ndarray
            This is the process variance value. This is a float if `pv_global` is True
            and it is an array if `pv_global` is False.
        x_prior_mean, x_prior_std : numeric
            This is the prior mean and std for `x` used when sampling the reverse for
            the first timepoint
        tune : int
            How often we should update the proposal for each ASV
        delay : int
            How many MCMC iterations we should delay the start of updating
        end_iter : int
            What iteration we should stop updating the proposal
        proposal_init_scale : float
            Scale to multiply the initial covariance of the poposal
        a0, a1 : floats
            These are the negative binomial dispersion parameters that specify how
            much noise there is in the counts
        x : np.ndarray((n_o, n_T))
            This is the x initialization
        pert_starts, pert_ends : np.ndarray((n_P, ))
            The starts and ends for each one of the perturbations
        ridx : int
            This is the replicate index that this object corresponds to
        h5py_xname : str
            This is the name for the x in the h5py object
        target_acceptance_rate : float
            This is the target acceptance rate for each point
        calculate_qpcr_loglik : bool
            If True, calculate the loglikelihood of the qPCR measurements during the proposal
        &#39;&#39;&#39;
        self.h5py_xname = h5py_xname
        self.target_acceptance_rate = target_acceptance_rate
        self.zero_inflation_transition_policy = zero_inflation_transition_policy

        self.times = times
        self.qpcr_log_measurements = qpcr_log_measurements
        self.reads = reads
        self.there_are_intermediate_timepoints = there_are_intermediate_timepoints
        self.there_are_perturbations = there_are_perturbations
        self.pv_global = pv_global
        if not pv_global:
            raise TypeError(&#39;Filtering with MP not implemented for non global process variance&#39;)
        self.x_prior_mean = x_prior_mean
        self.x_prior_std = x_prior_std
        self.tune = tune
        self.delay = 0
        self.end_iter = end_iter
        self.proposal_init_scale = proposal_init_scale
        self.a0 = a0
        self.a1 = a1
        self.n_asvs = x.shape[0]
        self.n_timepoints = len(times)
        self.n_timepoints_minus_1 = len(times)-1
        self.logx = np.log(x)
        self.x = x
        self.pert_starts = pert_starts
        self.pert_ends = pert_ends
        self.total_n_points = self.x.shape[0] * self.x.shape[1]
        self.ridx = ridx
        self.calculate_qpcr_loglik = calculate_qpcr_loglik

        self.sample_iter = 0
        self.n_data_points = self.x.shape[0] * self.x.shape[1]

        # latent state
        self.sum_q = np.sum(self.x, axis=0)
        shape = (self.tune, ) + self.x.shape
        self.trace_iter = 0

        # proposal
        self.proposal_std = np.log(1.5) #np.log(3)
        self.acceptances = 0
        self.n_props_total = 0
        self.n_props_local = 0
        self.total_acceptances = 0
        self.add_trace = True

        # Intermediate timepoints
        if self.there_are_intermediate_timepoints:
            self.is_intermediate_timepoint = {}
            self.data_loglik = self.data_loglik_w_intermediates
            for t in self.times:
                self.is_intermediate_timepoint[t] = t not in self.reads
        else:
            self.data_loglik = self.data_loglik_wo_intermediates

        # Reads
        self.read_depths = {}
        for t in self.reads:
            self.read_depths[t] = float(np.sum(self.reads[t]))

        # t
        self.dts = np.zeros(self.n_timepoints_minus_1)
        self.sqrt_dts = np.zeros(self.n_timepoints_minus_1)
        for k in range(self.n_timepoints_minus_1):
            self.dts[k] = self.times[k+1] - self.times[k]
            self.sqrt_dts[k] = np.sqrt(self.dts[k])
        self.t2tidx = {}
        for tidx, t in enumerate(self.times):
            self.t2tidx[t] = tidx

        self.cnt_accepted_times = np.zeros(len(self.times))

        # Perturbations
        # -------------
        # in_pert_transition : np.ndarray(dtype=bool)
        #   This is a bool array where if it is a true it means that the
        #   forward and reverse growth rates are different
        # fully_in_pert : np.ndarray(dtype=int)
        #   This is an int-array where it tells you which perturbation you are fully in
        #   (the forward and reverse growth rates are the same but not the default).
        #   If there is no perturbation then the value is -1. If it is not -1, then the
        #   number corresponds to what perturbation index you are in.
        #
        # Edge cases
        # ----------
        #   * missing data for start
        #       There could be a situation where there was no sample collection
        #       on the day that they started a perturbation. In this case we
        #       assume that the next time point is the `start` of the perturbation.
        #       i.e. the next time point is the perturbation transition.
        #   * missing data for end
        #       There could be a situation where no sample was collected when the
        #       perturbation ended. In this case we assume that the pervious time
        #       point was the end of the perturbation.
        if self.there_are_perturbations:
            self.in_pert_transition = np.zeros(self.n_timepoints, dtype=bool)
            self.fully_in_pert = np.ones(self.n_timepoints, dtype=int) * -1
            for pidx, t in enumerate(self.pert_starts):
                if t == self.times[-1] or t == self.times[0]:
                    raise ValueError(&#39;The code right now does not support either a perturbation that &#39; \
                        &#39;started on the first day or ended on the last day. The code where this is &#39; \
                        &#39;incompatible is when we checking if we are in a perturbation transition&#39;)
                if t &gt; np.max(self.times):
                    continue
                if t not in self.t2tidx:
                    # Use the next time point
                    tidx = np.searchsorted(self.times, t)
                else:
                    tidx = self.t2tidx[t]
                self.in_pert_transition[tidx] = True
            for pidx, t in enumerate(self.pert_ends):
                if t == self.times[-1] or t == self.times[0]:
                    raise ValueError(&#39;The code right now does not support either a perturbation that &#39; \
                        &#39;started on the first day or ended on the last day. The code where this is &#39; \
                        &#39;incompatible is when we checking if we are in a perturbation transition&#39;)
                if t &lt; np.min(self.times) or t &gt; np.max(self.times):
                    continue
                if t not in self.t2tidx:
                    # Use the previous time point
                    tidx = np.searchsorted(self.times, t) - 1
                else:
                    tidx = self.t2tidx[t]
                self.in_pert_transition[tidx] = True

            # check if anything is weird
            if np.sum(self.in_pert_transition) % 2 != 0:
                raise ValueError(&#39;The number of in_pert_transition periods must be even ({})&#39; \
                    &#39;. There is either something wrong with the data (start and end day are &#39; \
                    &#39;the same) or with the algorithm ({})&#39;.format(
                        np.sum(self.in_pert_transition),
                        self.in_pert_transition))

            # Make the fully in perturbation times
            for pidx in range(len(self.pert_ends)):
                try:
                    start_tidx = self.t2tidx[self.pert_starts[pidx]] + 1
                    end_tidx = self.t2tidx[self.pert_ends[pidx]]
                except:
                    # This means there is a missing datapoint at either the
                    # start or end of the perturbation
                    start_t = self.pert_starts[pidx]
                    end_t = self.pert_ends[pidx]
                    start_tidx = np.searchsorted(self.times, start_t)
                    end_tidx = np.searchsorted(self.times, end_t) - 1

                self.fully_in_pert[start_tidx:end_tidx] = pidx
    
    # @profile
    def persistent_run(self, growth, self_interactions, pv, interactions,
        perturbations, qpcr_variances, zero_inflation_data):
        &#39;&#39;&#39;Run an update of the values for a single gibbs step for all of the data points
        in this replicate

        Parameters
        ----------
        growth : np.ndarray((n_asvs, ))
            Growth rates for each ASV
        self_interactions : np.ndarray((n_asvs, ))
            Self-interactions for each ASV
        pv : numeric, np.ndarray
            This is the process variance
        interactions : np.ndarray((n_asvs, n_asvs))
            These are the ASV-ASV interactions
        perturbations : np.ndarray((n_perturbations, n_asvs))
            Perturbation values in the right perturbation order, per ASV
        zero_inflation : np.ndarray
            These are the points that are delibertly pushed down to zero
        qpcr_variances : np.ndarray
            These are the sampled qPCR variances as an array - they are in
            time order

        Returns
        -------
        (int, np.ndarray, float)
            1 This is the replicate index
            2 This is the updated latent state for logx
            3 This is the acceptance rate for this past update.
        &#39;&#39;&#39;
        self.master_growth_rate = growth

        if self.sample_iter &lt; self.delay:
            self.sample_iter += 1
            return self.ridx, self.x, np.nan

        self.update_proposals()
        self.n_accepted_iter = 0
        self.pv = pv
        self.pv_std = np.sqrt(pv)
        self.qpcr_stds = np.sqrt(qpcr_variances[self.ridx])
        self.qpcr_stds_d = {}
        # self.zero_inflation_data = zero_inflation_data[self.ridx]
        self.zero_inflation_data = None

        for tidx,t in enumerate(self.qpcr_log_measurements):
            self.qpcr_stds_d[t] = self.qpcr_stds[tidx]

        if self.there_are_perturbations:
            self.growth_rate_non_pert = growth.ravel()
            self.growth_rate_on_pert = growth.reshape(-1,1) * (1 + perturbations)
                
        # Go through each randomly ASV and go in time order
        oidxs = npr.permutation(self.n_asvs)
        # print(&#39;===============================&#39;)
        # print(&#39;===============================&#39;)
        # print(&#39;ridx&#39;, self.ridx)
        for oidx in oidxs:

            # Set the necessary global parameters
            self.oidx = oidx
            self.curr_x = self.x[oidx, :]
            self.curr_logx = self.logx[oidx, :]
            self.curr_interactions = interactions[oidx, :]
            self.curr_self_interaction = self_interactions[oidx]
            # self.curr_zero_inflation = self.zero_inflation[oidx, :]

            if self.pv_global:
                self.curr_pv_std = self.pv_std
            else:
                self.curr_pv_std = self.pv_std[oidx]

            # Set for first time point
            self.tidx = 0
            self.set_attrs_for_timepoint()
            self.forward_loglik = self.default_forward_loglik
            self.reverse_loglik = self.first_timepoint_reverse
            # Calculate A matrix for forward
            self.forward_interaction_vals = np.nansum(self.x[:, self.tidx] * self.curr_interactions)
            self.update_single()
            self.reverse_loglik = self.default_reverse_loglik
            # Set for middle timepoints
            for tidx in range(1, self.n_timepoints-1):
                # Check if it needs to be zero inflated
                # if not self.curr_zero_inflation[tidx]:
                #     raise NotImplementedError(&#39;Zero inflation not implemented for logmodel&#39;)

                self.tidx = tidx
                self.set_attrs_for_timepoint()

                # Calculate A matrix for forward and reverse
                # Set the reverse of the current time step to the forward of the previous
                self.reverse_interaction_vals = self.forward_interaction_vals #np.sum(self.x[:, self.prev_tidx] * self.curr_interactions)
                self.forward_interaction_vals = np.nansum(self.x[:, self.tidx] * self.curr_interactions)

                # Run single update
                self.update_single()

            # Set for last timepoint
            self.tidx = self.n_timepoints_minus_1
            self.set_attrs_for_timepoint()
            self.forward_loglik = self.last_timepoint_forward
            # Calculate A matrix for reverse
            # Set the reverse of the current time step to the forward of the previous
            self.reverse_interaction_vals = self.forward_interaction_vals # np.sum(self.x[:, self.prev_tidx] * self.curr_interactions)
            self.update_single()

            # if self.sample_iter == 4:
            # sys.exit()

        self.sample_iter += 1
        if self.add_trace:
            self.trace_iter += 1

        # print(self.cnt_accepted_times/self.sample_iter)

        return self.ridx, self.x, self.n_accepted_iter/self.n_data_points

    def set_attrs_for_timepoint(self):
        self.prev_tidx = self.tidx-1
        self.next_tidx = self.tidx+1
        self.forward_growth_rate = self.master_growth_rate[self.oidx]
        self.reverse_growth_rate = self.master_growth_rate[self.oidx]

        if self.there_are_intermediate_timepoints:
            if not self.is_intermediate_timepoint[self.times[self.tidx]]:
                # It is not intermediate timepoints - we need to get the data
                t = self.times[self.tidx]
                self.curr_reads = self.reads[t][self.oidx]
                self.curr_read_depth = self.read_depths[t]
                self.curr_qpcr_log_measurements = self.qpcr_log_measurements[t]
                self.curr_qpcr_std = self.qpcr_stds_d[t]
        else:
            t = self.times[self.tidx]
            self.curr_reads = self.reads[t][self.oidx]
            self.curr_read_depth = self.read_depths[t]
            self.curr_qpcr_log_measurements = self.qpcr_log_measurements[t]
            self.curr_qpcr_std = self.qpcr_stds_d[t]

        # Set perturbation growth rates
        if self.there_are_perturbations:
            if self.in_pert_transition[self.tidx]:
                if self.fully_in_pert[self.tidx-1] != -1:
                    # If the previous time point is in the perturbation, that means
                    # we are going out of the perturbation
                    # self.forward_growth_rate = self.master_growth_rate[self.oidx]
                    pidx = self.fully_in_pert[self.tidx-1]
                    self.reverse_growth_rate = self.growth_rate_on_pert[self.oidx,pidx]
                else:
                    # Else we are going into a perturbation
                    # self.reverse_growth_rate = self.master_growth_rate[self.oidx]
                    pidx = self.fully_in_pert[self.tidx+1]
                    self.forward_growth_rate = self.growth_rate_on_pert[self.oidx,pidx]
            elif self.fully_in_pert[self.tidx] != -1:
                pidx = self.fully_in_pert[self.tidx]
                self.forward_growth_rate = self.growth_rate_on_pert[self.oidx,pidx]
                self.reverse_growth_rate = self.forward_growth_rate

    # @profile
    def update_single(self):
        &#39;&#39;&#39;Update a single oidx, tidx
        &#39;&#39;&#39;
        tidx = self.tidx
        oidx = self.oidx

        # Check if we should update the zero inflation policy
        if self.zero_inflation_transition_policy is not None:
            if self.zero_inflation_transition_policy == &#39;ignore&#39;:
                if not self.zero_inflation_data[oidx,tidx]:
                    self.x[oidx, tidx] = np.nan
                    self.logx[oidx, tidx] = np.nan
                    return
                else:
                    if tidx &lt; self.zero_inflation_data.shape[1]-1:
                        do_forward = self.zero_inflation_data[oidx, tidx+1]
                    else:
                        do_forward = True
                    if tidx &gt; 0:
                        do_reverse = self.zero_inflation_data[oidx, tidx-1]
                    else:
                        do_reverse = True
            else:
                raise NotImplementedError(&#39;Not Implemented&#39;)
        else:
            do_forward = True
            do_reverse = True

        # t = self.times[self.tidx]
        # # proposal
        # mu1 = self.curr_logx[tidx]
        # rel = self.reads[t][oidx]/self.read_depths[t]
        # if rel == 0:
        #     rel = 1e-5
        # mu2 = np.log(rel*np.exp(self.curr_qpcr_loc + (self.curr_qpcr_scale/2)))

        # var1 = self.proposal_std[(tidx, oidx)]**2
        # var2 = (self.curr_qpcr_scale)**2
        # mu,var = prod_gaussians(means=[mu1,mu2], variances=[var1,var2])

        try:
            logx_new = pl.random.misc.fast_sample_normal(
                self.curr_logx[tidx],
                self.proposal_std)
        except:
            print(&#39;mu&#39;, self.curr_logx[tidx])
            print(&#39;std&#39;, self.proposal_std)
            raise
        # try:
        #     logx_new = pl.random.misc.fast_sample_normal(
        #         mu, np.sqrt(var))
        # except:
        #     print(&#39;mu&#39;, mu)
        #     print(&#39;std&#39;, np.sqrt(var))
        #     raise

        x_new = np.exp(logx_new)
        prev_logx_value = self.curr_logx[tidx]
        prev_x_value = self.curr_x[tidx]

        # print(&#39;prex_x&#39;, prev_x_value, np.exp(prev_logx_value))
        # print(&#39;prev_logx&#39;, prev_logx_value)

        # if tidx == 5:
        #     print(&#39;\ntidx&#39;, tidx)
        #     print(&#39;oidx&#39;, oidx)
        #     print(&#39;t&#39;, self.times[self.tidx])
        #     print(&#39;curr_logx&#39;, self.curr_logx[tidx])
        #     # print(&#39;curr_logx&#39;, self.curr_logx[tidx])
        #     # print(&#39;mu1, mu2&#39;, mu1, mu2)
        #     # print(&#39;mu&#39;, mu)
        #     # print(&#39;var1, var2&#39;, var1,var2)
        #     # print(&#39;var&#39;,var)
        #     print(&#39;prop_logx&#39;, logx_new)
        #     # print(&#39;start perts&#39;, self.pert_starts)
        #     # print(&#39;end perts&#39;, self.pert_ends)
        #     # print(&#39;in perturbation transition?&#39;, self.in_pert_transition[tidx])
        #     # print(&#39;fully in pert?&#39;, self.fully_in_pert[self.tidx])
        #     print(&#39;forward growth&#39;, self.forward_growth_rate)

        if do_forward:
            prev_aaa = self.forward_loglik()
        else:
            prev_aaa = 0
        if do_reverse:
            prev_bbb = self.reverse_loglik()
        else:
            prev_bbb = 0
        prev_ddd = self.data_loglik()

        # if tidx == 5:
        #     print(&#39;\nold&#39;)
        #     print(&#39;forward ll&#39;, aaa)
        #     print(&#39;reverse ll&#39;, bbb)
        #     print(&#39;data ll&#39;, ddd)

        l_old = prev_aaa + prev_bbb + prev_ddd

        self.curr_x[tidx] = x_new
        self.curr_logx[tidx] = logx_new
        self.sum_q[tidx] = self.sum_q[tidx] - prev_x_value + x_new

        if do_forward:
            new_aaa = self.forward_loglik()
        else:
            new_aaa = 0
        if do_reverse:
            new_bbb = self.reverse_loglik()
        else:
            new_bbb = 0
        new_ddd = self.data_loglik()

        # if tidx == 5:
        #     print(&#39;\nnew&#39;)
        #     print(&#39;forward ll&#39;, aaa)
        #     print(&#39;reverse ll&#39;, bbb)
        #     print(&#39;data ll&#39;, ddd)
        #     print(&#39;\nold x value&#39;, prev_x_value)
        #     print(&#39;old logx value&#39;, prev_logx_value)
        #     print(&#39;proposal std&#39;, self.proposal_std[(tidx, oidx)])
        #     print(&#39;new x value&#39;, x_new)
        #     print(&#39;new logx value&#39;, logx_new)

        l_new = new_aaa + new_bbb + new_ddd
        r_accept = l_new - l_old

        # if tidx == 0:
        #     print(&#39;\n\noidx {} diff lls:&#39;.format(oidx), r_accept)
        #     print(&#39;\tforward&#39;, new_aaa - prev_aaa)
        #     print(&#39;\treverse&#39;, new_bbb - prev_bbb)
        #     print(&#39;\tdata&#39;, new_ddd - prev_ddd)

        # if tidx == 5:
        #     print(&#39;r_accept&#39;, r_accept)
        r = pl.random.misc.fast_sample_standard_uniform()
        if math.log(r) &gt; r_accept:
            # print(&#39;reject&#39;)
            # reject
            self.sum_q[tidx] = self.sum_q[tidx] + prev_x_value - x_new
            self.curr_x[tidx] = prev_x_value
            self.curr_logx[tidx] = prev_logx_value
        else:
            # print(&#39;accept&#39;)
            self.x[oidx, tidx] = x_new
            self.logx[oidx, tidx] = logx_new
            self.acceptances += 1
            self.total_acceptances += 1
            self.n_accepted_iter += 1

        self.n_props_local += 1
        self.n_props_total += 1

    def update_proposals(self):
        &#39;&#39;&#39;Update the proposal if necessary
        &#39;&#39;&#39;
        if self.sample_iter &gt; self.end_iter:
            self.add_trace = False
            return
        if self.sample_iter == 0:
            return
        if self.trace_iter - self.delay == self.tune  and self.sample_iter - self.delay &gt; 0:

            # Adjust
            acc_rate = self.acceptances/self.n_props_total
            if acc_rate &lt; 0.1:
                logging.debug(&#39;Very low acceptance rate, scaling down past covariance&#39;)
                self.proposal_std *= 0.01
            elif acc_rate &lt; self.target_acceptance_rate:
                self.proposal_std /= np.sqrt(1.5)
            else:
                self.proposal_std *= np.sqrt(1.5)
            
            self.acceptances = 0
            self.n_props_local = 0

    def last_timepoint_forward(self):
        return 0

    # @profile
    def default_forward_loglik(self):
        &#39;&#39;&#39;From the current timepoint (tidx) to the next timepoint (tidx+1)
        &#39;&#39;&#39;
        logmu = self.compute_dynamics(
            tidx=self.tidx,
            Axj=self.forward_interaction_vals,
            a1=self.forward_growth_rate)

        try:
            return pl.random.normal.logpdf(
                value=self.curr_logx[self.next_tidx], 
                mean=logmu, std=self.curr_pv_std*self.sqrt_dts[self.tidx])
        except:
            return _normal_logpdf(
                value=self.curr_logx[self.next_tidx], 
                mean=logmu, std=self.curr_pv_std*self.sqrt_dts[self.tidx])

    def first_timepoint_reverse(self):
        # sample from the prior
        try:
            return pl.random.normal.logpdf(value=self.curr_logx[self.tidx],
                mean=self.x_prior_mean, std=self.x_prior_std)
        except:
            return _normal_logpdf(value=self.curr_logx[self.tidx],
                mean=self.x_prior_mean, std=self.x_prior_std)

    # @profile
    def default_reverse_loglik(self):
        &#39;&#39;&#39;From the previous timepoint (tidx-1) to the current time point (tidx)
        &#39;&#39;&#39;
        logmu = self.compute_dynamics(
            tidx=self.prev_tidx,
            Axj=self.reverse_interaction_vals,
            a1=self.reverse_growth_rate)

        try:
            return pl.random.normal.logpdf(value=self.curr_logx[self.tidx], 
                mean=logmu, std=self.curr_pv_std*self.sqrt_dts[self.prev_tidx])
        except:
            return _normal_logpdf(value=self.curr_logx[self.tidx], 
                mean=logmu, std=self.curr_pv_std*self.sqrt_dts[self.prev_tidx])

    def data_loglik_w_intermediates(self):
        &#39;&#39;&#39;data loglikelihood w/ intermediate timepoints
        &#39;&#39;&#39;
        if self.is_intermediate_timepoint[self.times[self.tidx]]:
            return 0
        else:
            return self.data_loglik_wo_intermediates()

    # @profile
    def data_loglik_wo_intermediates(self):
        &#39;&#39;&#39;data loglikelihood with intermediate timepoints
        &#39;&#39;&#39;
        sum_q = self.sum_q[self.tidx]
        log_sum_q = math.log(sum_q)
        rel = self.curr_x[self.tidx] / sum_q

        try:
            negbin = negbin_loglikelihood_MH_condensed(
                k=self.curr_reads,
                m=self.curr_read_depth * rel,
                dispersion=self.a0/rel + self.a1)
        except:
            negbin = negbin_loglikelihood_MH_condensed_not_fast(
                k=self.curr_reads,
                m=self.curr_read_depth * rel,
                dispersion=self.a0/rel + self.a1)

        qpcr = 0
        if self.calculate_qpcr_loglik:
            for qpcr_val in self.curr_qpcr_log_measurements:
                a = pl.random.normal.logpdf(value=qpcr_val, mean=log_sum_q, std=self.curr_qpcr_std)
                qpcr += a

        # tidx = self.tidx
        # if True: #tidx in [3,6,7]:
        #     print(&#39;\n\nData, tidx&#39;, tidx)
        #     print(&#39;sum_q:&#39;, sum_q)
        #     print(&#39;rel:&#39;, rel)
        #     print(&#39;qpcr: {}\n\tvalue: {}\n\tmean: {}\n\tstd: {}&#39;.format(
        #         qpcr, self.curr_qpcr_loc,
        #         sum_q,
        #         self.curr_qpcr_scale))
        #     print(&#39;neg_bin: {}\n\tk: {}\n\tm: {}\n\tdispersion: {}&#39;.format( 
        #         negbin, self.curr_reads, self.curr_read_depth * rel,
        #         self.a0/rel + self.a1))
            
        #     print(&#39;data\n\tcurr_x: {}, {}\n\tcurr_logx: {}&#39;.format(
        #         self.curr_x[self.tidx], 
        #         np.exp(self.curr_logx[self.tidx]),
        #         self.curr_logx[self.tidx]))
        return negbin + qpcr

    def compute_dynamics(self, tidx, Axj, a1):
        &#39;&#39;&#39;Compute dynamics going into tidx+1

        a1 : growth rates and perturbations (if necessary)
        Axj : cluster interactions with the other abundances already multiplied
        tidx : time index

        Zero-inflation
        --------------
        When we get here, the current asv at `tidx` is not a structural zero, but 
        there might be other bugs in the system that do have a structural zero there.
        Thus we do nan adds
        &#39;&#39;&#39;
        logxi = self.curr_logx[tidx]
        xi = self.curr_x[tidx]

        # print(&#39;dynamics&#39;)
        # print(&#39;xi*a1&#39;, xi*a1* self.dts[tidx])
        # print(&#39;xi*xi*self.curr_self_interaction&#39;, xi*xi*self.curr_self_interaction* self.dts[tidx])
        # print(&#39;xi*Axj&#39;, xi*Axj* self.dts[tidx])

        # compute dynamics
        return logxi + (a1 - xi*self.curr_self_interaction + Axj) * self.dts[tidx]


class ZeroInflation(pl.graph.Node):
    &#39;&#39;&#39;This is the posterior distribution for the zero inflation model. These are used
    to learn when the model should use use the data and when it should not. We do not need
    to trace this object because we set the structural zeros to nans in the trace for 
    filtering.

    TODO: Parallel version of the class
    &#39;&#39;&#39;

    def __init__(self, **kwargs):
        &#39;&#39;&#39;
        Parameters
        ----------
        mp : str
            This is the type of parallelization to use. This is not implemented yet.
        &#39;&#39;&#39;
        kwargs[&#39;name&#39;] = STRNAMES.ZERO_INFLATION
        pl.graph.Node.__init__(self, **kwargs)
        self.value = []
        self._strr = &#39;NA&#39;

        for ridx in range(self.G.data.n_replicates):
            n_timepoints = self.G.data.n_timepoints_for_replicate[ridx]
            self.value.append(np.ones(shape=(len(self.G.data.asvs), n_timepoints), dtype=bool))

    def reset_value_size(self):
        &#39;&#39;&#39;Change the size of the trajectory when we set the intermediate timepoints
        &#39;&#39;&#39;
        n_asvs = self.G.data.n_asvs
        for ridx in range(len(self.value)):
            n_timepoints = self.G.data.n_timepoints_for_replicate[ridx]
            self.value[ridx] = np.ones((n_asvs, n_timepoints), dtype=bool)

    def __str__(self):
        return self._strr

    def initialize(self, value_option, delay=0):
        &#39;&#39;&#39;Initialize the values. Right now this is static and we are not learning this so
        do not do anything fancy

        Parameters
        ----------

        delay : None, int
            How much to delay starting the sampling
        &#39;&#39;&#39;
        if delay is None:
            delay = 0
        if not pl.isint(delay):
            raise TypeError(&#39;`delay` ({}) must be an int&#39;.format(type(delay)))
        self.delay = delay

        if value_option in [None, &#39;auto&#39;]:
            # Set everything to on
            self.value = []
            for ridx in range(self.G.data.n_replicates):
                n_timepoints = self.G.data.n_timepoints_for_replicate[ridx]
                self.value.append(np.ones(
                    shape=(len(self.G.data.asvs), n_timepoints), dtype=bool))
            turn_on = None
            turn_off = None

        elif value_option == &#39;mdsine-cdiff&#39;:
            # Set everything to on except for cdiff before day 28 for every subject
            self.value = []
            for ridx in range(self.G.data.n_replicates):
                n_timepoints = self.G.data.n_timepoints_for_replicate[ridx]
                self.value.append(np.ones(
                    shape=(len(self.G.data.asvs), n_timepoints), dtype=bool))

            # Get cdiff
            cdiff_idx = self.G.data.asvs[&#39;Clostridium-difficile&#39;].idx
            turn_off = []
            turn_on = []
            for ridx in range(self.G.data.n_replicates):
                for tidx, t in enumerate(self.G.data.times[ridx]):
                    for oidx in range(len(self.G.data.asvs)):
                        if t &lt; 28 and oidx == cdiff_idx:
                            self.value[ridx][cdiff_idx, tidx] = False
                            turn_off.append((ridx, tidx, cdiff_idx))
                        else:
                            turn_on.append((ridx, tidx, oidx))

        else:
            raise ValueError(&#39;`value_option` ({}) not recognized&#39;.format(value_option))

        self.G.data.set_zero_inflation(turn_on=turn_on, turn_off=turn_off)
                </code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="mdsine2.filtering.FilteringLogMP"><code class="flex name class">
<span>class <span class="ident">FilteringLogMP</span></span>
<span>(</span><span>mp, zero_inflation_transition_policy, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>This is the posterior for the latent trajectory that are
sampled using a standard normal Metropolis-Hastings proposal.</p>
<p>This is the multiprocessing version of the class. All of the computation is
done on the subject level in parallel.</p>
<h2 id="parallelization-modes">Parallelization Modes</h2>
<p>'debug'
If this is selected, then we dont actually parallelize, but we go in
order of the objects in sequential order. We would do this if we want
to benchmark within each processor or do easier print statements
'full'
This is where each subject gets their own process</p>
<p>This assumes that we are using the log model</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>mp</code></strong> :&ensp;<code>str</code></dt>
<dd>'debug'
Does not actually parallelize, does it serially - we do this in case we
want to debug and/or benchmark
'full'
Send each replicate to a processor each</dd>
<dt><strong><code>zero_inflation_transition_policy</code></strong> :&ensp;<code>None, str</code></dt>
<dd>Type of zero inflation to do. If None then there is no zero inflation</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FilteringLogMP(pl.graph.Node):
    &#39;&#39;&#39;This is the posterior for the latent trajectory that are
    sampled using a standard normal Metropolis-Hastings proposal.

    This is the multiprocessing version of the class. All of the computation is
    done on the subject level in parallel.

    Parallelization Modes
    ---------------------
    &#39;debug&#39;
        If this is selected, then we dont actually parallelize, but we go in
        order of the objects in sequential order. We would do this if we want
        to benchmark within each processor or do easier print statements
    &#39;full&#39;
        This is where each subject gets their own process

    This assumes that we are using the log model
    &#39;&#39;&#39;
    def __init__(self, mp, zero_inflation_transition_policy,**kwargs):
        &#39;&#39;&#39;
        Parameters
        ----------
        mp : str
            &#39;debug&#39;
                Does not actually parallelize, does it serially - we do this in case we
                want to debug and/or benchmark
            &#39;full&#39;
                Send each replicate to a processor each
        zero_inflation_transition_policy : None, str
            Type of zero inflation to do. If None then there is no zero inflation
        &#39;&#39;&#39;
        kwargs[&#39;name&#39;] = STRNAMES.FILTERING
        pl.graph.Node.__init__(self, **kwargs)
        self.x = TrajectorySet(name=STRNAMES.LATENT_TRAJECTORY, G=self.G)
        self.mp = mp
        self.zero_inflation_transition_policy = zero_inflation_transition_policy

        self.print_vals = False
        self._strr = &#39;parallel&#39;

    def __str__(self):
        return self._strr

    @property
    def sample_iter(self):
        # It doesnt matter if we chose q or x because they are both the same
        return self.x.sample_iter

    def initialize(self, x_value_option, a0, a1, v1, v2, essential_timepoints, tune, 
        proposal_init_scale, intermediate_step, intermediate_interpolation=None, 
        delay=0, bandwidth=None, window=None, target_acceptance_rate=0.44, 
        calculate_qpcr_loglik=True):
        &#39;&#39;&#39;Initialize the values of the error model (values for the
        latent and the auxiliary trajectory). Additionally this sets
        the intermediate time points

        Initialize the values of the prior.

        Parameters
        ----------
        x_value_option : str
            Option to initialize the value of the latent trajectory.
            Options
                &#39;coupling&#39;
                    Sample the values around the data with extremely low variance.
                    This also truncates the data so that it stays &gt; 0.
                &#39;moving-avg&#39;
                    Initialize the values using a moving average around the points.
                    The bandwidth of the filter is by number of days, not the order
                    of timepoints. You must also provide the argument `bandwidth`.
                &#39;loess&#39;, &#39;auto&#39;
                    Implements the initialization of the values using LOESS (Locally
                    Estimated Scatterplot Smoothing) algorithm. You must also provide
                    the `window` parameter
        tune : tuple(int, int)
            This is how often to tune the individual covariances
            The first element indicates which MCMC sample to stop the tuning
            The second element is how often to update the proposal covariance
        a0, a1 : float, str
            These are the hyperparameters to calculate the dispersion of the
            negative binomial.
        v1, v2 : float, int, str
            These are the values used to calulcate the coupling variance between
            x and q
        intermediate_step : tuple(str, args), array, None
            This is the type of interemediate timestep to intialize and the arguments
            for them. If this is None, then we do no intermediate timesteps.
            Options:
                &#39;step&#39;
                    args: (stride (numeric), eps (numeric))
                    We simulate at each timepoint every `stride` days.
                    We do not set an intermediate time point if it is within `eps`
                    days of a given data point.
                &#39;preserve-density&#39;
                    args: (n (int), eps (numeric))
                    We preserve the denisty of the given data by only simulating data
                    `n` times between each essential datapoint. If a timepoint is within
                    `eps` days of a given timepoint then we do not make an intermediate
                    point there.
                &#39;manual;
                    args: np.ndarray
                    These are the points that we want to set. If these are not given times
                    then we set them as timepoints
        intermediate_interpolation : str
            This is the type of interpolation to perform on the intermediate timepoints.
            Options:
                &#39;linear-interpolation&#39;, &#39;auto&#39;
                    Perform linear interpolation between the two closest given timepoints
        essential_timepoints : np.ndarray, str, None
            These are the timepoints that must be included in each subject. If one of the
            subjects has a missing timepoint there then we use an intermediate time point
            that this timepoint. It is initialized with linear interpolation. If all of the
            timepoints specified in this vector are included in a subject then nothing is
            done. If it is a str:
                &#39;union&#39;, &#39;auto&#39;
                    We take a union of all the timepoints in each subject and make sure
                    that all of the subjects have all those points.
        bandwidth : float
            This is the day bandwidth of the filter if the initialization method is
            done with &#39;moving-avg&#39;.
        window : int
            This is the window term for the LOESS initialization scheme. This is
            only used if value_initialization is done with &#39;loess&#39;
        target_acceptance_rate : numeric
            This is the target acceptance rate for each time point individually
        calculate_qpcr_loglik : bool
            If True, calculate the loglikelihood of the qPCR measurements during the 
            proposal
        &#39;&#39;&#39;
        if not pl.isint(delay):
            raise TypeError(&#39;`delay` ({}) must be an int&#39;.format(type(delay)))
        if delay &lt; 0:
            raise ValueError(&#39;`delay` ({}) must be &gt;= 0&#39;.format(delay))
        self.delay = delay
        self._there_are_perturbations = self.G.perturbations is not None

        # Set the hyperparameters
        if not pl.isfloat(target_acceptance_rate):
            raise TypeError(&#39;`target_acceptance_rate` must be a float&#39;.format(
                type(target_acceptance_rate)))
        if target_acceptance_rate &lt; 0 or target_acceptance_rate &gt; 1:
            raise ValueError(&#39;`target_acceptance_rate` ({}) must be in (0,1)&#39;.format(
                target_acceptance_rate))
        if not pl.istuple(tune):
            raise TypeError(&#39;`tune` ({}) must be a tuple&#39;.format(type(tune)))
        if len(tune) != 2:
            raise ValueError(&#39;`tune` ({}) must have 2 elements&#39;.format(len(tune)))
        if not pl.isint(tune[0]):
            raise TypeError(&#39;`tune` ({}) 1st parameter must be an int&#39;.format(type(tune[0])))
        if tune[0] &lt; 0:
            raise ValueError(&#39;`tune` ({}) 1st parameter must be &gt; 0&#39;.format(tune[0]))
        if not pl.isint(tune[1]):
            raise TypeError(&#39;`tune` ({}) 2nd parameter must be an int&#39;.format(type(tune[1])))
        if tune[1] &lt; 0:
            raise ValueError(&#39;`tune` ({}) 2nd parameter must be &gt; 0&#39;.format(tune[1]))
        
        if not pl.isnumeric(a0):
            raise TypeError(&#39;`a0` ({}) must be a numeric type&#39;.format(type(a0)))
        elif a0 &lt;= 0:
            raise ValueError(&#39;`a0` ({}) must be &gt; 0&#39;.format(a0))
        if not pl.isnumeric(a1):
            raise TypeError(&#39;`a1` ({}) must be a numeric type&#39;.format(type(a1)))
        elif a1 &lt;= 0:
            raise ValueError(&#39;`a1` ({}) must be &gt; 0&#39;.format(a1))

        if not pl.isnumeric(proposal_init_scale):
            raise TypeError(&#39;`proposal_init_scale` ({}) must be a numeric type (int, float)&#39;.format(
                type(proposal_init_scale)))
        if proposal_init_scale &lt; 0:
            raise ValueError(&#39;`proposal_init_scale` ({}) must be positive&#39;.format(
                proposal_init_scale))

        self.tune = tune
        self.a0 = a0
        self.a1 = a1
        self.target_acceptance_rate = target_acceptance_rate
        self.proposal_init_scale = proposal_init_scale
        self.v1 = v1
        self.v2 = v2

        # Set the essential timepoints (check to see if there is any missing data)
        if essential_timepoints is not None:
            logging.info(&#39;Setting up the essential timepoints&#39;)
            if pl.isstr(essential_timepoints):
                if essential_timepoints in [&#39;auto&#39;, &#39;union&#39;]:
                    essential_timepoints = set()
                    for ts in self.G.data.times:
                        essential_timepoints = essential_timepoints.union(set(list(ts)))
                    essential_timepoints = np.sort(list(essential_timepoints))
                else:
                    raise ValueError(&#39;`essential_timepoints` ({}) not recognized&#39;.format(
                        essential_timepoints))
            elif not pl.isarray(essential_timepoints):
                raise TypeError(&#39;`essential_timepoints` ({}) must be a str or an array&#39;.format(
                    type(essential_timepoints)))
            logging.info(&#39;Essential timepoints: {}&#39;.format(essential_timepoints))
            self.G.data.set_timepoints(times=essential_timepoints, eps=None, reset_timepoints=True)
            self.x.reset_value_size()

        # Set the intermediate timepoints if necessary
        if intermediate_step is not None:
            # Set the intermediate timepoints in the data
            if not pl.istuple(intermediate_step):
                raise TypeError(&#39;`intermediate_step` ({}) must be a tuple&#39;.format(
                    type(intermediate_step)))
            if len(intermediate_step) != 2:
                raise ValueError(&#39;`intermediate_step` ({}) must be length 2&#39;.format(
                    len(intermediate_step)))
            f, args = intermediate_step
            if not pl.isstr(f):
                raise TypeError(&#39;intermediate_step type ({}) must be a str&#39;.format(type(f)))
            if f == &#39;step&#39;:
                if not pl.istuple(args):
                    raise TypeError(&#39;`args` ({}) must be a tuple&#39;.format(type(args)))
                if len(args) != 2:
                    raise TypeError(&#39;`args` ({}) must have 2 arguments&#39;.format(len(args)))
                step, eps = args
                self.G.data.set_timepoints(timestep=step, eps=eps, reset_timepoints=False)
            elif f == &#39;preserve-density&#39;:
                if not pl.istuple(args):
                    raise TypeError(&#39;`args` ({}) must be a tuple&#39;.format(type(args)))
                if len(args) != 2:
                    raise TypeError(&#39;`args` ({}) must have 2 arguments&#39;.format(len(args)))
                n, eps = args
                if not pl.isint(n):
                    raise TypeError(&#39;`n` ({}) must be an int&#39;.format(type(n)))

                # For each timepoint, add `n` intermediate timepoints
                for ridx in range(self.G.data.n_replicates):
                    times = []
                    for i in range(len(self.G.data.times[ridx])-1):
                        t0 = self.G.data.times[ridx][i]
                        t1 = self.G.data.times[ridx][i+1]
                        step = (t1-t0)/(n+1)
                        times = np.append(times, np.arange(t0,t1,step=step))
                    times = np.sort(np.unique(times))
                    # print(&#39;\n\ntimes to put in&#39;, times)
                    self.G.data.set_timepoints(times=times, eps=eps, ridx=ridx, reset_timepoints=False)
                    # print(&#39;times for ridx {}&#39;.format(self.G.data.times[ridx]))
                    # print(&#39;len times&#39;, len(self.G.data.times[ridx]))
                    # print(&#39;data shape&#39;, self.G.data.data[ridx].shape)

                # sys.exit()
            elif f == &#39;manual&#39;:
                raise NotImplementedError(&#39;Not Implemented&#39;)
            else:
                raise ValueError(&#39;`intermediate_step type ({}) not recognized&#39;.format(f))
            self.x.reset_value_size()

        if intermediate_interpolation is not None:
            if intermediate_interpolation in [&#39;linear-interpolation&#39;, &#39;auto&#39;]:
                for ridx in range(self.G.data.n_replicates):
                    for tidx in range(self.G.data.n_timepoints_for_replicate[ridx]):
                        if tidx not in self.G.data.given_timeindices[ridx]:
                            # We need to interpolate this time point
                            # get the previous given and next given timepoint
                            prev_tidx = None
                            for ii in range(tidx-1,-1,-1):
                                if ii in self.G.data.given_timeindices[ridx]:
                                    prev_tidx = ii
                                    break
                            if prev_tidx is None:
                                # Set to the same as the closest forward timepoint then continue
                                next_idx = None
                                for ii in range(tidx+1, self.G.data.n_timepoints_for_replicate[ridx]):
                                    if ii in self.G.data.given_timeindices[ridx]:
                                        next_idx = ii
                                        break
                                self.G.data.data[ridx][:,tidx] = self.G.data.data[ridx][:,next_idx]
                                continue

                            next_tidx = None
                            for ii in range(tidx+1, self.G.data.n_timepoints_for_replicate[ridx]):
                                if ii in self.G.data.given_timeindices[ridx]:
                                    next_tidx = ii
                                    break
                            if next_tidx is None:
                                # Set to the previous timepoint then continue
                                self.G.data.data[ridx][:,tidx] = self.G.data.data[ridx][:,prev_tidx]
                                continue

                            # Interpolate from prev_tidx to next_tidx
                            x = self.G.data.times[ridx][tidx]
                            x0 = self.G.data.times[ridx][prev_tidx]
                            y0 = self.G.data.data[ridx][:,prev_tidx]
                            x1 = self.G.data.times[ridx][next_tidx]
                            y1 = self.G.data.data[ridx][:,next_tidx]
                            self.G.data.data[ridx][:,tidx] = y0 * (1-((x-x0)/(x1-x0))) + y1 * (1-((x1-x)/(x1-x0)))
            else:
                raise ValueError(&#39;`intermediate_interpolation` ({}) not recognized&#39;.format(intermediate_interpolation))

        # Initialize the latent trajectory
        if not pl.isstr(x_value_option):
            raise TypeError(&#39;`x_value_option` ({}) is not a str&#39;.format(type(x_value_option)))
        if x_value_option == &#39;coupling&#39;:
            self._init_coupling()
        elif x_value_option == &#39;moving-avg&#39;:
            if not pl.isnumeric(bandwidth):
                raise TypeError(&#39;`bandwidth` ({}) must be a numeric&#39;.format(type(bandwidth)))
            if bandwidth &lt;= 0:
                raise ValueError(&#39;`bandwidth` ({}) must be positive&#39;.format(bandwidth))
            self.bandwidth = bandwidth
            self._init_moving_avg()
        elif x_value_option in [&#39;loess&#39;, &#39;auto&#39;]:
            if window is None:
                raise TypeError(&#39;If `value_option` is loess, then `window` must be specified&#39;)
            if not pl.isint(window):
                raise TypeError(&#39;`window` ({}) must be an int&#39;.format(type(window)))
            if window &lt;= 0:
                raise ValueError(&#39;`window` ({}) must be &gt; 0&#39;.format(window))
            self.window = window
            self._init_loess()
        else:
            raise ValueError(&#39;`x_value_option` ({}) not recognized&#39;.format(x_value_option))

        # Get necessary data and set the parallel objects
        if self._there_are_perturbations:
            pert_starts = []
            pert_ends = []
            for perturbation in self.G.perturbations:
                pert_starts.append(perturbation.start)
                pert_ends.append(perturbation.end)
        else:
            pert_starts = None
            pert_ends = None

        if self.mp is None:
            self.mp = &#39;debug&#39;
        if not pl.isstr(self.mp):
            raise TypeError(&#39;`mp` ({}) must either be a string or None&#39;.format(type(self.mp)))
        if self.mp == &#39;debug&#39;:
            self.pool = []
        elif self.mp == &#39;full&#39;:
            self.pool = pl.multiprocessing.PersistentPool(G=self.G, ptype=&#39;sadw&#39;)
            self.worker_pids = []
        else:
            raise ValueError(&#39;`mp` ({}) not recognized&#39;.format(self.mp))

        for ridx in range(self.G.data.n_replicates):
            # Set up qPCR measurements and reads to send
            qpcr_log_measurements = {}
            for t in self.G.data.given_timepoints[ridx]:
                qpcr_log_measurements[t] = self.G.data.qpcr[ridx][t].log_data
            reads = self.G.data.subjects.iloc(ridx).reads

            worker = SubjectLogTrajectorySetMP()
            worker.initialize(
                zero_inflation_transition_policy=self.zero_inflation_transition_policy,
                times=self.G.data.times[ridx],
                qpcr_log_measurements=qpcr_log_measurements,
                reads=reads,
                there_are_intermediate_timepoints=True,
                there_are_perturbations=self._there_are_perturbations,
                pv_global=self.G[REPRNAMES.PROCESSVAR].global_variance,
                x_prior_mean=np.log(1e7),
                x_prior_std=1e10,
                tune=tune[1],
                delay=delay,
                end_iter=tune[0],
                proposal_init_scale=proposal_init_scale,
                a0=a0,
                a1=a1,
                x=self.x[ridx].value,
                pert_starts=np.asarray(pert_starts),
                pert_ends=np.asarray(pert_ends),
                ridx=ridx,
                calculate_qpcr_loglik=calculate_qpcr_loglik,
                h5py_xname=self.x[ridx].name,
                target_acceptance_rate=self.target_acceptance_rate)
            if self.mp == &#39;debug&#39;:
                self.pool.append(worker)
            elif self.mp == &#39;full&#39;:
                pid = self.pool.add_worker(worker)
                self.worker_pids.append(pid)

        # Set the data to the latent values
        self.set_latent_as_data(update_values=False)

        self.total_n_datapoints = 0
        for ridx in range(self.G.data.n_replicates):
            self.total_n_datapoints += self.x[ridx].value.shape[0] * self.x[ridx].value.shape[1]

    def _init_coupling(self):
        &#39;&#39;&#39;Initialize `x` by sampling around the data using a small
        variance using a truncated normal distribution
        &#39;&#39;&#39;
        for ridx, tidx, oidx in self.x.iter_indices():
            val = self.G.data.data[ridx][oidx,tidx]
            self.x[ridx][oidx,tidx] = pl.random.truncnormal.sample(
                mean=val,
                std=math.sqrt(self.v1 * (val ** 2) + self.v2),
                low=0, high=float(&#39;inf&#39;))

    def _init_moving_avg(self):
        &#39;&#39;&#39;Initializes `x` by using a moving
        average over the data - using `self.bandwidth` as the bandwidth
        of number of days - it then samples around that point using the
        coupling variance.

        If there are no other points within the bandwidth around the point,
        then it just samples around the current timepoint with the coupling
        variance.
        &#39;&#39;&#39;
        for ridx in range(self.G.data.n_replicates):
            for tidx in range(self.G.data.n_timepoints_for_replicate[ridx]):
                tidx_low = np.searchsorted(
                    self.G.data.times[ridx], self.G.data.times[ridx][tidx]-self.bandwidth)
                tidx_high = np.searchsorted(
                    self.G.data.times[ridx], self.G.data.times[ridx][tidx]+self.bandwidth)

                for oidx in range(len(self.G.data.asvs)):
                    val = np.mean(self.G.data.data[ridx][oidx, tidx_low: tidx_high])
                    self.x[ridx][oidx,tidx] = pl.random.truncnormal.sample(
                        mean=val,
                        std=math.sqrt(self.v1 * (val ** 2) + self.v2),
                        low=0, high=float(&#39;inf&#39;))

    def _init_loess(self):
        &#39;&#39;&#39;Initialize the data using LOESS algorithm and then samples around that
        the coupling variance we implement the LOESS algorithm in the module
        `fit_loess.py`
        &#39;&#39;&#39;
        for ridx in range(self.G.data.n_replicates):
            xx = self.G.data.times[ridx]
            for oidx in range(len(self.G.data.asvs)):
                yy = self.G.data.data[ridx][oidx, :]
                loess = Loess(xx, yy)

                for tidx, t in enumerate(self.G.data.times[ridx]):
                    val = loess.estimate(t, window=self.window)
                    self.x[ridx][oidx,tidx] = pl.random.truncnormal.sample(
                        mean=val,
                        std=math.sqrt(self.v1 * (val ** 2) + self.v2),
                        low=0, high=float(&#39;inf&#39;))

                    if np.isnan(self.x[ridx][oidx, tidx]):
                        print(&#39;crashed here&#39;, ridx, tidx, oidx)
                        print(&#39;mean&#39;, val)
                        print(&#39;t&#39;, t)
                        print(&#39;yy&#39;, yy)
                        print(&#39;std&#39;, math.sqrt(self.v1 * (val ** 2) + self.v2))
                        raise ValueError(&#39;&#39;)

    def set_latent_as_data(self, update_values=True):
        &#39;&#39;&#39;Change the values in the data matrix so that it is the latent variables
        &#39;&#39;&#39;
        data = []
        for obj in self.x.value:
            data.append(obj.value)
        self.G.data.data = data
        if update_values:
            self.G.data.update_values()

    def add_trace(self):
        self.x.add_trace()

    def add_init_value(self):
        self.x.add_init_value()

    def set_trace(self, *args, **kwargs):
        self.x.set_trace(*args, **kwargs)

    def kill(self):
        if self.mp == &#39;full&#39;:
            self.pool.kill()

    def update(self):
        &#39;&#39;&#39;Send out to each parallel object
        &#39;&#39;&#39;
        if self.sample_iter &lt; self.delay:
            return
        start_time = time.time()

        growth = self.G[REPRNAMES.GROWTH_VALUE].value.ravel()
        self_interactions = self.G[REPRNAMES.SELF_INTERACTION_VALUE].value.ravel()
        pv = self.G[REPRNAMES.PROCESSVAR].value
        interactions = self.G[REPRNAMES.INTERACTIONS_OBJ].get_datalevel_value_matrix(
            set_neg_indicators_to_nan=False)
        perts = None
        if self._there_are_perturbations:
            perts = []
            for perturbation in self.G.perturbations:
                perts.append(perturbation.item_array().reshape(-1,1))
            perts = np.hstack(perts)

        # zero_inflation = [self.G[REPRNAMES.ZERO_INFLATION].value[ridx] for ridx in range(self.G.data.n_replicates)]
        qpcr_vars = []
        for aaa in self.G[REPRNAMES.QPCR_VARIANCES].value:
            qpcr_vars.append(aaa.value)
        
        
        kwargs = {&#39;growth&#39;:growth, &#39;self_interactions&#39;:self_interactions,
            &#39;pv&#39;:pv, &#39;interactions&#39;:interactions, &#39;perturbations&#39;:perts, 
            &#39;zero_inflation_data&#39;: None, &#39;qpcr_variances&#39;:qpcr_vars}

        str_acc = [None]*self.G.data.n_replicates
        if self.mp == &#39;debug&#39;:

            for ridx in range(self.G.data.n_replicates):
                _, x, acc_rate = self.pool[ridx].persistent_run(**kwargs)
                self.x[ridx].value = x
                str_acc[ridx] = &#39;{:.3f}&#39;.format(acc_rate)

        else:
            # raise NotImplementedError(&#39;Multiprocessing for filtering with zero inflation &#39; \
            #     &#39;is not implemented&#39;)
            ret = self.pool.map(func=&#39;persistent_run&#39;, args=kwargs)
            for ridx, x, acc_rate in ret:
                self.x[ridx].value = x
                str_acc[ridx] = &#39;{:.3f}&#39;.format(acc_rate)

        self.set_latent_as_data()

        t = time.time() - start_time
        try:
            self._strr = &#39;Time: {:.4f}, Acc: {}, data/sec: {:.2f}&#39;.format(t,
                str(str_acc).replace(&#34;&#39;&#34;,&#39;&#39;), self.total_n_datapoints/t)
        except:
            self._strr = &#39;NA&#39;</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mdsine2.pylab.graph.Node" href="pylab/graph.html#mdsine2.pylab.graph.Node">Node</a></li>
<li><a title="mdsine2.pylab.graph.BaseNode" href="pylab/graph.html#mdsine2.pylab.graph.BaseNode">BaseNode</a></li>
<li><a title="mdsine2.pylab.base.Saveable" href="pylab/base.html#mdsine2.pylab.base.Saveable">Saveable</a></li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="mdsine2.filtering.FilteringLogMP.sample_iter"><code class="name">var <span class="ident">sample_iter</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def sample_iter(self):
    # It doesnt matter if we chose q or x because they are both the same
    return self.x.sample_iter</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="mdsine2.filtering.FilteringLogMP.add_init_value"><code class="name flex">
<span>def <span class="ident">add_init_value</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_init_value(self):
    self.x.add_init_value()</code></pre>
</details>
</dd>
<dt id="mdsine2.filtering.FilteringLogMP.add_trace"><code class="name flex">
<span>def <span class="ident">add_trace</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_trace(self):
    self.x.add_trace()</code></pre>
</details>
</dd>
<dt id="mdsine2.filtering.FilteringLogMP.initialize"><code class="name flex">
<span>def <span class="ident">initialize</span></span>(<span>self, x_value_option, a0, a1, v1, v2, essential_timepoints, tune, proposal_init_scale, intermediate_step, intermediate_interpolation=None, delay=0, bandwidth=None, window=None, target_acceptance_rate=0.44, calculate_qpcr_loglik=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Initialize the values of the error model (values for the
latent and the auxiliary trajectory). Additionally this sets
the intermediate time points</p>
<p>Initialize the values of the prior.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x_value_option</code></strong> :&ensp;<code>str</code></dt>
<dd>Option to initialize the value of the latent trajectory.
Options
'coupling'
Sample the values around the data with extremely low variance.
This also truncates the data so that it stays &gt; 0.
'moving-avg'
Initialize the values using a moving average around the points.
The bandwidth of the filter is by number of days, not the order
of timepoints. You must also provide the argument <code>bandwidth</code>.
'loess', 'auto'
Implements the initialization of the values using LOESS (Locally
Estimated Scatterplot Smoothing) algorithm. You must also provide
the <code>window</code> parameter</dd>
<dt><strong><code>tune</code></strong> :&ensp;<code>tuple(int, int)</code></dt>
<dd>This is how often to tune the individual covariances
The first element indicates which MCMC sample to stop the tuning
The second element is how often to update the proposal covariance</dd>
<dt><strong><code>a0</code></strong>, <strong><code>a1</code></strong> :&ensp;<code>float, str</code></dt>
<dd>These are the hyperparameters to calculate the dispersion of the
negative binomial.</dd>
<dt><strong><code>v1</code></strong>, <strong><code>v2</code></strong> :&ensp;<code>float, int, str</code></dt>
<dd>These are the values used to calulcate the coupling variance between
x and q</dd>
<dt><strong><code>intermediate_step</code></strong> :&ensp;<code>tuple(str, args), array, None</code></dt>
<dd>This is the type of interemediate timestep to intialize and the arguments
for them. If this is None, then we do no intermediate timesteps.
Options:
'step'
args: (stride (numeric), eps (numeric))
We simulate at each timepoint every <code>stride</code> days.
We do not set an intermediate time point if it is within <code>eps</code>
days of a given data point.
'preserve-density'
args: (n (int), eps (numeric))
We preserve the denisty of the given data by only simulating data
<code>n</code> times between each essential datapoint. If a timepoint is within
<code>eps</code> days of a given timepoint then we do not make an intermediate
point there.
'manual;
args: np.ndarray
These are the points that we want to set. If these are not given times
then we set them as timepoints</dd>
<dt><strong><code>intermediate_interpolation</code></strong> :&ensp;<code>str</code></dt>
<dd>This is the type of interpolation to perform on the intermediate timepoints.
Options:
'linear-interpolation', 'auto'
Perform linear interpolation between the two closest given timepoints</dd>
<dt><strong><code>essential_timepoints</code></strong> :&ensp;<code>np.ndarray, str, None</code></dt>
<dd>These are the timepoints that must be included in each subject. If one of the
subjects has a missing timepoint there then we use an intermediate time point
that this timepoint. It is initialized with linear interpolation. If all of the
timepoints specified in this vector are included in a subject then nothing is
done. If it is a str:
'union', 'auto'
We take a union of all the timepoints in each subject and make sure
that all of the subjects have all those points.</dd>
<dt><strong><code>bandwidth</code></strong> :&ensp;<code>float</code></dt>
<dd>This is the day bandwidth of the filter if the initialization method is
done with 'moving-avg'.</dd>
<dt><strong><code>window</code></strong> :&ensp;<code>int</code></dt>
<dd>This is the window term for the LOESS initialization scheme. This is
only used if value_initialization is done with 'loess'</dd>
<dt><strong><code>target_acceptance_rate</code></strong> :&ensp;<code>numeric</code></dt>
<dd>This is the target acceptance rate for each time point individually</dd>
<dt><strong><code>calculate_qpcr_loglik</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, calculate the loglikelihood of the qPCR measurements during the
proposal</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def initialize(self, x_value_option, a0, a1, v1, v2, essential_timepoints, tune, 
    proposal_init_scale, intermediate_step, intermediate_interpolation=None, 
    delay=0, bandwidth=None, window=None, target_acceptance_rate=0.44, 
    calculate_qpcr_loglik=True):
    &#39;&#39;&#39;Initialize the values of the error model (values for the
    latent and the auxiliary trajectory). Additionally this sets
    the intermediate time points

    Initialize the values of the prior.

    Parameters
    ----------
    x_value_option : str
        Option to initialize the value of the latent trajectory.
        Options
            &#39;coupling&#39;
                Sample the values around the data with extremely low variance.
                This also truncates the data so that it stays &gt; 0.
            &#39;moving-avg&#39;
                Initialize the values using a moving average around the points.
                The bandwidth of the filter is by number of days, not the order
                of timepoints. You must also provide the argument `bandwidth`.
            &#39;loess&#39;, &#39;auto&#39;
                Implements the initialization of the values using LOESS (Locally
                Estimated Scatterplot Smoothing) algorithm. You must also provide
                the `window` parameter
    tune : tuple(int, int)
        This is how often to tune the individual covariances
        The first element indicates which MCMC sample to stop the tuning
        The second element is how often to update the proposal covariance
    a0, a1 : float, str
        These are the hyperparameters to calculate the dispersion of the
        negative binomial.
    v1, v2 : float, int, str
        These are the values used to calulcate the coupling variance between
        x and q
    intermediate_step : tuple(str, args), array, None
        This is the type of interemediate timestep to intialize and the arguments
        for them. If this is None, then we do no intermediate timesteps.
        Options:
            &#39;step&#39;
                args: (stride (numeric), eps (numeric))
                We simulate at each timepoint every `stride` days.
                We do not set an intermediate time point if it is within `eps`
                days of a given data point.
            &#39;preserve-density&#39;
                args: (n (int), eps (numeric))
                We preserve the denisty of the given data by only simulating data
                `n` times between each essential datapoint. If a timepoint is within
                `eps` days of a given timepoint then we do not make an intermediate
                point there.
            &#39;manual;
                args: np.ndarray
                These are the points that we want to set. If these are not given times
                then we set them as timepoints
    intermediate_interpolation : str
        This is the type of interpolation to perform on the intermediate timepoints.
        Options:
            &#39;linear-interpolation&#39;, &#39;auto&#39;
                Perform linear interpolation between the two closest given timepoints
    essential_timepoints : np.ndarray, str, None
        These are the timepoints that must be included in each subject. If one of the
        subjects has a missing timepoint there then we use an intermediate time point
        that this timepoint. It is initialized with linear interpolation. If all of the
        timepoints specified in this vector are included in a subject then nothing is
        done. If it is a str:
            &#39;union&#39;, &#39;auto&#39;
                We take a union of all the timepoints in each subject and make sure
                that all of the subjects have all those points.
    bandwidth : float
        This is the day bandwidth of the filter if the initialization method is
        done with &#39;moving-avg&#39;.
    window : int
        This is the window term for the LOESS initialization scheme. This is
        only used if value_initialization is done with &#39;loess&#39;
    target_acceptance_rate : numeric
        This is the target acceptance rate for each time point individually
    calculate_qpcr_loglik : bool
        If True, calculate the loglikelihood of the qPCR measurements during the 
        proposal
    &#39;&#39;&#39;
    if not pl.isint(delay):
        raise TypeError(&#39;`delay` ({}) must be an int&#39;.format(type(delay)))
    if delay &lt; 0:
        raise ValueError(&#39;`delay` ({}) must be &gt;= 0&#39;.format(delay))
    self.delay = delay
    self._there_are_perturbations = self.G.perturbations is not None

    # Set the hyperparameters
    if not pl.isfloat(target_acceptance_rate):
        raise TypeError(&#39;`target_acceptance_rate` must be a float&#39;.format(
            type(target_acceptance_rate)))
    if target_acceptance_rate &lt; 0 or target_acceptance_rate &gt; 1:
        raise ValueError(&#39;`target_acceptance_rate` ({}) must be in (0,1)&#39;.format(
            target_acceptance_rate))
    if not pl.istuple(tune):
        raise TypeError(&#39;`tune` ({}) must be a tuple&#39;.format(type(tune)))
    if len(tune) != 2:
        raise ValueError(&#39;`tune` ({}) must have 2 elements&#39;.format(len(tune)))
    if not pl.isint(tune[0]):
        raise TypeError(&#39;`tune` ({}) 1st parameter must be an int&#39;.format(type(tune[0])))
    if tune[0] &lt; 0:
        raise ValueError(&#39;`tune` ({}) 1st parameter must be &gt; 0&#39;.format(tune[0]))
    if not pl.isint(tune[1]):
        raise TypeError(&#39;`tune` ({}) 2nd parameter must be an int&#39;.format(type(tune[1])))
    if tune[1] &lt; 0:
        raise ValueError(&#39;`tune` ({}) 2nd parameter must be &gt; 0&#39;.format(tune[1]))
    
    if not pl.isnumeric(a0):
        raise TypeError(&#39;`a0` ({}) must be a numeric type&#39;.format(type(a0)))
    elif a0 &lt;= 0:
        raise ValueError(&#39;`a0` ({}) must be &gt; 0&#39;.format(a0))
    if not pl.isnumeric(a1):
        raise TypeError(&#39;`a1` ({}) must be a numeric type&#39;.format(type(a1)))
    elif a1 &lt;= 0:
        raise ValueError(&#39;`a1` ({}) must be &gt; 0&#39;.format(a1))

    if not pl.isnumeric(proposal_init_scale):
        raise TypeError(&#39;`proposal_init_scale` ({}) must be a numeric type (int, float)&#39;.format(
            type(proposal_init_scale)))
    if proposal_init_scale &lt; 0:
        raise ValueError(&#39;`proposal_init_scale` ({}) must be positive&#39;.format(
            proposal_init_scale))

    self.tune = tune
    self.a0 = a0
    self.a1 = a1
    self.target_acceptance_rate = target_acceptance_rate
    self.proposal_init_scale = proposal_init_scale
    self.v1 = v1
    self.v2 = v2

    # Set the essential timepoints (check to see if there is any missing data)
    if essential_timepoints is not None:
        logging.info(&#39;Setting up the essential timepoints&#39;)
        if pl.isstr(essential_timepoints):
            if essential_timepoints in [&#39;auto&#39;, &#39;union&#39;]:
                essential_timepoints = set()
                for ts in self.G.data.times:
                    essential_timepoints = essential_timepoints.union(set(list(ts)))
                essential_timepoints = np.sort(list(essential_timepoints))
            else:
                raise ValueError(&#39;`essential_timepoints` ({}) not recognized&#39;.format(
                    essential_timepoints))
        elif not pl.isarray(essential_timepoints):
            raise TypeError(&#39;`essential_timepoints` ({}) must be a str or an array&#39;.format(
                type(essential_timepoints)))
        logging.info(&#39;Essential timepoints: {}&#39;.format(essential_timepoints))
        self.G.data.set_timepoints(times=essential_timepoints, eps=None, reset_timepoints=True)
        self.x.reset_value_size()

    # Set the intermediate timepoints if necessary
    if intermediate_step is not None:
        # Set the intermediate timepoints in the data
        if not pl.istuple(intermediate_step):
            raise TypeError(&#39;`intermediate_step` ({}) must be a tuple&#39;.format(
                type(intermediate_step)))
        if len(intermediate_step) != 2:
            raise ValueError(&#39;`intermediate_step` ({}) must be length 2&#39;.format(
                len(intermediate_step)))
        f, args = intermediate_step
        if not pl.isstr(f):
            raise TypeError(&#39;intermediate_step type ({}) must be a str&#39;.format(type(f)))
        if f == &#39;step&#39;:
            if not pl.istuple(args):
                raise TypeError(&#39;`args` ({}) must be a tuple&#39;.format(type(args)))
            if len(args) != 2:
                raise TypeError(&#39;`args` ({}) must have 2 arguments&#39;.format(len(args)))
            step, eps = args
            self.G.data.set_timepoints(timestep=step, eps=eps, reset_timepoints=False)
        elif f == &#39;preserve-density&#39;:
            if not pl.istuple(args):
                raise TypeError(&#39;`args` ({}) must be a tuple&#39;.format(type(args)))
            if len(args) != 2:
                raise TypeError(&#39;`args` ({}) must have 2 arguments&#39;.format(len(args)))
            n, eps = args
            if not pl.isint(n):
                raise TypeError(&#39;`n` ({}) must be an int&#39;.format(type(n)))

            # For each timepoint, add `n` intermediate timepoints
            for ridx in range(self.G.data.n_replicates):
                times = []
                for i in range(len(self.G.data.times[ridx])-1):
                    t0 = self.G.data.times[ridx][i]
                    t1 = self.G.data.times[ridx][i+1]
                    step = (t1-t0)/(n+1)
                    times = np.append(times, np.arange(t0,t1,step=step))
                times = np.sort(np.unique(times))
                # print(&#39;\n\ntimes to put in&#39;, times)
                self.G.data.set_timepoints(times=times, eps=eps, ridx=ridx, reset_timepoints=False)
                # print(&#39;times for ridx {}&#39;.format(self.G.data.times[ridx]))
                # print(&#39;len times&#39;, len(self.G.data.times[ridx]))
                # print(&#39;data shape&#39;, self.G.data.data[ridx].shape)

            # sys.exit()
        elif f == &#39;manual&#39;:
            raise NotImplementedError(&#39;Not Implemented&#39;)
        else:
            raise ValueError(&#39;`intermediate_step type ({}) not recognized&#39;.format(f))
        self.x.reset_value_size()

    if intermediate_interpolation is not None:
        if intermediate_interpolation in [&#39;linear-interpolation&#39;, &#39;auto&#39;]:
            for ridx in range(self.G.data.n_replicates):
                for tidx in range(self.G.data.n_timepoints_for_replicate[ridx]):
                    if tidx not in self.G.data.given_timeindices[ridx]:
                        # We need to interpolate this time point
                        # get the previous given and next given timepoint
                        prev_tidx = None
                        for ii in range(tidx-1,-1,-1):
                            if ii in self.G.data.given_timeindices[ridx]:
                                prev_tidx = ii
                                break
                        if prev_tidx is None:
                            # Set to the same as the closest forward timepoint then continue
                            next_idx = None
                            for ii in range(tidx+1, self.G.data.n_timepoints_for_replicate[ridx]):
                                if ii in self.G.data.given_timeindices[ridx]:
                                    next_idx = ii
                                    break
                            self.G.data.data[ridx][:,tidx] = self.G.data.data[ridx][:,next_idx]
                            continue

                        next_tidx = None
                        for ii in range(tidx+1, self.G.data.n_timepoints_for_replicate[ridx]):
                            if ii in self.G.data.given_timeindices[ridx]:
                                next_tidx = ii
                                break
                        if next_tidx is None:
                            # Set to the previous timepoint then continue
                            self.G.data.data[ridx][:,tidx] = self.G.data.data[ridx][:,prev_tidx]
                            continue

                        # Interpolate from prev_tidx to next_tidx
                        x = self.G.data.times[ridx][tidx]
                        x0 = self.G.data.times[ridx][prev_tidx]
                        y0 = self.G.data.data[ridx][:,prev_tidx]
                        x1 = self.G.data.times[ridx][next_tidx]
                        y1 = self.G.data.data[ridx][:,next_tidx]
                        self.G.data.data[ridx][:,tidx] = y0 * (1-((x-x0)/(x1-x0))) + y1 * (1-((x1-x)/(x1-x0)))
        else:
            raise ValueError(&#39;`intermediate_interpolation` ({}) not recognized&#39;.format(intermediate_interpolation))

    # Initialize the latent trajectory
    if not pl.isstr(x_value_option):
        raise TypeError(&#39;`x_value_option` ({}) is not a str&#39;.format(type(x_value_option)))
    if x_value_option == &#39;coupling&#39;:
        self._init_coupling()
    elif x_value_option == &#39;moving-avg&#39;:
        if not pl.isnumeric(bandwidth):
            raise TypeError(&#39;`bandwidth` ({}) must be a numeric&#39;.format(type(bandwidth)))
        if bandwidth &lt;= 0:
            raise ValueError(&#39;`bandwidth` ({}) must be positive&#39;.format(bandwidth))
        self.bandwidth = bandwidth
        self._init_moving_avg()
    elif x_value_option in [&#39;loess&#39;, &#39;auto&#39;]:
        if window is None:
            raise TypeError(&#39;If `value_option` is loess, then `window` must be specified&#39;)
        if not pl.isint(window):
            raise TypeError(&#39;`window` ({}) must be an int&#39;.format(type(window)))
        if window &lt;= 0:
            raise ValueError(&#39;`window` ({}) must be &gt; 0&#39;.format(window))
        self.window = window
        self._init_loess()
    else:
        raise ValueError(&#39;`x_value_option` ({}) not recognized&#39;.format(x_value_option))

    # Get necessary data and set the parallel objects
    if self._there_are_perturbations:
        pert_starts = []
        pert_ends = []
        for perturbation in self.G.perturbations:
            pert_starts.append(perturbation.start)
            pert_ends.append(perturbation.end)
    else:
        pert_starts = None
        pert_ends = None

    if self.mp is None:
        self.mp = &#39;debug&#39;
    if not pl.isstr(self.mp):
        raise TypeError(&#39;`mp` ({}) must either be a string or None&#39;.format(type(self.mp)))
    if self.mp == &#39;debug&#39;:
        self.pool = []
    elif self.mp == &#39;full&#39;:
        self.pool = pl.multiprocessing.PersistentPool(G=self.G, ptype=&#39;sadw&#39;)
        self.worker_pids = []
    else:
        raise ValueError(&#39;`mp` ({}) not recognized&#39;.format(self.mp))

    for ridx in range(self.G.data.n_replicates):
        # Set up qPCR measurements and reads to send
        qpcr_log_measurements = {}
        for t in self.G.data.given_timepoints[ridx]:
            qpcr_log_measurements[t] = self.G.data.qpcr[ridx][t].log_data
        reads = self.G.data.subjects.iloc(ridx).reads

        worker = SubjectLogTrajectorySetMP()
        worker.initialize(
            zero_inflation_transition_policy=self.zero_inflation_transition_policy,
            times=self.G.data.times[ridx],
            qpcr_log_measurements=qpcr_log_measurements,
            reads=reads,
            there_are_intermediate_timepoints=True,
            there_are_perturbations=self._there_are_perturbations,
            pv_global=self.G[REPRNAMES.PROCESSVAR].global_variance,
            x_prior_mean=np.log(1e7),
            x_prior_std=1e10,
            tune=tune[1],
            delay=delay,
            end_iter=tune[0],
            proposal_init_scale=proposal_init_scale,
            a0=a0,
            a1=a1,
            x=self.x[ridx].value,
            pert_starts=np.asarray(pert_starts),
            pert_ends=np.asarray(pert_ends),
            ridx=ridx,
            calculate_qpcr_loglik=calculate_qpcr_loglik,
            h5py_xname=self.x[ridx].name,
            target_acceptance_rate=self.target_acceptance_rate)
        if self.mp == &#39;debug&#39;:
            self.pool.append(worker)
        elif self.mp == &#39;full&#39;:
            pid = self.pool.add_worker(worker)
            self.worker_pids.append(pid)

    # Set the data to the latent values
    self.set_latent_as_data(update_values=False)

    self.total_n_datapoints = 0
    for ridx in range(self.G.data.n_replicates):
        self.total_n_datapoints += self.x[ridx].value.shape[0] * self.x[ridx].value.shape[1]</code></pre>
</details>
</dd>
<dt id="mdsine2.filtering.FilteringLogMP.kill"><code class="name flex">
<span>def <span class="ident">kill</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def kill(self):
    if self.mp == &#39;full&#39;:
        self.pool.kill()</code></pre>
</details>
</dd>
<dt id="mdsine2.filtering.FilteringLogMP.set_latent_as_data"><code class="name flex">
<span>def <span class="ident">set_latent_as_data</span></span>(<span>self, update_values=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Change the values in the data matrix so that it is the latent variables</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_latent_as_data(self, update_values=True):
    &#39;&#39;&#39;Change the values in the data matrix so that it is the latent variables
    &#39;&#39;&#39;
    data = []
    for obj in self.x.value:
        data.append(obj.value)
    self.G.data.data = data
    if update_values:
        self.G.data.update_values()</code></pre>
</details>
</dd>
<dt id="mdsine2.filtering.FilteringLogMP.set_trace"><code class="name flex">
<span>def <span class="ident">set_trace</span></span>(<span>self, *args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_trace(self, *args, **kwargs):
    self.x.set_trace(*args, **kwargs)</code></pre>
</details>
</dd>
<dt id="mdsine2.filtering.FilteringLogMP.update"><code class="name flex">
<span>def <span class="ident">update</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Send out to each parallel object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update(self):
    &#39;&#39;&#39;Send out to each parallel object
    &#39;&#39;&#39;
    if self.sample_iter &lt; self.delay:
        return
    start_time = time.time()

    growth = self.G[REPRNAMES.GROWTH_VALUE].value.ravel()
    self_interactions = self.G[REPRNAMES.SELF_INTERACTION_VALUE].value.ravel()
    pv = self.G[REPRNAMES.PROCESSVAR].value
    interactions = self.G[REPRNAMES.INTERACTIONS_OBJ].get_datalevel_value_matrix(
        set_neg_indicators_to_nan=False)
    perts = None
    if self._there_are_perturbations:
        perts = []
        for perturbation in self.G.perturbations:
            perts.append(perturbation.item_array().reshape(-1,1))
        perts = np.hstack(perts)

    # zero_inflation = [self.G[REPRNAMES.ZERO_INFLATION].value[ridx] for ridx in range(self.G.data.n_replicates)]
    qpcr_vars = []
    for aaa in self.G[REPRNAMES.QPCR_VARIANCES].value:
        qpcr_vars.append(aaa.value)
    
    
    kwargs = {&#39;growth&#39;:growth, &#39;self_interactions&#39;:self_interactions,
        &#39;pv&#39;:pv, &#39;interactions&#39;:interactions, &#39;perturbations&#39;:perts, 
        &#39;zero_inflation_data&#39;: None, &#39;qpcr_variances&#39;:qpcr_vars}

    str_acc = [None]*self.G.data.n_replicates
    if self.mp == &#39;debug&#39;:

        for ridx in range(self.G.data.n_replicates):
            _, x, acc_rate = self.pool[ridx].persistent_run(**kwargs)
            self.x[ridx].value = x
            str_acc[ridx] = &#39;{:.3f}&#39;.format(acc_rate)

    else:
        # raise NotImplementedError(&#39;Multiprocessing for filtering with zero inflation &#39; \
        #     &#39;is not implemented&#39;)
        ret = self.pool.map(func=&#39;persistent_run&#39;, args=kwargs)
        for ridx, x, acc_rate in ret:
            self.x[ridx].value = x
            str_acc[ridx] = &#39;{:.3f}&#39;.format(acc_rate)

    self.set_latent_as_data()

    t = time.time() - start_time
    try:
        self._strr = &#39;Time: {:.4f}, Acc: {}, data/sec: {:.2f}&#39;.format(t,
            str(str_acc).replace(&#34;&#39;&#34;,&#39;&#39;), self.total_n_datapoints/t)
    except:
        self._strr = &#39;NA&#39;</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="mdsine2.pylab.graph.Node" href="pylab/graph.html#mdsine2.pylab.graph.Node">Node</a></b></code>:
<ul class="hlist">
<li><code><a title="mdsine2.pylab.graph.Node.add_child" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_child">add_child</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.add_parent" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_parent">add_parent</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.add_prior" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_prior">add_prior</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.add_undirected" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_undirected">add_undirected</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.degree" href="pylab/graph.html#mdsine2.pylab.graph.Node.degree">degree</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.delete" href="pylab/graph.html#mdsine2.pylab.graph.BaseNode.delete">delete</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.get_adjacent_keys" href="pylab/graph.html#mdsine2.pylab.graph.Node.get_adjacent_keys">get_adjacent_keys</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.load" href="pylab/base.html#mdsine2.pylab.base.Saveable.load">load</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.metropolis" href="pylab/graph.html#mdsine2.pylab.graph.Node.metropolis">metropolis</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.save" href="pylab/base.html#mdsine2.pylab.base.Saveable.save">save</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.set_save_location" href="pylab/base.html#mdsine2.pylab.base.Saveable.set_save_location">set_save_location</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="mdsine2.filtering.SubjectLogTrajectorySetMP"><code class="flex name class">
<span>class <span class="ident">SubjectLogTrajectorySetMP</span></span>
</code></dt>
<dd>
<div class="desc"><p>This performs filtering on a multiprocessing level. We send the
other parameters of the model and return the filtered <code>x</code> and values.
With multiprocessing, this class has ~91% efficiency. Additionally, the code
in this class is optimized to be ~20X faster than the code in <code>Filtering</code>. This
assumes we have the log model.</p>
<p>It might seem unneccessary to have so many local attributes, but it speeds up
the inference considerably if we index a value from an array once and store it
as a float instead of repeatedly indexing the array - the difference in
reality is super small but we do this so often that it adds up to ~40% speedup
as supposed to not doing it - so we do this as often as possible - This speedup
is even greater for indexing keys of dictionaries and getting parameters of objects.</p>
<h2 id="general-efficiency-speedups">General Efficiency Speedups</h2>
<p>All of these are done relative to a non-optimized filtering implementation
- Specialized sampling and logpdf functions. About 95% faster than
scipy or numpy functions. All of these add up to a ~35% speed up
- Explicit function definitions:
instead of doing <code>self.prior.logpdf(&hellip;)</code>, we do
<code>pl.random.normal.logpdf(&hellip;)</code>, about a ~10% overall speedup
- Precomputation of values so that the least amout of computation
is done on a data level - All of these add up to a ~25% speed up
Benchmarked on a MacPro</p>
<h2 id="nontrivial-efficiency-speedups-wrt-non-multiprocessed-filtering">Non/trivial efficiency speedups w.r.t. non-multiprocessed filtering</h2>
<p>All of the speed ups are done relative to the current implementation
in non-multiprocessed filtering.
- Whenever possible we replace a 2D variable like <code>self.x</code> with a
<code>curr_x</code>, which is 1D, because indexing a 1D array is 10-20% faster
than a 2D array. All of these add up to a ~8% speed up
- We only every compute the forward dynamics (not the reverse), because
we can use the forward of the previous timepoint as the reverse for
the next timepoint. This is about 45% faster and adds up to a ~40%
speedup.
- Whenever possible, we replace a dictionary like <code>read_depths</code>
with a float because indexing a dict is 12-20% slower than a 1D
array. All these add up to a ~7% speed up
- We precompute AS MUCH AS POSSIBLE in <code>update</code> and in <code>initialize</code>,
even simple this as <code>self.curr_tidx_minus_1</code>: all of these add up
to about ~5% speedup
- If an attribute of a class is being referenced more than once in
a subroutine, we "get" it by making it a local variable. Example:
<code>tidx = self.tidx</code>. This has about a 5% speed up PER ADDITIONAL
CALL within the subroutine. All of these add up to ~2.5% speed up.
- If an indexed value gets indexed more than once within a subroutine,
we "get" the value by making it a local variable. All of these
add up to ~4% speed up.
- We "get" all of the means and stds of the qPCR data-structures so we
do not reference an object. This is about 22% faster and adds up
to a ~3% speed up.
Benchmarked on a MacPro</p>
<p>Set all local variables to None</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SubjectLogTrajectorySetMP(pl.multiprocessing.PersistentWorker):
    &#39;&#39;&#39;This performs filtering on a multiprocessing level. We send the
    other parameters of the model and return the filtered `x` and values.
    With multiprocessing, this class has ~91% efficiency. Additionally, the code
    in this class is optimized to be ~20X faster than the code in `Filtering`. This
    assumes we have the log model.

    It might seem unneccessary to have so many local attributes, but it speeds up
    the inference considerably if we index a value from an array once and store it
    as a float instead of repeatedly indexing the array - the difference in
    reality is super small but we do this so often that it adds up to ~40% speedup
    as supposed to not doing it - so we do this as often as possible - This speedup
    is even greater for indexing keys of dictionaries and getting parameters of objects.

    General efficiency speedups
    ---------------------------
    All of these are done relative to a non-optimized filtering implementation
        - Specialized sampling and logpdf functions. About 95% faster than
          scipy or numpy functions. All of these add up to a ~35% speed up
        - Explicit function definitions:
          instead of doing `self.prior.logpdf(...)`, we do
          `pl.random.normal.logpdf(...)`, about a ~10% overall speedup
        - Precomputation of values so that the least amout of computation
          is done on a data level - All of these add up to a ~25% speed up
    Benchmarked on a MacPro

    Non/trivial efficiency speedups w.r.t. non-multiprocessed filtering
    -------------------------------------------------------------------
    All of the speed ups are done relative to the current implementation
    in non-multiprocessed filtering.
        - Whenever possible we replace a 2D variable like `self.x` with a
          `curr_x`, which is 1D, because indexing a 1D array is 10-20% faster
          than a 2D array. All of these add up to a ~8% speed up
        - We only every compute the forward dynamics (not the reverse), because
          we can use the forward of the previous timepoint as the reverse for
          the next timepoint. This is about 45% faster and adds up to a ~40%
          speedup.
        - Whenever possible, we replace a dictionary like `read_depths`
          with a float because indexing a dict is 12-20% slower than a 1D
          array. All these add up to a ~7% speed up
        - We precompute AS MUCH AS POSSIBLE in `update` and in `initialize`,
          even simple this as `self.curr_tidx_minus_1`: all of these add up
          to about ~5% speedup
        - If an attribute of a class is being referenced more than once in
          a subroutine, we &#34;get&#34; it by making it a local variable. Example:
          `tidx = self.tidx`. This has about a 5% speed up PER ADDITIONAL
          CALL within the subroutine. All of these add up to ~2.5% speed up.
        - If an indexed value gets indexed more than once within a subroutine,
          we &#34;get&#34; the value by making it a local variable. All of these
          add up to ~4% speed up.
        - We &#34;get&#34; all of the means and stds of the qPCR data-structures so we
          do not reference an object. This is about 22% faster and adds up
          to a ~3% speed up.
    Benchmarked on a MacPro
    &#39;&#39;&#39;
    def __init__(self):
        &#39;&#39;&#39;Set all local variables to None
        &#39;&#39;&#39;
        return

    def initialize(self, times, qpcr_log_measurements, reads, there_are_intermediate_timepoints,
        there_are_perturbations, pv_global, x_prior_mean,
        x_prior_std, tune, delay, end_iter, proposal_init_scale, a0, a1, x, calculate_qpcr_loglik,
        pert_starts, pert_ends, ridx, h5py_xname, target_acceptance_rate,
        zero_inflation_transition_policy):
        &#39;&#39;&#39;Initialize the object at the beginning of the inference

        n_o = Number of ASVs
        n_gT = Number of given time points
        n_T = Total number of time points, including intermediate
        n_P = Number of Perturbations

        Parameters
        ----------
        times : np.array((n_T, ))
            Times for each of the time points
        qpcr_log_measurements : dict(t -&gt; np.ndarray(float))
            These are the qPCR observations for every timepoint in log space.
        reads : dict (float -&gt; np.ndarray((n_o, )))
            The counts for each of the given timepoints. Each value is an
            array for the counts for each of the ASVs
        there_are_intermediate_timepoints : bool
            If True, then there are intermediate timepoints, else there are only
            given timepoints
        there_are_perturbations : bool
            If True, that means there are perturbations, else there are no
            perturbations
        pv_global : bool
            If True, it means the process variance is is global for each ASV. If
            False it means that there is a separate `pv` for each ASV
        pv : float, np.ndarray
            This is the process variance value. This is a float if `pv_global` is True
            and it is an array if `pv_global` is False.
        x_prior_mean, x_prior_std : numeric
            This is the prior mean and std for `x` used when sampling the reverse for
            the first timepoint
        tune : int
            How often we should update the proposal for each ASV
        delay : int
            How many MCMC iterations we should delay the start of updating
        end_iter : int
            What iteration we should stop updating the proposal
        proposal_init_scale : float
            Scale to multiply the initial covariance of the poposal
        a0, a1 : floats
            These are the negative binomial dispersion parameters that specify how
            much noise there is in the counts
        x : np.ndarray((n_o, n_T))
            This is the x initialization
        pert_starts, pert_ends : np.ndarray((n_P, ))
            The starts and ends for each one of the perturbations
        ridx : int
            This is the replicate index that this object corresponds to
        h5py_xname : str
            This is the name for the x in the h5py object
        target_acceptance_rate : float
            This is the target acceptance rate for each point
        calculate_qpcr_loglik : bool
            If True, calculate the loglikelihood of the qPCR measurements during the proposal
        &#39;&#39;&#39;
        self.h5py_xname = h5py_xname
        self.target_acceptance_rate = target_acceptance_rate
        self.zero_inflation_transition_policy = zero_inflation_transition_policy

        self.times = times
        self.qpcr_log_measurements = qpcr_log_measurements
        self.reads = reads
        self.there_are_intermediate_timepoints = there_are_intermediate_timepoints
        self.there_are_perturbations = there_are_perturbations
        self.pv_global = pv_global
        if not pv_global:
            raise TypeError(&#39;Filtering with MP not implemented for non global process variance&#39;)
        self.x_prior_mean = x_prior_mean
        self.x_prior_std = x_prior_std
        self.tune = tune
        self.delay = 0
        self.end_iter = end_iter
        self.proposal_init_scale = proposal_init_scale
        self.a0 = a0
        self.a1 = a1
        self.n_asvs = x.shape[0]
        self.n_timepoints = len(times)
        self.n_timepoints_minus_1 = len(times)-1
        self.logx = np.log(x)
        self.x = x
        self.pert_starts = pert_starts
        self.pert_ends = pert_ends
        self.total_n_points = self.x.shape[0] * self.x.shape[1]
        self.ridx = ridx
        self.calculate_qpcr_loglik = calculate_qpcr_loglik

        self.sample_iter = 0
        self.n_data_points = self.x.shape[0] * self.x.shape[1]

        # latent state
        self.sum_q = np.sum(self.x, axis=0)
        shape = (self.tune, ) + self.x.shape
        self.trace_iter = 0

        # proposal
        self.proposal_std = np.log(1.5) #np.log(3)
        self.acceptances = 0
        self.n_props_total = 0
        self.n_props_local = 0
        self.total_acceptances = 0
        self.add_trace = True

        # Intermediate timepoints
        if self.there_are_intermediate_timepoints:
            self.is_intermediate_timepoint = {}
            self.data_loglik = self.data_loglik_w_intermediates
            for t in self.times:
                self.is_intermediate_timepoint[t] = t not in self.reads
        else:
            self.data_loglik = self.data_loglik_wo_intermediates

        # Reads
        self.read_depths = {}
        for t in self.reads:
            self.read_depths[t] = float(np.sum(self.reads[t]))

        # t
        self.dts = np.zeros(self.n_timepoints_minus_1)
        self.sqrt_dts = np.zeros(self.n_timepoints_minus_1)
        for k in range(self.n_timepoints_minus_1):
            self.dts[k] = self.times[k+1] - self.times[k]
            self.sqrt_dts[k] = np.sqrt(self.dts[k])
        self.t2tidx = {}
        for tidx, t in enumerate(self.times):
            self.t2tidx[t] = tidx

        self.cnt_accepted_times = np.zeros(len(self.times))

        # Perturbations
        # -------------
        # in_pert_transition : np.ndarray(dtype=bool)
        #   This is a bool array where if it is a true it means that the
        #   forward and reverse growth rates are different
        # fully_in_pert : np.ndarray(dtype=int)
        #   This is an int-array where it tells you which perturbation you are fully in
        #   (the forward and reverse growth rates are the same but not the default).
        #   If there is no perturbation then the value is -1. If it is not -1, then the
        #   number corresponds to what perturbation index you are in.
        #
        # Edge cases
        # ----------
        #   * missing data for start
        #       There could be a situation where there was no sample collection
        #       on the day that they started a perturbation. In this case we
        #       assume that the next time point is the `start` of the perturbation.
        #       i.e. the next time point is the perturbation transition.
        #   * missing data for end
        #       There could be a situation where no sample was collected when the
        #       perturbation ended. In this case we assume that the pervious time
        #       point was the end of the perturbation.
        if self.there_are_perturbations:
            self.in_pert_transition = np.zeros(self.n_timepoints, dtype=bool)
            self.fully_in_pert = np.ones(self.n_timepoints, dtype=int) * -1
            for pidx, t in enumerate(self.pert_starts):
                if t == self.times[-1] or t == self.times[0]:
                    raise ValueError(&#39;The code right now does not support either a perturbation that &#39; \
                        &#39;started on the first day or ended on the last day. The code where this is &#39; \
                        &#39;incompatible is when we checking if we are in a perturbation transition&#39;)
                if t &gt; np.max(self.times):
                    continue
                if t not in self.t2tidx:
                    # Use the next time point
                    tidx = np.searchsorted(self.times, t)
                else:
                    tidx = self.t2tidx[t]
                self.in_pert_transition[tidx] = True
            for pidx, t in enumerate(self.pert_ends):
                if t == self.times[-1] or t == self.times[0]:
                    raise ValueError(&#39;The code right now does not support either a perturbation that &#39; \
                        &#39;started on the first day or ended on the last day. The code where this is &#39; \
                        &#39;incompatible is when we checking if we are in a perturbation transition&#39;)
                if t &lt; np.min(self.times) or t &gt; np.max(self.times):
                    continue
                if t not in self.t2tidx:
                    # Use the previous time point
                    tidx = np.searchsorted(self.times, t) - 1
                else:
                    tidx = self.t2tidx[t]
                self.in_pert_transition[tidx] = True

            # check if anything is weird
            if np.sum(self.in_pert_transition) % 2 != 0:
                raise ValueError(&#39;The number of in_pert_transition periods must be even ({})&#39; \
                    &#39;. There is either something wrong with the data (start and end day are &#39; \
                    &#39;the same) or with the algorithm ({})&#39;.format(
                        np.sum(self.in_pert_transition),
                        self.in_pert_transition))

            # Make the fully in perturbation times
            for pidx in range(len(self.pert_ends)):
                try:
                    start_tidx = self.t2tidx[self.pert_starts[pidx]] + 1
                    end_tidx = self.t2tidx[self.pert_ends[pidx]]
                except:
                    # This means there is a missing datapoint at either the
                    # start or end of the perturbation
                    start_t = self.pert_starts[pidx]
                    end_t = self.pert_ends[pidx]
                    start_tidx = np.searchsorted(self.times, start_t)
                    end_tidx = np.searchsorted(self.times, end_t) - 1

                self.fully_in_pert[start_tidx:end_tidx] = pidx
    
    # @profile
    def persistent_run(self, growth, self_interactions, pv, interactions,
        perturbations, qpcr_variances, zero_inflation_data):
        &#39;&#39;&#39;Run an update of the values for a single gibbs step for all of the data points
        in this replicate

        Parameters
        ----------
        growth : np.ndarray((n_asvs, ))
            Growth rates for each ASV
        self_interactions : np.ndarray((n_asvs, ))
            Self-interactions for each ASV
        pv : numeric, np.ndarray
            This is the process variance
        interactions : np.ndarray((n_asvs, n_asvs))
            These are the ASV-ASV interactions
        perturbations : np.ndarray((n_perturbations, n_asvs))
            Perturbation values in the right perturbation order, per ASV
        zero_inflation : np.ndarray
            These are the points that are delibertly pushed down to zero
        qpcr_variances : np.ndarray
            These are the sampled qPCR variances as an array - they are in
            time order

        Returns
        -------
        (int, np.ndarray, float)
            1 This is the replicate index
            2 This is the updated latent state for logx
            3 This is the acceptance rate for this past update.
        &#39;&#39;&#39;
        self.master_growth_rate = growth

        if self.sample_iter &lt; self.delay:
            self.sample_iter += 1
            return self.ridx, self.x, np.nan

        self.update_proposals()
        self.n_accepted_iter = 0
        self.pv = pv
        self.pv_std = np.sqrt(pv)
        self.qpcr_stds = np.sqrt(qpcr_variances[self.ridx])
        self.qpcr_stds_d = {}
        # self.zero_inflation_data = zero_inflation_data[self.ridx]
        self.zero_inflation_data = None

        for tidx,t in enumerate(self.qpcr_log_measurements):
            self.qpcr_stds_d[t] = self.qpcr_stds[tidx]

        if self.there_are_perturbations:
            self.growth_rate_non_pert = growth.ravel()
            self.growth_rate_on_pert = growth.reshape(-1,1) * (1 + perturbations)
                
        # Go through each randomly ASV and go in time order
        oidxs = npr.permutation(self.n_asvs)
        # print(&#39;===============================&#39;)
        # print(&#39;===============================&#39;)
        # print(&#39;ridx&#39;, self.ridx)
        for oidx in oidxs:

            # Set the necessary global parameters
            self.oidx = oidx
            self.curr_x = self.x[oidx, :]
            self.curr_logx = self.logx[oidx, :]
            self.curr_interactions = interactions[oidx, :]
            self.curr_self_interaction = self_interactions[oidx]
            # self.curr_zero_inflation = self.zero_inflation[oidx, :]

            if self.pv_global:
                self.curr_pv_std = self.pv_std
            else:
                self.curr_pv_std = self.pv_std[oidx]

            # Set for first time point
            self.tidx = 0
            self.set_attrs_for_timepoint()
            self.forward_loglik = self.default_forward_loglik
            self.reverse_loglik = self.first_timepoint_reverse
            # Calculate A matrix for forward
            self.forward_interaction_vals = np.nansum(self.x[:, self.tidx] * self.curr_interactions)
            self.update_single()
            self.reverse_loglik = self.default_reverse_loglik
            # Set for middle timepoints
            for tidx in range(1, self.n_timepoints-1):
                # Check if it needs to be zero inflated
                # if not self.curr_zero_inflation[tidx]:
                #     raise NotImplementedError(&#39;Zero inflation not implemented for logmodel&#39;)

                self.tidx = tidx
                self.set_attrs_for_timepoint()

                # Calculate A matrix for forward and reverse
                # Set the reverse of the current time step to the forward of the previous
                self.reverse_interaction_vals = self.forward_interaction_vals #np.sum(self.x[:, self.prev_tidx] * self.curr_interactions)
                self.forward_interaction_vals = np.nansum(self.x[:, self.tidx] * self.curr_interactions)

                # Run single update
                self.update_single()

            # Set for last timepoint
            self.tidx = self.n_timepoints_minus_1
            self.set_attrs_for_timepoint()
            self.forward_loglik = self.last_timepoint_forward
            # Calculate A matrix for reverse
            # Set the reverse of the current time step to the forward of the previous
            self.reverse_interaction_vals = self.forward_interaction_vals # np.sum(self.x[:, self.prev_tidx] * self.curr_interactions)
            self.update_single()

            # if self.sample_iter == 4:
            # sys.exit()

        self.sample_iter += 1
        if self.add_trace:
            self.trace_iter += 1

        # print(self.cnt_accepted_times/self.sample_iter)

        return self.ridx, self.x, self.n_accepted_iter/self.n_data_points

    def set_attrs_for_timepoint(self):
        self.prev_tidx = self.tidx-1
        self.next_tidx = self.tidx+1
        self.forward_growth_rate = self.master_growth_rate[self.oidx]
        self.reverse_growth_rate = self.master_growth_rate[self.oidx]

        if self.there_are_intermediate_timepoints:
            if not self.is_intermediate_timepoint[self.times[self.tidx]]:
                # It is not intermediate timepoints - we need to get the data
                t = self.times[self.tidx]
                self.curr_reads = self.reads[t][self.oidx]
                self.curr_read_depth = self.read_depths[t]
                self.curr_qpcr_log_measurements = self.qpcr_log_measurements[t]
                self.curr_qpcr_std = self.qpcr_stds_d[t]
        else:
            t = self.times[self.tidx]
            self.curr_reads = self.reads[t][self.oidx]
            self.curr_read_depth = self.read_depths[t]
            self.curr_qpcr_log_measurements = self.qpcr_log_measurements[t]
            self.curr_qpcr_std = self.qpcr_stds_d[t]

        # Set perturbation growth rates
        if self.there_are_perturbations:
            if self.in_pert_transition[self.tidx]:
                if self.fully_in_pert[self.tidx-1] != -1:
                    # If the previous time point is in the perturbation, that means
                    # we are going out of the perturbation
                    # self.forward_growth_rate = self.master_growth_rate[self.oidx]
                    pidx = self.fully_in_pert[self.tidx-1]
                    self.reverse_growth_rate = self.growth_rate_on_pert[self.oidx,pidx]
                else:
                    # Else we are going into a perturbation
                    # self.reverse_growth_rate = self.master_growth_rate[self.oidx]
                    pidx = self.fully_in_pert[self.tidx+1]
                    self.forward_growth_rate = self.growth_rate_on_pert[self.oidx,pidx]
            elif self.fully_in_pert[self.tidx] != -1:
                pidx = self.fully_in_pert[self.tidx]
                self.forward_growth_rate = self.growth_rate_on_pert[self.oidx,pidx]
                self.reverse_growth_rate = self.forward_growth_rate

    # @profile
    def update_single(self):
        &#39;&#39;&#39;Update a single oidx, tidx
        &#39;&#39;&#39;
        tidx = self.tidx
        oidx = self.oidx

        # Check if we should update the zero inflation policy
        if self.zero_inflation_transition_policy is not None:
            if self.zero_inflation_transition_policy == &#39;ignore&#39;:
                if not self.zero_inflation_data[oidx,tidx]:
                    self.x[oidx, tidx] = np.nan
                    self.logx[oidx, tidx] = np.nan
                    return
                else:
                    if tidx &lt; self.zero_inflation_data.shape[1]-1:
                        do_forward = self.zero_inflation_data[oidx, tidx+1]
                    else:
                        do_forward = True
                    if tidx &gt; 0:
                        do_reverse = self.zero_inflation_data[oidx, tidx-1]
                    else:
                        do_reverse = True
            else:
                raise NotImplementedError(&#39;Not Implemented&#39;)
        else:
            do_forward = True
            do_reverse = True

        # t = self.times[self.tidx]
        # # proposal
        # mu1 = self.curr_logx[tidx]
        # rel = self.reads[t][oidx]/self.read_depths[t]
        # if rel == 0:
        #     rel = 1e-5
        # mu2 = np.log(rel*np.exp(self.curr_qpcr_loc + (self.curr_qpcr_scale/2)))

        # var1 = self.proposal_std[(tidx, oidx)]**2
        # var2 = (self.curr_qpcr_scale)**2
        # mu,var = prod_gaussians(means=[mu1,mu2], variances=[var1,var2])

        try:
            logx_new = pl.random.misc.fast_sample_normal(
                self.curr_logx[tidx],
                self.proposal_std)
        except:
            print(&#39;mu&#39;, self.curr_logx[tidx])
            print(&#39;std&#39;, self.proposal_std)
            raise
        # try:
        #     logx_new = pl.random.misc.fast_sample_normal(
        #         mu, np.sqrt(var))
        # except:
        #     print(&#39;mu&#39;, mu)
        #     print(&#39;std&#39;, np.sqrt(var))
        #     raise

        x_new = np.exp(logx_new)
        prev_logx_value = self.curr_logx[tidx]
        prev_x_value = self.curr_x[tidx]

        # print(&#39;prex_x&#39;, prev_x_value, np.exp(prev_logx_value))
        # print(&#39;prev_logx&#39;, prev_logx_value)

        # if tidx == 5:
        #     print(&#39;\ntidx&#39;, tidx)
        #     print(&#39;oidx&#39;, oidx)
        #     print(&#39;t&#39;, self.times[self.tidx])
        #     print(&#39;curr_logx&#39;, self.curr_logx[tidx])
        #     # print(&#39;curr_logx&#39;, self.curr_logx[tidx])
        #     # print(&#39;mu1, mu2&#39;, mu1, mu2)
        #     # print(&#39;mu&#39;, mu)
        #     # print(&#39;var1, var2&#39;, var1,var2)
        #     # print(&#39;var&#39;,var)
        #     print(&#39;prop_logx&#39;, logx_new)
        #     # print(&#39;start perts&#39;, self.pert_starts)
        #     # print(&#39;end perts&#39;, self.pert_ends)
        #     # print(&#39;in perturbation transition?&#39;, self.in_pert_transition[tidx])
        #     # print(&#39;fully in pert?&#39;, self.fully_in_pert[self.tidx])
        #     print(&#39;forward growth&#39;, self.forward_growth_rate)

        if do_forward:
            prev_aaa = self.forward_loglik()
        else:
            prev_aaa = 0
        if do_reverse:
            prev_bbb = self.reverse_loglik()
        else:
            prev_bbb = 0
        prev_ddd = self.data_loglik()

        # if tidx == 5:
        #     print(&#39;\nold&#39;)
        #     print(&#39;forward ll&#39;, aaa)
        #     print(&#39;reverse ll&#39;, bbb)
        #     print(&#39;data ll&#39;, ddd)

        l_old = prev_aaa + prev_bbb + prev_ddd

        self.curr_x[tidx] = x_new
        self.curr_logx[tidx] = logx_new
        self.sum_q[tidx] = self.sum_q[tidx] - prev_x_value + x_new

        if do_forward:
            new_aaa = self.forward_loglik()
        else:
            new_aaa = 0
        if do_reverse:
            new_bbb = self.reverse_loglik()
        else:
            new_bbb = 0
        new_ddd = self.data_loglik()

        # if tidx == 5:
        #     print(&#39;\nnew&#39;)
        #     print(&#39;forward ll&#39;, aaa)
        #     print(&#39;reverse ll&#39;, bbb)
        #     print(&#39;data ll&#39;, ddd)
        #     print(&#39;\nold x value&#39;, prev_x_value)
        #     print(&#39;old logx value&#39;, prev_logx_value)
        #     print(&#39;proposal std&#39;, self.proposal_std[(tidx, oidx)])
        #     print(&#39;new x value&#39;, x_new)
        #     print(&#39;new logx value&#39;, logx_new)

        l_new = new_aaa + new_bbb + new_ddd
        r_accept = l_new - l_old

        # if tidx == 0:
        #     print(&#39;\n\noidx {} diff lls:&#39;.format(oidx), r_accept)
        #     print(&#39;\tforward&#39;, new_aaa - prev_aaa)
        #     print(&#39;\treverse&#39;, new_bbb - prev_bbb)
        #     print(&#39;\tdata&#39;, new_ddd - prev_ddd)

        # if tidx == 5:
        #     print(&#39;r_accept&#39;, r_accept)
        r = pl.random.misc.fast_sample_standard_uniform()
        if math.log(r) &gt; r_accept:
            # print(&#39;reject&#39;)
            # reject
            self.sum_q[tidx] = self.sum_q[tidx] + prev_x_value - x_new
            self.curr_x[tidx] = prev_x_value
            self.curr_logx[tidx] = prev_logx_value
        else:
            # print(&#39;accept&#39;)
            self.x[oidx, tidx] = x_new
            self.logx[oidx, tidx] = logx_new
            self.acceptances += 1
            self.total_acceptances += 1
            self.n_accepted_iter += 1

        self.n_props_local += 1
        self.n_props_total += 1

    def update_proposals(self):
        &#39;&#39;&#39;Update the proposal if necessary
        &#39;&#39;&#39;
        if self.sample_iter &gt; self.end_iter:
            self.add_trace = False
            return
        if self.sample_iter == 0:
            return
        if self.trace_iter - self.delay == self.tune  and self.sample_iter - self.delay &gt; 0:

            # Adjust
            acc_rate = self.acceptances/self.n_props_total
            if acc_rate &lt; 0.1:
                logging.debug(&#39;Very low acceptance rate, scaling down past covariance&#39;)
                self.proposal_std *= 0.01
            elif acc_rate &lt; self.target_acceptance_rate:
                self.proposal_std /= np.sqrt(1.5)
            else:
                self.proposal_std *= np.sqrt(1.5)
            
            self.acceptances = 0
            self.n_props_local = 0

    def last_timepoint_forward(self):
        return 0

    # @profile
    def default_forward_loglik(self):
        &#39;&#39;&#39;From the current timepoint (tidx) to the next timepoint (tidx+1)
        &#39;&#39;&#39;
        logmu = self.compute_dynamics(
            tidx=self.tidx,
            Axj=self.forward_interaction_vals,
            a1=self.forward_growth_rate)

        try:
            return pl.random.normal.logpdf(
                value=self.curr_logx[self.next_tidx], 
                mean=logmu, std=self.curr_pv_std*self.sqrt_dts[self.tidx])
        except:
            return _normal_logpdf(
                value=self.curr_logx[self.next_tidx], 
                mean=logmu, std=self.curr_pv_std*self.sqrt_dts[self.tidx])

    def first_timepoint_reverse(self):
        # sample from the prior
        try:
            return pl.random.normal.logpdf(value=self.curr_logx[self.tidx],
                mean=self.x_prior_mean, std=self.x_prior_std)
        except:
            return _normal_logpdf(value=self.curr_logx[self.tidx],
                mean=self.x_prior_mean, std=self.x_prior_std)

    # @profile
    def default_reverse_loglik(self):
        &#39;&#39;&#39;From the previous timepoint (tidx-1) to the current time point (tidx)
        &#39;&#39;&#39;
        logmu = self.compute_dynamics(
            tidx=self.prev_tidx,
            Axj=self.reverse_interaction_vals,
            a1=self.reverse_growth_rate)

        try:
            return pl.random.normal.logpdf(value=self.curr_logx[self.tidx], 
                mean=logmu, std=self.curr_pv_std*self.sqrt_dts[self.prev_tidx])
        except:
            return _normal_logpdf(value=self.curr_logx[self.tidx], 
                mean=logmu, std=self.curr_pv_std*self.sqrt_dts[self.prev_tidx])

    def data_loglik_w_intermediates(self):
        &#39;&#39;&#39;data loglikelihood w/ intermediate timepoints
        &#39;&#39;&#39;
        if self.is_intermediate_timepoint[self.times[self.tidx]]:
            return 0
        else:
            return self.data_loglik_wo_intermediates()

    # @profile
    def data_loglik_wo_intermediates(self):
        &#39;&#39;&#39;data loglikelihood with intermediate timepoints
        &#39;&#39;&#39;
        sum_q = self.sum_q[self.tidx]
        log_sum_q = math.log(sum_q)
        rel = self.curr_x[self.tidx] / sum_q

        try:
            negbin = negbin_loglikelihood_MH_condensed(
                k=self.curr_reads,
                m=self.curr_read_depth * rel,
                dispersion=self.a0/rel + self.a1)
        except:
            negbin = negbin_loglikelihood_MH_condensed_not_fast(
                k=self.curr_reads,
                m=self.curr_read_depth * rel,
                dispersion=self.a0/rel + self.a1)

        qpcr = 0
        if self.calculate_qpcr_loglik:
            for qpcr_val in self.curr_qpcr_log_measurements:
                a = pl.random.normal.logpdf(value=qpcr_val, mean=log_sum_q, std=self.curr_qpcr_std)
                qpcr += a

        # tidx = self.tidx
        # if True: #tidx in [3,6,7]:
        #     print(&#39;\n\nData, tidx&#39;, tidx)
        #     print(&#39;sum_q:&#39;, sum_q)
        #     print(&#39;rel:&#39;, rel)
        #     print(&#39;qpcr: {}\n\tvalue: {}\n\tmean: {}\n\tstd: {}&#39;.format(
        #         qpcr, self.curr_qpcr_loc,
        #         sum_q,
        #         self.curr_qpcr_scale))
        #     print(&#39;neg_bin: {}\n\tk: {}\n\tm: {}\n\tdispersion: {}&#39;.format( 
        #         negbin, self.curr_reads, self.curr_read_depth * rel,
        #         self.a0/rel + self.a1))
            
        #     print(&#39;data\n\tcurr_x: {}, {}\n\tcurr_logx: {}&#39;.format(
        #         self.curr_x[self.tidx], 
        #         np.exp(self.curr_logx[self.tidx]),
        #         self.curr_logx[self.tidx]))
        return negbin + qpcr

    def compute_dynamics(self, tidx, Axj, a1):
        &#39;&#39;&#39;Compute dynamics going into tidx+1

        a1 : growth rates and perturbations (if necessary)
        Axj : cluster interactions with the other abundances already multiplied
        tidx : time index

        Zero-inflation
        --------------
        When we get here, the current asv at `tidx` is not a structural zero, but 
        there might be other bugs in the system that do have a structural zero there.
        Thus we do nan adds
        &#39;&#39;&#39;
        logxi = self.curr_logx[tidx]
        xi = self.curr_x[tidx]

        # print(&#39;dynamics&#39;)
        # print(&#39;xi*a1&#39;, xi*a1* self.dts[tidx])
        # print(&#39;xi*xi*self.curr_self_interaction&#39;, xi*xi*self.curr_self_interaction* self.dts[tidx])
        # print(&#39;xi*Axj&#39;, xi*Axj* self.dts[tidx])

        # compute dynamics
        return logxi + (a1 - xi*self.curr_self_interaction + Axj) * self.dts[tidx]</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mdsine2.pylab.multiprocessing.PersistentWorker" href="pylab/multiprocessing.html#mdsine2.pylab.multiprocessing.PersistentWorker">PersistentWorker</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="mdsine2.filtering.SubjectLogTrajectorySetMP.compute_dynamics"><code class="name flex">
<span>def <span class="ident">compute_dynamics</span></span>(<span>self, tidx, Axj, a1)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute dynamics going into tidx+1</p>
<p>a1 : growth rates and perturbations (if necessary)
Axj : cluster interactions with the other abundances already multiplied
tidx : time index</p>
<h2 id="zero-inflation">Zero-inflation</h2>
<p>When we get here, the current asv at <code>tidx</code> is not a structural zero, but
there might be other bugs in the system that do have a structural zero there.
Thus we do nan adds</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_dynamics(self, tidx, Axj, a1):
    &#39;&#39;&#39;Compute dynamics going into tidx+1

    a1 : growth rates and perturbations (if necessary)
    Axj : cluster interactions with the other abundances already multiplied
    tidx : time index

    Zero-inflation
    --------------
    When we get here, the current asv at `tidx` is not a structural zero, but 
    there might be other bugs in the system that do have a structural zero there.
    Thus we do nan adds
    &#39;&#39;&#39;
    logxi = self.curr_logx[tidx]
    xi = self.curr_x[tidx]

    # print(&#39;dynamics&#39;)
    # print(&#39;xi*a1&#39;, xi*a1* self.dts[tidx])
    # print(&#39;xi*xi*self.curr_self_interaction&#39;, xi*xi*self.curr_self_interaction* self.dts[tidx])
    # print(&#39;xi*Axj&#39;, xi*Axj* self.dts[tidx])

    # compute dynamics
    return logxi + (a1 - xi*self.curr_self_interaction + Axj) * self.dts[tidx]</code></pre>
</details>
</dd>
<dt id="mdsine2.filtering.SubjectLogTrajectorySetMP.data_loglik_w_intermediates"><code class="name flex">
<span>def <span class="ident">data_loglik_w_intermediates</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>data loglikelihood w/ intermediate timepoints</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def data_loglik_w_intermediates(self):
    &#39;&#39;&#39;data loglikelihood w/ intermediate timepoints
    &#39;&#39;&#39;
    if self.is_intermediate_timepoint[self.times[self.tidx]]:
        return 0
    else:
        return self.data_loglik_wo_intermediates()</code></pre>
</details>
</dd>
<dt id="mdsine2.filtering.SubjectLogTrajectorySetMP.data_loglik_wo_intermediates"><code class="name flex">
<span>def <span class="ident">data_loglik_wo_intermediates</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>data loglikelihood with intermediate timepoints</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def data_loglik_wo_intermediates(self):
    &#39;&#39;&#39;data loglikelihood with intermediate timepoints
    &#39;&#39;&#39;
    sum_q = self.sum_q[self.tidx]
    log_sum_q = math.log(sum_q)
    rel = self.curr_x[self.tidx] / sum_q

    try:
        negbin = negbin_loglikelihood_MH_condensed(
            k=self.curr_reads,
            m=self.curr_read_depth * rel,
            dispersion=self.a0/rel + self.a1)
    except:
        negbin = negbin_loglikelihood_MH_condensed_not_fast(
            k=self.curr_reads,
            m=self.curr_read_depth * rel,
            dispersion=self.a0/rel + self.a1)

    qpcr = 0
    if self.calculate_qpcr_loglik:
        for qpcr_val in self.curr_qpcr_log_measurements:
            a = pl.random.normal.logpdf(value=qpcr_val, mean=log_sum_q, std=self.curr_qpcr_std)
            qpcr += a

    # tidx = self.tidx
    # if True: #tidx in [3,6,7]:
    #     print(&#39;\n\nData, tidx&#39;, tidx)
    #     print(&#39;sum_q:&#39;, sum_q)
    #     print(&#39;rel:&#39;, rel)
    #     print(&#39;qpcr: {}\n\tvalue: {}\n\tmean: {}\n\tstd: {}&#39;.format(
    #         qpcr, self.curr_qpcr_loc,
    #         sum_q,
    #         self.curr_qpcr_scale))
    #     print(&#39;neg_bin: {}\n\tk: {}\n\tm: {}\n\tdispersion: {}&#39;.format( 
    #         negbin, self.curr_reads, self.curr_read_depth * rel,
    #         self.a0/rel + self.a1))
        
    #     print(&#39;data\n\tcurr_x: {}, {}\n\tcurr_logx: {}&#39;.format(
    #         self.curr_x[self.tidx], 
    #         np.exp(self.curr_logx[self.tidx]),
    #         self.curr_logx[self.tidx]))
    return negbin + qpcr</code></pre>
</details>
</dd>
<dt id="mdsine2.filtering.SubjectLogTrajectorySetMP.default_forward_loglik"><code class="name flex">
<span>def <span class="ident">default_forward_loglik</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>From the current timepoint (tidx) to the next timepoint (tidx+1)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def default_forward_loglik(self):
    &#39;&#39;&#39;From the current timepoint (tidx) to the next timepoint (tidx+1)
    &#39;&#39;&#39;
    logmu = self.compute_dynamics(
        tidx=self.tidx,
        Axj=self.forward_interaction_vals,
        a1=self.forward_growth_rate)

    try:
        return pl.random.normal.logpdf(
            value=self.curr_logx[self.next_tidx], 
            mean=logmu, std=self.curr_pv_std*self.sqrt_dts[self.tidx])
    except:
        return _normal_logpdf(
            value=self.curr_logx[self.next_tidx], 
            mean=logmu, std=self.curr_pv_std*self.sqrt_dts[self.tidx])</code></pre>
</details>
</dd>
<dt id="mdsine2.filtering.SubjectLogTrajectorySetMP.default_reverse_loglik"><code class="name flex">
<span>def <span class="ident">default_reverse_loglik</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>From the previous timepoint (tidx-1) to the current time point (tidx)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def default_reverse_loglik(self):
    &#39;&#39;&#39;From the previous timepoint (tidx-1) to the current time point (tidx)
    &#39;&#39;&#39;
    logmu = self.compute_dynamics(
        tidx=self.prev_tidx,
        Axj=self.reverse_interaction_vals,
        a1=self.reverse_growth_rate)

    try:
        return pl.random.normal.logpdf(value=self.curr_logx[self.tidx], 
            mean=logmu, std=self.curr_pv_std*self.sqrt_dts[self.prev_tidx])
    except:
        return _normal_logpdf(value=self.curr_logx[self.tidx], 
            mean=logmu, std=self.curr_pv_std*self.sqrt_dts[self.prev_tidx])</code></pre>
</details>
</dd>
<dt id="mdsine2.filtering.SubjectLogTrajectorySetMP.first_timepoint_reverse"><code class="name flex">
<span>def <span class="ident">first_timepoint_reverse</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def first_timepoint_reverse(self):
    # sample from the prior
    try:
        return pl.random.normal.logpdf(value=self.curr_logx[self.tidx],
            mean=self.x_prior_mean, std=self.x_prior_std)
    except:
        return _normal_logpdf(value=self.curr_logx[self.tidx],
            mean=self.x_prior_mean, std=self.x_prior_std)</code></pre>
</details>
</dd>
<dt id="mdsine2.filtering.SubjectLogTrajectorySetMP.initialize"><code class="name flex">
<span>def <span class="ident">initialize</span></span>(<span>self, times, qpcr_log_measurements, reads, there_are_intermediate_timepoints, there_are_perturbations, pv_global, x_prior_mean, x_prior_std, tune, delay, end_iter, proposal_init_scale, a0, a1, x, calculate_qpcr_loglik, pert_starts, pert_ends, ridx, h5py_xname, target_acceptance_rate, zero_inflation_transition_policy)</span>
</code></dt>
<dd>
<div class="desc"><p>Initialize the object at the beginning of the inference</p>
<p>n_o = Number of ASVs
n_gT = Number of given time points
n_T = Total number of time points, including intermediate
n_P = Number of Perturbations</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>times</code></strong> :&ensp;<code>np.array((n_T, ))</code></dt>
<dd>Times for each of the time points</dd>
<dt><strong><code>qpcr_log_measurements</code></strong> :&ensp;<code>dict(t -&gt; np.ndarray(float))</code></dt>
<dd>These are the qPCR observations for every timepoint in log space.</dd>
<dt><strong><code>reads</code></strong> :&ensp;<code>dict (float -&gt; np.ndarray((n_o, )))</code></dt>
<dd>The counts for each of the given timepoints. Each value is an
array for the counts for each of the ASVs</dd>
<dt><strong><code>there_are_intermediate_timepoints</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, then there are intermediate timepoints, else there are only
given timepoints</dd>
<dt><strong><code>there_are_perturbations</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, that means there are perturbations, else there are no
perturbations</dd>
<dt><strong><code>pv_global</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, it means the process variance is is global for each ASV. If
False it means that there is a separate <code>pv</code> for each ASV</dd>
<dt><strong><code>pv</code></strong> :&ensp;<code>float, np.ndarray</code></dt>
<dd>This is the process variance value. This is a float if <code>pv_global</code> is True
and it is an array if <code>pv_global</code> is False.</dd>
<dt><strong><code>x_prior_mean</code></strong>, <strong><code>x_prior_std</code></strong> :&ensp;<code>numeric</code></dt>
<dd>This is the prior mean and std for <code>x</code> used when sampling the reverse for
the first timepoint</dd>
<dt><strong><code>tune</code></strong> :&ensp;<code>int</code></dt>
<dd>How often we should update the proposal for each ASV</dd>
<dt><strong><code>delay</code></strong> :&ensp;<code>int</code></dt>
<dd>How many MCMC iterations we should delay the start of updating</dd>
<dt><strong><code>end_iter</code></strong> :&ensp;<code>int</code></dt>
<dd>What iteration we should stop updating the proposal</dd>
<dt><strong><code>proposal_init_scale</code></strong> :&ensp;<code>float</code></dt>
<dd>Scale to multiply the initial covariance of the poposal</dd>
<dt><strong><code>a0</code></strong>, <strong><code>a1</code></strong> :&ensp;<code>floats</code></dt>
<dd>These are the negative binomial dispersion parameters that specify how
much noise there is in the counts</dd>
<dt><strong><code>x</code></strong> :&ensp;<code>np.ndarray((n_o, n_T))</code></dt>
<dd>This is the x initialization</dd>
<dt><strong><code>pert_starts</code></strong>, <strong><code>pert_ends</code></strong> :&ensp;<code>np.ndarray((n_P, ))</code></dt>
<dd>The starts and ends for each one of the perturbations</dd>
<dt><strong><code>ridx</code></strong> :&ensp;<code>int</code></dt>
<dd>This is the replicate index that this object corresponds to</dd>
<dt><strong><code>h5py_xname</code></strong> :&ensp;<code>str</code></dt>
<dd>This is the name for the x in the h5py object</dd>
<dt><strong><code>target_acceptance_rate</code></strong> :&ensp;<code>float</code></dt>
<dd>This is the target acceptance rate for each point</dd>
<dt><strong><code>calculate_qpcr_loglik</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, calculate the loglikelihood of the qPCR measurements during the proposal</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def initialize(self, times, qpcr_log_measurements, reads, there_are_intermediate_timepoints,
    there_are_perturbations, pv_global, x_prior_mean,
    x_prior_std, tune, delay, end_iter, proposal_init_scale, a0, a1, x, calculate_qpcr_loglik,
    pert_starts, pert_ends, ridx, h5py_xname, target_acceptance_rate,
    zero_inflation_transition_policy):
    &#39;&#39;&#39;Initialize the object at the beginning of the inference

    n_o = Number of ASVs
    n_gT = Number of given time points
    n_T = Total number of time points, including intermediate
    n_P = Number of Perturbations

    Parameters
    ----------
    times : np.array((n_T, ))
        Times for each of the time points
    qpcr_log_measurements : dict(t -&gt; np.ndarray(float))
        These are the qPCR observations for every timepoint in log space.
    reads : dict (float -&gt; np.ndarray((n_o, )))
        The counts for each of the given timepoints. Each value is an
        array for the counts for each of the ASVs
    there_are_intermediate_timepoints : bool
        If True, then there are intermediate timepoints, else there are only
        given timepoints
    there_are_perturbations : bool
        If True, that means there are perturbations, else there are no
        perturbations
    pv_global : bool
        If True, it means the process variance is is global for each ASV. If
        False it means that there is a separate `pv` for each ASV
    pv : float, np.ndarray
        This is the process variance value. This is a float if `pv_global` is True
        and it is an array if `pv_global` is False.
    x_prior_mean, x_prior_std : numeric
        This is the prior mean and std for `x` used when sampling the reverse for
        the first timepoint
    tune : int
        How often we should update the proposal for each ASV
    delay : int
        How many MCMC iterations we should delay the start of updating
    end_iter : int
        What iteration we should stop updating the proposal
    proposal_init_scale : float
        Scale to multiply the initial covariance of the poposal
    a0, a1 : floats
        These are the negative binomial dispersion parameters that specify how
        much noise there is in the counts
    x : np.ndarray((n_o, n_T))
        This is the x initialization
    pert_starts, pert_ends : np.ndarray((n_P, ))
        The starts and ends for each one of the perturbations
    ridx : int
        This is the replicate index that this object corresponds to
    h5py_xname : str
        This is the name for the x in the h5py object
    target_acceptance_rate : float
        This is the target acceptance rate for each point
    calculate_qpcr_loglik : bool
        If True, calculate the loglikelihood of the qPCR measurements during the proposal
    &#39;&#39;&#39;
    self.h5py_xname = h5py_xname
    self.target_acceptance_rate = target_acceptance_rate
    self.zero_inflation_transition_policy = zero_inflation_transition_policy

    self.times = times
    self.qpcr_log_measurements = qpcr_log_measurements
    self.reads = reads
    self.there_are_intermediate_timepoints = there_are_intermediate_timepoints
    self.there_are_perturbations = there_are_perturbations
    self.pv_global = pv_global
    if not pv_global:
        raise TypeError(&#39;Filtering with MP not implemented for non global process variance&#39;)
    self.x_prior_mean = x_prior_mean
    self.x_prior_std = x_prior_std
    self.tune = tune
    self.delay = 0
    self.end_iter = end_iter
    self.proposal_init_scale = proposal_init_scale
    self.a0 = a0
    self.a1 = a1
    self.n_asvs = x.shape[0]
    self.n_timepoints = len(times)
    self.n_timepoints_minus_1 = len(times)-1
    self.logx = np.log(x)
    self.x = x
    self.pert_starts = pert_starts
    self.pert_ends = pert_ends
    self.total_n_points = self.x.shape[0] * self.x.shape[1]
    self.ridx = ridx
    self.calculate_qpcr_loglik = calculate_qpcr_loglik

    self.sample_iter = 0
    self.n_data_points = self.x.shape[0] * self.x.shape[1]

    # latent state
    self.sum_q = np.sum(self.x, axis=0)
    shape = (self.tune, ) + self.x.shape
    self.trace_iter = 0

    # proposal
    self.proposal_std = np.log(1.5) #np.log(3)
    self.acceptances = 0
    self.n_props_total = 0
    self.n_props_local = 0
    self.total_acceptances = 0
    self.add_trace = True

    # Intermediate timepoints
    if self.there_are_intermediate_timepoints:
        self.is_intermediate_timepoint = {}
        self.data_loglik = self.data_loglik_w_intermediates
        for t in self.times:
            self.is_intermediate_timepoint[t] = t not in self.reads
    else:
        self.data_loglik = self.data_loglik_wo_intermediates

    # Reads
    self.read_depths = {}
    for t in self.reads:
        self.read_depths[t] = float(np.sum(self.reads[t]))

    # t
    self.dts = np.zeros(self.n_timepoints_minus_1)
    self.sqrt_dts = np.zeros(self.n_timepoints_minus_1)
    for k in range(self.n_timepoints_minus_1):
        self.dts[k] = self.times[k+1] - self.times[k]
        self.sqrt_dts[k] = np.sqrt(self.dts[k])
    self.t2tidx = {}
    for tidx, t in enumerate(self.times):
        self.t2tidx[t] = tidx

    self.cnt_accepted_times = np.zeros(len(self.times))

    # Perturbations
    # -------------
    # in_pert_transition : np.ndarray(dtype=bool)
    #   This is a bool array where if it is a true it means that the
    #   forward and reverse growth rates are different
    # fully_in_pert : np.ndarray(dtype=int)
    #   This is an int-array where it tells you which perturbation you are fully in
    #   (the forward and reverse growth rates are the same but not the default).
    #   If there is no perturbation then the value is -1. If it is not -1, then the
    #   number corresponds to what perturbation index you are in.
    #
    # Edge cases
    # ----------
    #   * missing data for start
    #       There could be a situation where there was no sample collection
    #       on the day that they started a perturbation. In this case we
    #       assume that the next time point is the `start` of the perturbation.
    #       i.e. the next time point is the perturbation transition.
    #   * missing data for end
    #       There could be a situation where no sample was collected when the
    #       perturbation ended. In this case we assume that the pervious time
    #       point was the end of the perturbation.
    if self.there_are_perturbations:
        self.in_pert_transition = np.zeros(self.n_timepoints, dtype=bool)
        self.fully_in_pert = np.ones(self.n_timepoints, dtype=int) * -1
        for pidx, t in enumerate(self.pert_starts):
            if t == self.times[-1] or t == self.times[0]:
                raise ValueError(&#39;The code right now does not support either a perturbation that &#39; \
                    &#39;started on the first day or ended on the last day. The code where this is &#39; \
                    &#39;incompatible is when we checking if we are in a perturbation transition&#39;)
            if t &gt; np.max(self.times):
                continue
            if t not in self.t2tidx:
                # Use the next time point
                tidx = np.searchsorted(self.times, t)
            else:
                tidx = self.t2tidx[t]
            self.in_pert_transition[tidx] = True
        for pidx, t in enumerate(self.pert_ends):
            if t == self.times[-1] or t == self.times[0]:
                raise ValueError(&#39;The code right now does not support either a perturbation that &#39; \
                    &#39;started on the first day or ended on the last day. The code where this is &#39; \
                    &#39;incompatible is when we checking if we are in a perturbation transition&#39;)
            if t &lt; np.min(self.times) or t &gt; np.max(self.times):
                continue
            if t not in self.t2tidx:
                # Use the previous time point
                tidx = np.searchsorted(self.times, t) - 1
            else:
                tidx = self.t2tidx[t]
            self.in_pert_transition[tidx] = True

        # check if anything is weird
        if np.sum(self.in_pert_transition) % 2 != 0:
            raise ValueError(&#39;The number of in_pert_transition periods must be even ({})&#39; \
                &#39;. There is either something wrong with the data (start and end day are &#39; \
                &#39;the same) or with the algorithm ({})&#39;.format(
                    np.sum(self.in_pert_transition),
                    self.in_pert_transition))

        # Make the fully in perturbation times
        for pidx in range(len(self.pert_ends)):
            try:
                start_tidx = self.t2tidx[self.pert_starts[pidx]] + 1
                end_tidx = self.t2tidx[self.pert_ends[pidx]]
            except:
                # This means there is a missing datapoint at either the
                # start or end of the perturbation
                start_t = self.pert_starts[pidx]
                end_t = self.pert_ends[pidx]
                start_tidx = np.searchsorted(self.times, start_t)
                end_tidx = np.searchsorted(self.times, end_t) - 1

            self.fully_in_pert[start_tidx:end_tidx] = pidx</code></pre>
</details>
</dd>
<dt id="mdsine2.filtering.SubjectLogTrajectorySetMP.last_timepoint_forward"><code class="name flex">
<span>def <span class="ident">last_timepoint_forward</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def last_timepoint_forward(self):
    return 0</code></pre>
</details>
</dd>
<dt id="mdsine2.filtering.SubjectLogTrajectorySetMP.persistent_run"><code class="name flex">
<span>def <span class="ident">persistent_run</span></span>(<span>self, growth, self_interactions, pv, interactions, perturbations, qpcr_variances, zero_inflation_data)</span>
</code></dt>
<dd>
<div class="desc"><p>Run an update of the values for a single gibbs step for all of the data points
in this replicate</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>growth</code></strong> :&ensp;<code>np.ndarray((n_asvs, ))</code></dt>
<dd>Growth rates for each ASV</dd>
<dt><strong><code>self_interactions</code></strong> :&ensp;<code>np.ndarray((n_asvs, ))</code></dt>
<dd>Self-interactions for each ASV</dd>
<dt><strong><code>pv</code></strong> :&ensp;<code>numeric, np.ndarray</code></dt>
<dd>This is the process variance</dd>
<dt><strong><code>interactions</code></strong> :&ensp;<code>np.ndarray((n_asvs, n_asvs))</code></dt>
<dd>These are the ASV-ASV interactions</dd>
<dt><strong><code>perturbations</code></strong> :&ensp;<code>np.ndarray((n_perturbations, n_asvs))</code></dt>
<dd>Perturbation values in the right perturbation order, per ASV</dd>
<dt><strong><code>zero_inflation</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>These are the points that are delibertly pushed down to zero</dd>
<dt><strong><code>qpcr_variances</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>These are the sampled qPCR variances as an array - they are in
time order</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>(int, np.ndarray, float)
1 This is the replicate index
2 This is the updated latent state for logx
3 This is the acceptance rate for this past update.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def persistent_run(self, growth, self_interactions, pv, interactions,
    perturbations, qpcr_variances, zero_inflation_data):
    &#39;&#39;&#39;Run an update of the values for a single gibbs step for all of the data points
    in this replicate

    Parameters
    ----------
    growth : np.ndarray((n_asvs, ))
        Growth rates for each ASV
    self_interactions : np.ndarray((n_asvs, ))
        Self-interactions for each ASV
    pv : numeric, np.ndarray
        This is the process variance
    interactions : np.ndarray((n_asvs, n_asvs))
        These are the ASV-ASV interactions
    perturbations : np.ndarray((n_perturbations, n_asvs))
        Perturbation values in the right perturbation order, per ASV
    zero_inflation : np.ndarray
        These are the points that are delibertly pushed down to zero
    qpcr_variances : np.ndarray
        These are the sampled qPCR variances as an array - they are in
        time order

    Returns
    -------
    (int, np.ndarray, float)
        1 This is the replicate index
        2 This is the updated latent state for logx
        3 This is the acceptance rate for this past update.
    &#39;&#39;&#39;
    self.master_growth_rate = growth

    if self.sample_iter &lt; self.delay:
        self.sample_iter += 1
        return self.ridx, self.x, np.nan

    self.update_proposals()
    self.n_accepted_iter = 0
    self.pv = pv
    self.pv_std = np.sqrt(pv)
    self.qpcr_stds = np.sqrt(qpcr_variances[self.ridx])
    self.qpcr_stds_d = {}
    # self.zero_inflation_data = zero_inflation_data[self.ridx]
    self.zero_inflation_data = None

    for tidx,t in enumerate(self.qpcr_log_measurements):
        self.qpcr_stds_d[t] = self.qpcr_stds[tidx]

    if self.there_are_perturbations:
        self.growth_rate_non_pert = growth.ravel()
        self.growth_rate_on_pert = growth.reshape(-1,1) * (1 + perturbations)
            
    # Go through each randomly ASV and go in time order
    oidxs = npr.permutation(self.n_asvs)
    # print(&#39;===============================&#39;)
    # print(&#39;===============================&#39;)
    # print(&#39;ridx&#39;, self.ridx)
    for oidx in oidxs:

        # Set the necessary global parameters
        self.oidx = oidx
        self.curr_x = self.x[oidx, :]
        self.curr_logx = self.logx[oidx, :]
        self.curr_interactions = interactions[oidx, :]
        self.curr_self_interaction = self_interactions[oidx]
        # self.curr_zero_inflation = self.zero_inflation[oidx, :]

        if self.pv_global:
            self.curr_pv_std = self.pv_std
        else:
            self.curr_pv_std = self.pv_std[oidx]

        # Set for first time point
        self.tidx = 0
        self.set_attrs_for_timepoint()
        self.forward_loglik = self.default_forward_loglik
        self.reverse_loglik = self.first_timepoint_reverse
        # Calculate A matrix for forward
        self.forward_interaction_vals = np.nansum(self.x[:, self.tidx] * self.curr_interactions)
        self.update_single()
        self.reverse_loglik = self.default_reverse_loglik
        # Set for middle timepoints
        for tidx in range(1, self.n_timepoints-1):
            # Check if it needs to be zero inflated
            # if not self.curr_zero_inflation[tidx]:
            #     raise NotImplementedError(&#39;Zero inflation not implemented for logmodel&#39;)

            self.tidx = tidx
            self.set_attrs_for_timepoint()

            # Calculate A matrix for forward and reverse
            # Set the reverse of the current time step to the forward of the previous
            self.reverse_interaction_vals = self.forward_interaction_vals #np.sum(self.x[:, self.prev_tidx] * self.curr_interactions)
            self.forward_interaction_vals = np.nansum(self.x[:, self.tidx] * self.curr_interactions)

            # Run single update
            self.update_single()

        # Set for last timepoint
        self.tidx = self.n_timepoints_minus_1
        self.set_attrs_for_timepoint()
        self.forward_loglik = self.last_timepoint_forward
        # Calculate A matrix for reverse
        # Set the reverse of the current time step to the forward of the previous
        self.reverse_interaction_vals = self.forward_interaction_vals # np.sum(self.x[:, self.prev_tidx] * self.curr_interactions)
        self.update_single()

        # if self.sample_iter == 4:
        # sys.exit()

    self.sample_iter += 1
    if self.add_trace:
        self.trace_iter += 1

    # print(self.cnt_accepted_times/self.sample_iter)

    return self.ridx, self.x, self.n_accepted_iter/self.n_data_points</code></pre>
</details>
</dd>
<dt id="mdsine2.filtering.SubjectLogTrajectorySetMP.set_attrs_for_timepoint"><code class="name flex">
<span>def <span class="ident">set_attrs_for_timepoint</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_attrs_for_timepoint(self):
    self.prev_tidx = self.tidx-1
    self.next_tidx = self.tidx+1
    self.forward_growth_rate = self.master_growth_rate[self.oidx]
    self.reverse_growth_rate = self.master_growth_rate[self.oidx]

    if self.there_are_intermediate_timepoints:
        if not self.is_intermediate_timepoint[self.times[self.tidx]]:
            # It is not intermediate timepoints - we need to get the data
            t = self.times[self.tidx]
            self.curr_reads = self.reads[t][self.oidx]
            self.curr_read_depth = self.read_depths[t]
            self.curr_qpcr_log_measurements = self.qpcr_log_measurements[t]
            self.curr_qpcr_std = self.qpcr_stds_d[t]
    else:
        t = self.times[self.tidx]
        self.curr_reads = self.reads[t][self.oidx]
        self.curr_read_depth = self.read_depths[t]
        self.curr_qpcr_log_measurements = self.qpcr_log_measurements[t]
        self.curr_qpcr_std = self.qpcr_stds_d[t]

    # Set perturbation growth rates
    if self.there_are_perturbations:
        if self.in_pert_transition[self.tidx]:
            if self.fully_in_pert[self.tidx-1] != -1:
                # If the previous time point is in the perturbation, that means
                # we are going out of the perturbation
                # self.forward_growth_rate = self.master_growth_rate[self.oidx]
                pidx = self.fully_in_pert[self.tidx-1]
                self.reverse_growth_rate = self.growth_rate_on_pert[self.oidx,pidx]
            else:
                # Else we are going into a perturbation
                # self.reverse_growth_rate = self.master_growth_rate[self.oidx]
                pidx = self.fully_in_pert[self.tidx+1]
                self.forward_growth_rate = self.growth_rate_on_pert[self.oidx,pidx]
        elif self.fully_in_pert[self.tidx] != -1:
            pidx = self.fully_in_pert[self.tidx]
            self.forward_growth_rate = self.growth_rate_on_pert[self.oidx,pidx]
            self.reverse_growth_rate = self.forward_growth_rate</code></pre>
</details>
</dd>
<dt id="mdsine2.filtering.SubjectLogTrajectorySetMP.update_proposals"><code class="name flex">
<span>def <span class="ident">update_proposals</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Update the proposal if necessary</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_proposals(self):
    &#39;&#39;&#39;Update the proposal if necessary
    &#39;&#39;&#39;
    if self.sample_iter &gt; self.end_iter:
        self.add_trace = False
        return
    if self.sample_iter == 0:
        return
    if self.trace_iter - self.delay == self.tune  and self.sample_iter - self.delay &gt; 0:

        # Adjust
        acc_rate = self.acceptances/self.n_props_total
        if acc_rate &lt; 0.1:
            logging.debug(&#39;Very low acceptance rate, scaling down past covariance&#39;)
            self.proposal_std *= 0.01
        elif acc_rate &lt; self.target_acceptance_rate:
            self.proposal_std /= np.sqrt(1.5)
        else:
            self.proposal_std *= np.sqrt(1.5)
        
        self.acceptances = 0
        self.n_props_local = 0</code></pre>
</details>
</dd>
<dt id="mdsine2.filtering.SubjectLogTrajectorySetMP.update_single"><code class="name flex">
<span>def <span class="ident">update_single</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Update a single oidx, tidx</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_single(self):
    &#39;&#39;&#39;Update a single oidx, tidx
    &#39;&#39;&#39;
    tidx = self.tidx
    oidx = self.oidx

    # Check if we should update the zero inflation policy
    if self.zero_inflation_transition_policy is not None:
        if self.zero_inflation_transition_policy == &#39;ignore&#39;:
            if not self.zero_inflation_data[oidx,tidx]:
                self.x[oidx, tidx] = np.nan
                self.logx[oidx, tidx] = np.nan
                return
            else:
                if tidx &lt; self.zero_inflation_data.shape[1]-1:
                    do_forward = self.zero_inflation_data[oidx, tidx+1]
                else:
                    do_forward = True
                if tidx &gt; 0:
                    do_reverse = self.zero_inflation_data[oidx, tidx-1]
                else:
                    do_reverse = True
        else:
            raise NotImplementedError(&#39;Not Implemented&#39;)
    else:
        do_forward = True
        do_reverse = True

    # t = self.times[self.tidx]
    # # proposal
    # mu1 = self.curr_logx[tidx]
    # rel = self.reads[t][oidx]/self.read_depths[t]
    # if rel == 0:
    #     rel = 1e-5
    # mu2 = np.log(rel*np.exp(self.curr_qpcr_loc + (self.curr_qpcr_scale/2)))

    # var1 = self.proposal_std[(tidx, oidx)]**2
    # var2 = (self.curr_qpcr_scale)**2
    # mu,var = prod_gaussians(means=[mu1,mu2], variances=[var1,var2])

    try:
        logx_new = pl.random.misc.fast_sample_normal(
            self.curr_logx[tidx],
            self.proposal_std)
    except:
        print(&#39;mu&#39;, self.curr_logx[tidx])
        print(&#39;std&#39;, self.proposal_std)
        raise
    # try:
    #     logx_new = pl.random.misc.fast_sample_normal(
    #         mu, np.sqrt(var))
    # except:
    #     print(&#39;mu&#39;, mu)
    #     print(&#39;std&#39;, np.sqrt(var))
    #     raise

    x_new = np.exp(logx_new)
    prev_logx_value = self.curr_logx[tidx]
    prev_x_value = self.curr_x[tidx]

    # print(&#39;prex_x&#39;, prev_x_value, np.exp(prev_logx_value))
    # print(&#39;prev_logx&#39;, prev_logx_value)

    # if tidx == 5:
    #     print(&#39;\ntidx&#39;, tidx)
    #     print(&#39;oidx&#39;, oidx)
    #     print(&#39;t&#39;, self.times[self.tidx])
    #     print(&#39;curr_logx&#39;, self.curr_logx[tidx])
    #     # print(&#39;curr_logx&#39;, self.curr_logx[tidx])
    #     # print(&#39;mu1, mu2&#39;, mu1, mu2)
    #     # print(&#39;mu&#39;, mu)
    #     # print(&#39;var1, var2&#39;, var1,var2)
    #     # print(&#39;var&#39;,var)
    #     print(&#39;prop_logx&#39;, logx_new)
    #     # print(&#39;start perts&#39;, self.pert_starts)
    #     # print(&#39;end perts&#39;, self.pert_ends)
    #     # print(&#39;in perturbation transition?&#39;, self.in_pert_transition[tidx])
    #     # print(&#39;fully in pert?&#39;, self.fully_in_pert[self.tidx])
    #     print(&#39;forward growth&#39;, self.forward_growth_rate)

    if do_forward:
        prev_aaa = self.forward_loglik()
    else:
        prev_aaa = 0
    if do_reverse:
        prev_bbb = self.reverse_loglik()
    else:
        prev_bbb = 0
    prev_ddd = self.data_loglik()

    # if tidx == 5:
    #     print(&#39;\nold&#39;)
    #     print(&#39;forward ll&#39;, aaa)
    #     print(&#39;reverse ll&#39;, bbb)
    #     print(&#39;data ll&#39;, ddd)

    l_old = prev_aaa + prev_bbb + prev_ddd

    self.curr_x[tidx] = x_new
    self.curr_logx[tidx] = logx_new
    self.sum_q[tidx] = self.sum_q[tidx] - prev_x_value + x_new

    if do_forward:
        new_aaa = self.forward_loglik()
    else:
        new_aaa = 0
    if do_reverse:
        new_bbb = self.reverse_loglik()
    else:
        new_bbb = 0
    new_ddd = self.data_loglik()

    # if tidx == 5:
    #     print(&#39;\nnew&#39;)
    #     print(&#39;forward ll&#39;, aaa)
    #     print(&#39;reverse ll&#39;, bbb)
    #     print(&#39;data ll&#39;, ddd)
    #     print(&#39;\nold x value&#39;, prev_x_value)
    #     print(&#39;old logx value&#39;, prev_logx_value)
    #     print(&#39;proposal std&#39;, self.proposal_std[(tidx, oidx)])
    #     print(&#39;new x value&#39;, x_new)
    #     print(&#39;new logx value&#39;, logx_new)

    l_new = new_aaa + new_bbb + new_ddd
    r_accept = l_new - l_old

    # if tidx == 0:
    #     print(&#39;\n\noidx {} diff lls:&#39;.format(oidx), r_accept)
    #     print(&#39;\tforward&#39;, new_aaa - prev_aaa)
    #     print(&#39;\treverse&#39;, new_bbb - prev_bbb)
    #     print(&#39;\tdata&#39;, new_ddd - prev_ddd)

    # if tidx == 5:
    #     print(&#39;r_accept&#39;, r_accept)
    r = pl.random.misc.fast_sample_standard_uniform()
    if math.log(r) &gt; r_accept:
        # print(&#39;reject&#39;)
        # reject
        self.sum_q[tidx] = self.sum_q[tidx] + prev_x_value - x_new
        self.curr_x[tidx] = prev_x_value
        self.curr_logx[tidx] = prev_logx_value
    else:
        # print(&#39;accept&#39;)
        self.x[oidx, tidx] = x_new
        self.logx[oidx, tidx] = logx_new
        self.acceptances += 1
        self.total_acceptances += 1
        self.n_accepted_iter += 1

    self.n_props_local += 1
    self.n_props_total += 1</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="mdsine2.filtering.TrajectorySet"><code class="flex name class">
<span>class <span class="ident">TrajectorySet</span></span>
<span>(</span><span>name, G, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>This aggregates a set of trajectories from each set</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>name</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of the object</dd>
<dt><strong><code>G</code></strong> :&ensp;<code>pylab.graph.Graph</code></dt>
<dd>Graph object to attach it to</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TrajectorySet(pl.graph.Node):
    &#39;&#39;&#39;This aggregates a set of trajectories from each set

    Parameters
    ----------
    name : str
        Name of the object
    G : pylab.graph.Graph
        Graph object to attach it to
    &#39;&#39;&#39;
    def __init__(self, name, G, **kwargs):
        pl.graph.Node.__init__(self, name=name, G=G)
        self.value = []
        n_asvs = self.G.data.n_asvs

        for ridx in range(self.G.data.n_replicates):
            n_timepoints = self.G.data.n_timepoints_for_replicate[ridx]

            # initialize values to zeros for initialization
            self.value.append(pl.variables.Variable(
                name=name+&#39;_ridx{}&#39;.format(ridx), G=G, shape=(n_asvs, n_timepoints),
                value=np.zeros((n_asvs, n_timepoints), dtype=float), **kwargs))
        prior = pl.variables.Normal(
            mean=pl.variables.Constant(name=self.name+&#39;_prior_mean&#39;, value=0, G=self.G),
            var=pl.variables.Constant(name=self.name+&#39;_prior_var&#39;, value=1, G=self.G),
            name=self.name+&#39;_prior&#39;, G=self.G)
        self.add_prior(prior)

    def __getitem__(self, ridx):
        return self.value[ridx]

    @property
    def sample_iter(self):
        return self.value[0].sample_iter

    def reset_value_size(self):
        &#39;&#39;&#39;Change the size of the trajectory when we set the intermediate timepoints
        &#39;&#39;&#39;
        n_asvs = self.G.data.n_asvs
        for ridx in range(len(self.value)):
            n_timepoints = self.G.data.n_timepoints_for_replicate[ridx]
            self.value[ridx].value = np.zeros((n_asvs, n_timepoints),dtype=float)
            self.value[ridx].set_value_shape(self.value[ridx].value.shape)

    def _vectorize(self):
        &#39;&#39;&#39;Get all the data in vector form
        &#39;&#39;&#39;
        vals = np.array([])
        for data in self.value:
            vals = np.append(vals, data.value)
        return vals

    def mean(self):
        return np.mean(self._vectorize())

    def var(self):
        return np.var(self._vectorize())

    def iter_indices(self):
        &#39;&#39;&#39;Iterate through the indices and the values
        &#39;&#39;&#39;
        for ridx in range(self.G.data.n_replicates):
            for tidx in range(self.G.data.n_timepoints_for_replicate[ridx]):
                for oidx in range(self.G.data.asvs.n_asvs):
                    yield (ridx, tidx, oidx)

    def set_trace(self, *args, **kwargs):
        for ridx in range(len(self.value)):
            self.value[ridx].set_trace(*args, **kwargs)

    def add_trace(self):
        for ridx in range(len(self.value)):
            # Set the zero inflation values to nans
            self.value[ridx].value[~self.G[REPRNAMES.ZERO_INFLATION].value[ridx]] = np.nan
            self.value[ridx].add_trace()

    def add_init_value(self):
        for ridx in range(len(self.value)):
            self.value[ridx].add_init_value()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mdsine2.pylab.graph.Node" href="pylab/graph.html#mdsine2.pylab.graph.Node">Node</a></li>
<li><a title="mdsine2.pylab.graph.BaseNode" href="pylab/graph.html#mdsine2.pylab.graph.BaseNode">BaseNode</a></li>
<li><a title="mdsine2.pylab.base.Saveable" href="pylab/base.html#mdsine2.pylab.base.Saveable">Saveable</a></li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="mdsine2.filtering.TrajectorySet.sample_iter"><code class="name">var <span class="ident">sample_iter</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def sample_iter(self):
    return self.value[0].sample_iter</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="mdsine2.filtering.TrajectorySet.add_init_value"><code class="name flex">
<span>def <span class="ident">add_init_value</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_init_value(self):
    for ridx in range(len(self.value)):
        self.value[ridx].add_init_value()</code></pre>
</details>
</dd>
<dt id="mdsine2.filtering.TrajectorySet.add_trace"><code class="name flex">
<span>def <span class="ident">add_trace</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_trace(self):
    for ridx in range(len(self.value)):
        # Set the zero inflation values to nans
        self.value[ridx].value[~self.G[REPRNAMES.ZERO_INFLATION].value[ridx]] = np.nan
        self.value[ridx].add_trace()</code></pre>
</details>
</dd>
<dt id="mdsine2.filtering.TrajectorySet.iter_indices"><code class="name flex">
<span>def <span class="ident">iter_indices</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Iterate through the indices and the values</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def iter_indices(self):
    &#39;&#39;&#39;Iterate through the indices and the values
    &#39;&#39;&#39;
    for ridx in range(self.G.data.n_replicates):
        for tidx in range(self.G.data.n_timepoints_for_replicate[ridx]):
            for oidx in range(self.G.data.asvs.n_asvs):
                yield (ridx, tidx, oidx)</code></pre>
</details>
</dd>
<dt id="mdsine2.filtering.TrajectorySet.mean"><code class="name flex">
<span>def <span class="ident">mean</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def mean(self):
    return np.mean(self._vectorize())</code></pre>
</details>
</dd>
<dt id="mdsine2.filtering.TrajectorySet.reset_value_size"><code class="name flex">
<span>def <span class="ident">reset_value_size</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Change the size of the trajectory when we set the intermediate timepoints</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset_value_size(self):
    &#39;&#39;&#39;Change the size of the trajectory when we set the intermediate timepoints
    &#39;&#39;&#39;
    n_asvs = self.G.data.n_asvs
    for ridx in range(len(self.value)):
        n_timepoints = self.G.data.n_timepoints_for_replicate[ridx]
        self.value[ridx].value = np.zeros((n_asvs, n_timepoints),dtype=float)
        self.value[ridx].set_value_shape(self.value[ridx].value.shape)</code></pre>
</details>
</dd>
<dt id="mdsine2.filtering.TrajectorySet.set_trace"><code class="name flex">
<span>def <span class="ident">set_trace</span></span>(<span>self, *args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_trace(self, *args, **kwargs):
    for ridx in range(len(self.value)):
        self.value[ridx].set_trace(*args, **kwargs)</code></pre>
</details>
</dd>
<dt id="mdsine2.filtering.TrajectorySet.var"><code class="name flex">
<span>def <span class="ident">var</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def var(self):
    return np.var(self._vectorize())</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="mdsine2.pylab.graph.Node" href="pylab/graph.html#mdsine2.pylab.graph.Node">Node</a></b></code>:
<ul class="hlist">
<li><code><a title="mdsine2.pylab.graph.Node.add_child" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_child">add_child</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.add_parent" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_parent">add_parent</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.add_prior" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_prior">add_prior</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.add_undirected" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_undirected">add_undirected</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.degree" href="pylab/graph.html#mdsine2.pylab.graph.Node.degree">degree</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.delete" href="pylab/graph.html#mdsine2.pylab.graph.BaseNode.delete">delete</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.get_adjacent_keys" href="pylab/graph.html#mdsine2.pylab.graph.Node.get_adjacent_keys">get_adjacent_keys</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.load" href="pylab/base.html#mdsine2.pylab.base.Saveable.load">load</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.metropolis" href="pylab/graph.html#mdsine2.pylab.graph.Node.metropolis">metropolis</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.save" href="pylab/base.html#mdsine2.pylab.base.Saveable.save">save</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.set_save_location" href="pylab/base.html#mdsine2.pylab.base.Saveable.set_save_location">set_save_location</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="mdsine2.filtering.ZeroInflation"><code class="flex name class">
<span>class <span class="ident">ZeroInflation</span></span>
<span>(</span><span>**kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>This is the posterior distribution for the zero inflation model. These are used
to learn when the model should use use the data and when it should not. We do not need
to trace this object because we set the structural zeros to nans in the trace for
filtering.</p>
<p>TODO: Parallel version of the class</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>mp</code></strong> :&ensp;<code>str</code></dt>
<dd>This is the type of parallelization to use. This is not implemented yet.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ZeroInflation(pl.graph.Node):
    &#39;&#39;&#39;This is the posterior distribution for the zero inflation model. These are used
    to learn when the model should use use the data and when it should not. We do not need
    to trace this object because we set the structural zeros to nans in the trace for 
    filtering.

    TODO: Parallel version of the class
    &#39;&#39;&#39;

    def __init__(self, **kwargs):
        &#39;&#39;&#39;
        Parameters
        ----------
        mp : str
            This is the type of parallelization to use. This is not implemented yet.
        &#39;&#39;&#39;
        kwargs[&#39;name&#39;] = STRNAMES.ZERO_INFLATION
        pl.graph.Node.__init__(self, **kwargs)
        self.value = []
        self._strr = &#39;NA&#39;

        for ridx in range(self.G.data.n_replicates):
            n_timepoints = self.G.data.n_timepoints_for_replicate[ridx]
            self.value.append(np.ones(shape=(len(self.G.data.asvs), n_timepoints), dtype=bool))

    def reset_value_size(self):
        &#39;&#39;&#39;Change the size of the trajectory when we set the intermediate timepoints
        &#39;&#39;&#39;
        n_asvs = self.G.data.n_asvs
        for ridx in range(len(self.value)):
            n_timepoints = self.G.data.n_timepoints_for_replicate[ridx]
            self.value[ridx] = np.ones((n_asvs, n_timepoints), dtype=bool)

    def __str__(self):
        return self._strr

    def initialize(self, value_option, delay=0):
        &#39;&#39;&#39;Initialize the values. Right now this is static and we are not learning this so
        do not do anything fancy

        Parameters
        ----------

        delay : None, int
            How much to delay starting the sampling
        &#39;&#39;&#39;
        if delay is None:
            delay = 0
        if not pl.isint(delay):
            raise TypeError(&#39;`delay` ({}) must be an int&#39;.format(type(delay)))
        self.delay = delay

        if value_option in [None, &#39;auto&#39;]:
            # Set everything to on
            self.value = []
            for ridx in range(self.G.data.n_replicates):
                n_timepoints = self.G.data.n_timepoints_for_replicate[ridx]
                self.value.append(np.ones(
                    shape=(len(self.G.data.asvs), n_timepoints), dtype=bool))
            turn_on = None
            turn_off = None

        elif value_option == &#39;mdsine-cdiff&#39;:
            # Set everything to on except for cdiff before day 28 for every subject
            self.value = []
            for ridx in range(self.G.data.n_replicates):
                n_timepoints = self.G.data.n_timepoints_for_replicate[ridx]
                self.value.append(np.ones(
                    shape=(len(self.G.data.asvs), n_timepoints), dtype=bool))

            # Get cdiff
            cdiff_idx = self.G.data.asvs[&#39;Clostridium-difficile&#39;].idx
            turn_off = []
            turn_on = []
            for ridx in range(self.G.data.n_replicates):
                for tidx, t in enumerate(self.G.data.times[ridx]):
                    for oidx in range(len(self.G.data.asvs)):
                        if t &lt; 28 and oidx == cdiff_idx:
                            self.value[ridx][cdiff_idx, tidx] = False
                            turn_off.append((ridx, tidx, cdiff_idx))
                        else:
                            turn_on.append((ridx, tidx, oidx))

        else:
            raise ValueError(&#39;`value_option` ({}) not recognized&#39;.format(value_option))

        self.G.data.set_zero_inflation(turn_on=turn_on, turn_off=turn_off)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mdsine2.pylab.graph.Node" href="pylab/graph.html#mdsine2.pylab.graph.Node">Node</a></li>
<li><a title="mdsine2.pylab.graph.BaseNode" href="pylab/graph.html#mdsine2.pylab.graph.BaseNode">BaseNode</a></li>
<li><a title="mdsine2.pylab.base.Saveable" href="pylab/base.html#mdsine2.pylab.base.Saveable">Saveable</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="mdsine2.filtering.ZeroInflation.initialize"><code class="name flex">
<span>def <span class="ident">initialize</span></span>(<span>self, value_option, delay=0)</span>
</code></dt>
<dd>
<div class="desc"><p>Initialize the values. Right now this is static and we are not learning this so
do not do anything fancy</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>delay</code></strong> :&ensp;<code>None, int</code></dt>
<dd>How much to delay starting the sampling</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def initialize(self, value_option, delay=0):
    &#39;&#39;&#39;Initialize the values. Right now this is static and we are not learning this so
    do not do anything fancy

    Parameters
    ----------

    delay : None, int
        How much to delay starting the sampling
    &#39;&#39;&#39;
    if delay is None:
        delay = 0
    if not pl.isint(delay):
        raise TypeError(&#39;`delay` ({}) must be an int&#39;.format(type(delay)))
    self.delay = delay

    if value_option in [None, &#39;auto&#39;]:
        # Set everything to on
        self.value = []
        for ridx in range(self.G.data.n_replicates):
            n_timepoints = self.G.data.n_timepoints_for_replicate[ridx]
            self.value.append(np.ones(
                shape=(len(self.G.data.asvs), n_timepoints), dtype=bool))
        turn_on = None
        turn_off = None

    elif value_option == &#39;mdsine-cdiff&#39;:
        # Set everything to on except for cdiff before day 28 for every subject
        self.value = []
        for ridx in range(self.G.data.n_replicates):
            n_timepoints = self.G.data.n_timepoints_for_replicate[ridx]
            self.value.append(np.ones(
                shape=(len(self.G.data.asvs), n_timepoints), dtype=bool))

        # Get cdiff
        cdiff_idx = self.G.data.asvs[&#39;Clostridium-difficile&#39;].idx
        turn_off = []
        turn_on = []
        for ridx in range(self.G.data.n_replicates):
            for tidx, t in enumerate(self.G.data.times[ridx]):
                for oidx in range(len(self.G.data.asvs)):
                    if t &lt; 28 and oidx == cdiff_idx:
                        self.value[ridx][cdiff_idx, tidx] = False
                        turn_off.append((ridx, tidx, cdiff_idx))
                    else:
                        turn_on.append((ridx, tidx, oidx))

    else:
        raise ValueError(&#39;`value_option` ({}) not recognized&#39;.format(value_option))

    self.G.data.set_zero_inflation(turn_on=turn_on, turn_off=turn_off)</code></pre>
</details>
</dd>
<dt id="mdsine2.filtering.ZeroInflation.reset_value_size"><code class="name flex">
<span>def <span class="ident">reset_value_size</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Change the size of the trajectory when we set the intermediate timepoints</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset_value_size(self):
    &#39;&#39;&#39;Change the size of the trajectory when we set the intermediate timepoints
    &#39;&#39;&#39;
    n_asvs = self.G.data.n_asvs
    for ridx in range(len(self.value)):
        n_timepoints = self.G.data.n_timepoints_for_replicate[ridx]
        self.value[ridx] = np.ones((n_asvs, n_timepoints), dtype=bool)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="mdsine2.pylab.graph.Node" href="pylab/graph.html#mdsine2.pylab.graph.Node">Node</a></b></code>:
<ul class="hlist">
<li><code><a title="mdsine2.pylab.graph.Node.add_child" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_child">add_child</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.add_parent" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_parent">add_parent</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.add_prior" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_prior">add_prior</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.add_undirected" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_undirected">add_undirected</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.degree" href="pylab/graph.html#mdsine2.pylab.graph.Node.degree">degree</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.delete" href="pylab/graph.html#mdsine2.pylab.graph.BaseNode.delete">delete</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.get_adjacent_keys" href="pylab/graph.html#mdsine2.pylab.graph.Node.get_adjacent_keys">get_adjacent_keys</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.load" href="pylab/base.html#mdsine2.pylab.base.Saveable.load">load</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.metropolis" href="pylab/graph.html#mdsine2.pylab.graph.Node.metropolis">metropolis</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.save" href="pylab/base.html#mdsine2.pylab.base.Saveable.save">save</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.set_save_location" href="pylab/base.html#mdsine2.pylab.base.Saveable.set_save_location">set_save_location</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="mdsine2" href="index.html">mdsine2</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="mdsine2.filtering.FilteringLogMP" href="#mdsine2.filtering.FilteringLogMP">FilteringLogMP</a></code></h4>
<ul class="two-column">
<li><code><a title="mdsine2.filtering.FilteringLogMP.add_init_value" href="#mdsine2.filtering.FilteringLogMP.add_init_value">add_init_value</a></code></li>
<li><code><a title="mdsine2.filtering.FilteringLogMP.add_trace" href="#mdsine2.filtering.FilteringLogMP.add_trace">add_trace</a></code></li>
<li><code><a title="mdsine2.filtering.FilteringLogMP.initialize" href="#mdsine2.filtering.FilteringLogMP.initialize">initialize</a></code></li>
<li><code><a title="mdsine2.filtering.FilteringLogMP.kill" href="#mdsine2.filtering.FilteringLogMP.kill">kill</a></code></li>
<li><code><a title="mdsine2.filtering.FilteringLogMP.sample_iter" href="#mdsine2.filtering.FilteringLogMP.sample_iter">sample_iter</a></code></li>
<li><code><a title="mdsine2.filtering.FilteringLogMP.set_latent_as_data" href="#mdsine2.filtering.FilteringLogMP.set_latent_as_data">set_latent_as_data</a></code></li>
<li><code><a title="mdsine2.filtering.FilteringLogMP.set_trace" href="#mdsine2.filtering.FilteringLogMP.set_trace">set_trace</a></code></li>
<li><code><a title="mdsine2.filtering.FilteringLogMP.update" href="#mdsine2.filtering.FilteringLogMP.update">update</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="mdsine2.filtering.SubjectLogTrajectorySetMP" href="#mdsine2.filtering.SubjectLogTrajectorySetMP">SubjectLogTrajectorySetMP</a></code></h4>
<ul class="">
<li><code><a title="mdsine2.filtering.SubjectLogTrajectorySetMP.compute_dynamics" href="#mdsine2.filtering.SubjectLogTrajectorySetMP.compute_dynamics">compute_dynamics</a></code></li>
<li><code><a title="mdsine2.filtering.SubjectLogTrajectorySetMP.data_loglik_w_intermediates" href="#mdsine2.filtering.SubjectLogTrajectorySetMP.data_loglik_w_intermediates">data_loglik_w_intermediates</a></code></li>
<li><code><a title="mdsine2.filtering.SubjectLogTrajectorySetMP.data_loglik_wo_intermediates" href="#mdsine2.filtering.SubjectLogTrajectorySetMP.data_loglik_wo_intermediates">data_loglik_wo_intermediates</a></code></li>
<li><code><a title="mdsine2.filtering.SubjectLogTrajectorySetMP.default_forward_loglik" href="#mdsine2.filtering.SubjectLogTrajectorySetMP.default_forward_loglik">default_forward_loglik</a></code></li>
<li><code><a title="mdsine2.filtering.SubjectLogTrajectorySetMP.default_reverse_loglik" href="#mdsine2.filtering.SubjectLogTrajectorySetMP.default_reverse_loglik">default_reverse_loglik</a></code></li>
<li><code><a title="mdsine2.filtering.SubjectLogTrajectorySetMP.first_timepoint_reverse" href="#mdsine2.filtering.SubjectLogTrajectorySetMP.first_timepoint_reverse">first_timepoint_reverse</a></code></li>
<li><code><a title="mdsine2.filtering.SubjectLogTrajectorySetMP.initialize" href="#mdsine2.filtering.SubjectLogTrajectorySetMP.initialize">initialize</a></code></li>
<li><code><a title="mdsine2.filtering.SubjectLogTrajectorySetMP.last_timepoint_forward" href="#mdsine2.filtering.SubjectLogTrajectorySetMP.last_timepoint_forward">last_timepoint_forward</a></code></li>
<li><code><a title="mdsine2.filtering.SubjectLogTrajectorySetMP.persistent_run" href="#mdsine2.filtering.SubjectLogTrajectorySetMP.persistent_run">persistent_run</a></code></li>
<li><code><a title="mdsine2.filtering.SubjectLogTrajectorySetMP.set_attrs_for_timepoint" href="#mdsine2.filtering.SubjectLogTrajectorySetMP.set_attrs_for_timepoint">set_attrs_for_timepoint</a></code></li>
<li><code><a title="mdsine2.filtering.SubjectLogTrajectorySetMP.update_proposals" href="#mdsine2.filtering.SubjectLogTrajectorySetMP.update_proposals">update_proposals</a></code></li>
<li><code><a title="mdsine2.filtering.SubjectLogTrajectorySetMP.update_single" href="#mdsine2.filtering.SubjectLogTrajectorySetMP.update_single">update_single</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="mdsine2.filtering.TrajectorySet" href="#mdsine2.filtering.TrajectorySet">TrajectorySet</a></code></h4>
<ul class="two-column">
<li><code><a title="mdsine2.filtering.TrajectorySet.add_init_value" href="#mdsine2.filtering.TrajectorySet.add_init_value">add_init_value</a></code></li>
<li><code><a title="mdsine2.filtering.TrajectorySet.add_trace" href="#mdsine2.filtering.TrajectorySet.add_trace">add_trace</a></code></li>
<li><code><a title="mdsine2.filtering.TrajectorySet.iter_indices" href="#mdsine2.filtering.TrajectorySet.iter_indices">iter_indices</a></code></li>
<li><code><a title="mdsine2.filtering.TrajectorySet.mean" href="#mdsine2.filtering.TrajectorySet.mean">mean</a></code></li>
<li><code><a title="mdsine2.filtering.TrajectorySet.reset_value_size" href="#mdsine2.filtering.TrajectorySet.reset_value_size">reset_value_size</a></code></li>
<li><code><a title="mdsine2.filtering.TrajectorySet.sample_iter" href="#mdsine2.filtering.TrajectorySet.sample_iter">sample_iter</a></code></li>
<li><code><a title="mdsine2.filtering.TrajectorySet.set_trace" href="#mdsine2.filtering.TrajectorySet.set_trace">set_trace</a></code></li>
<li><code><a title="mdsine2.filtering.TrajectorySet.var" href="#mdsine2.filtering.TrajectorySet.var">var</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="mdsine2.filtering.ZeroInflation" href="#mdsine2.filtering.ZeroInflation">ZeroInflation</a></code></h4>
<ul class="">
<li><code><a title="mdsine2.filtering.ZeroInflation.initialize" href="#mdsine2.filtering.ZeroInflation.initialize">initialize</a></code></li>
<li><code><a title="mdsine2.filtering.ZeroInflation.reset_value_size" href="#mdsine2.filtering.ZeroInflation.reset_value_size">reset_value_size</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.1</a>.</p>
</footer>
</body>
</html>