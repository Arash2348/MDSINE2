<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.1" />
<title>mdsine2.clustering API documentation</title>
<meta name="description" content="Clustering parameters for the posterior" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>mdsine2.clustering</code></h1>
</header>
<section id="section-intro">
<p>Clustering parameters for the posterior</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#39;&#39;&#39;Clustering parameters for the posterior
&#39;&#39;&#39;

import logging
import time
import itertools
import psutil

import numpy as np
import numba
import scipy.sparse
import numpy.random as npr
import scipy.stats
import scipy
import math

from .util import build_prior_covariance, build_prior_mean, sample_categorical_log, \
    log_det, pinv, generate_cluster_assignments_posthoc
from .names import STRNAMES, REPRNAMES
from . import pylab as pl

from . import visualization
import matplotlib.pyplot as plt


class Concentration(pl.variables.Gamma):
    &#39;&#39;&#39;Defines the posterior for the concentration parameter that is used
    in learning the cluster assignments.
    The posterior is implemented as it is describes in &#39;Bayesian Inference
    for Density Estimation&#39; by M. D. Escobar and M. West, 1995.
    &#39;&#39;&#39;
    def __init__(self, prior, value=None, n_iter=None, **kwargs):
        &#39;&#39;&#39;Parameters

        value (float, int)
            - Initial value of the concentration
            - Default value is the mean of the prior
        &#39;&#39;&#39;
        kwargs[&#39;name&#39;] = STRNAMES.CONCENTRATION
        # initialize shape and scale as the same as the priors
        # we will be updating this later
        pl.variables.Gamma.__init__(self, shape=prior.shape.value, scale=prior.scale.value,
            dtype=float, **kwargs)
        self.add_prior(prior)

    def initialize(self, value_option, hyperparam_option, n_iter=None, value=None,
        shape=None, scale=None, delay=0):
        &#39;&#39;&#39;Initialize the hyperparameters of the beta prior

        Parameters
        ----------
        value_option (str)
            - Options to initialize the value
            - &#39;manual&#39;
                - Set the value manually, `value` must also be specified
            - &#39;auto&#39;, &#39;prior-mean&#39;
                - Set to the mean of the prior
        hyperparam_option (str)
            - Options ot initialize the hyperparameters
            - Options
                - &#39;manual&#39;
                    - Set the values manually. `shape` and `scale` must also be specified
                - &#39;auto&#39;, &#39;diffuse&#39;
                    - shape = 1e-5, scale= 1e5
        shape, scale (int, float)
            - User specified values
            - Only necessary if `hyperparam_option` == &#39;manual&#39;
        &#39;&#39;&#39;
        if not pl.isint(delay):
            raise TypeError(&#39;`delay` ({}) must be an int&#39;.format(type(delay)))
        if delay &lt; 0:
            raise ValueError(&#39;`delay` ({}) must be &gt;= 0&#39;.format(delay))
        self.delay = delay

        if hyperparam_option == &#39;manual&#39;:
            if pl.isnumeric(shape) and pl.isnumeric(scale):
                self.prior.shape.override_value(shape)
                self.prior.scale.override_value(scale)
            else:
                raise ValueError(&#39;shape ({}) and scale ({}) must be numeric&#39; \
                    &#39; (float, int)&#39;.format(shape.__class__, scale.__class__))

            if not pl.isint(n_iter):
                raise ValueError(&#39;`n_iter` ({}) needs ot be an int&#39;.format(n_iter.__class__))
            self.n_iter = n_iter

        elif hyperparam_option == &#39;strong-few&#39;:
            self.prior.shape.override_value(1)
            self.prior.scale.override_value(1)
            self.n_iter = 20

        elif hyperparam_option in [&#39;diffuse&#39;, &#39;auto&#39;]:
            self.prior.shape.override_value(1e-5)
            self.prior.scale.override_value(1e5)
            self.n_iter = 20
        else:
            raise ValueError(&#39;hyperparam_option `{}` not recognized&#39;.format(hyperparam_option))

        if value_option == &#39;manual&#39;:
            if pl.isnumeric(value):
                self.value = value
            else:
                raise ValueError(&#39;`value` ({}) must be numeric (float, int)&#39;.format(
                    value.__class__))
        elif value_option in [&#39;auto&#39;, &#39;prior-mean&#39;]:
            self.value = self.prior.mean()
        else:
            raise ValueError(&#39;value_option `{}` not recognized&#39;.format(value_option))

        self.shape.value = self.prior.shape.value
        self.scale.value = self.prior.scale.value
        logging.info(&#39;Cluster Concentration initialization results:\n&#39; \
            &#39;\tprior shape: {}\n&#39; \
            &#39;\tprior scale: {}\n&#39; \
            &#39;\tvalue: {}&#39;.format(
                self.prior.shape.value, self.prior.scale.value, self.value))

    def update(self):
        &#39;&#39;&#39;Sample the posterior of the concentration parameter
        &#39;&#39;&#39;
        if self.sample_iter &lt; self.delay:
            return

        clustering = self.G[REPRNAMES.CLUSTER_INTERACTION_VALUE].clustering
        k = len(clustering)
        n = self.G.data.n_asvs
        # If there are 1 or 0 clusters, do not update
        # if k &lt;= 1:
        #     return
        for i in range(self.n_iter):
            #first sample eta from a beta distribution
            eta = npr.beta(self.value+1,n)
            #sample alpha from a mixture of gammas
            pi_eta = [0.0, n]
            pi_eta[0] = (self.prior.shape.value+k-1.)/(1/(self.prior.scale.value)-np.log(eta))
            self.scale.value =  1/(1/self.prior.scale.value - np.log(eta))
            self.shape.value = self.prior.shape.value + k
            if np.random.choice([0,1], p=pi_eta/np.sum(pi_eta)) != 0:
                self.shape.value -= 1
            self.sample()
            # print(&#39;pi_eta[0]&#39;,pi_eta[0])

    def visualize_posterior(self, path, f, section=&#39;posterior&#39;):
        &#39;&#39;&#39;Render the traces in the folder `basepath` and write the 
        learned values to the file `f`.

        Parameters
        ----------
        path : str
            This is the path to write the files to
        f : _io.TextIOWrapper
            File that we are writing the values to
        section : str
            Section of the trace to compute on. Options:
                &#39;posterior&#39; : posterior samples
                &#39;burnin&#39; : burn-in samples
                &#39;entire&#39; : both burn-in and posterior samples

        Returns
        -------
        _io.TextIOWrapper
        &#39;&#39;&#39;
        f.write(&#39;\n\n###################################\n&#39;)
        f.write(self.name)
        f.write(&#39;\n###################################\n&#39;)
        if not self.G.inference.is_being_traced(self):
            f.write(&#39;`{}` not learned\n\tValue: {}\n&#39;.format(self.name, self.value))
            return f

        summ = pl.summary(self, section=section)
        for k,v in summ.items():
            f.write(&#39;\t{}: {}\n&#39;.format(k,v))

        ax1, _ = visualization.render_trace(var=self, plt_type=&#39;both&#39;, 
            section=section, include_burnin=True, log_scale=True, rasterized=True)

        l,h = ax1.get_xlim()
        xs = np.arange(l,h,step=(h-l)/1000) 
        ys = []
        for x in xs:
            ys.append(self.prior.pdf(value=x))
        ax1.plot(xs, ys, label=&#39;prior&#39;, alpha=0.5, color=&#39;red&#39;)
        ax1.legend()

        fig = plt.gcf()
        fig.suptitle(&#39;Concentration&#39;)
        plt.savefig(path)
        plt.close()
        f.close()


class ClusterAssignments(pl.graph.Node):
    &#39;&#39;&#39;This is the posterior of the cluster assignments for each ASV.

    To calculate the loglikelihood of an ASV being in a cluster, we have to
    marginalize out the cluster that the ASV belongs to - this means marginalizing
    out the interactions going in and out of the cluster in question, along with all
    the perturbations associated with it.
    &#39;&#39;&#39;
    def __init__(self, clustering, concentration, m=1, mp=None, **kwargs):
        &#39;&#39;&#39;Parameters

        clustering (pylab.cluster.Clustering)
            - Defines the clusters
        concentration (pylab.Variable or subclass of pylab.Variable)
            - Defines the concentration parameter for the base distribution
        m (int, Optional)
            - Number of auxiliary variables defined in the model
            - Default is 1
        mp : str, None
            This is the type of multiprocessing it is going to be. Options:
                None
                    No multiprocessing
                &#39;full-#&#39;
                    Send out to the different processors, where &#39;#&#39; is the number of
                    processors to make
                &#39;debug&#39;
                    Send out to the different classes but stay on a single core. This
                    is necessary for benchmarking and easier debugging.
        &#39;&#39;&#39;
        self.clustering = clustering
        self.concentration = concentration
        self.m = m
        self.mp = mp
        self._strtime = -1
        # self.compute_device = &#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;

        if self.mp is not None:
            self.update = self.update_mp
        else:
            self.update = self.update_slow_fast

        kwargs[&#39;name&#39;] = STRNAMES.CLUSTERING
        pl.graph.Node.__init__(self, **kwargs)

    def __str__(self):
        return str(self.clustering) + &#39;\nTotal time: {}&#39;.format(self._strtime)

    def __getstate__(self):
        state = self.__dict__.copy()
        state.pop(&#39;pool&#39;, None)
        state.pop(&#39;actors&#39;, None)
        return state

    def __setstate__(self, state):
        self.__dict__.update(state)
        self.pool = []
        self.actors = None

    @property
    def value(self):
        &#39;&#39;&#39;This is so that the cluster assignments get printed in inference.MCMC
        &#39;&#39;&#39;
        return self.clustering

    @property
    def sample_iter(self):
        return self.clustering.n_clusters.sample_iter

    def initialize(self, value_option, hyperparam_option=None, value=None, n_clusters=None,
        delay=0, run_every_n_iterations=1):
        &#39;&#39;&#39;Initialize the cluster assingments - there are no hyperparamters to
        initialize because the concentration is initialized somewhere else

        Note - if `n_clusters` is not specified and the cluster initialization
        method requires it - it will be set to the expected number of clusters
        which = log(n_asvs)/log(2)

        Parameters
        ----------
        value_option : str
            The different methods to initialize the clusters
            Options
                &#39;manual&#39;
                    Manually set the cluster assignments
                &#39;no-clusters&#39;
                    Every ASV in their own cluster
                &#39;random&#39;
                    Every ASV is randomly assigned to the number of clusters. `n_clusters` required
                &#39;taxonomy&#39;
                    Cluster ASVs based on their taxonomic similarity. `n_clusters` required
                &#39;sequence&#39;
                    Cluster ASVs based on their sequence similarity. `n_clusters` required
                &#39;phylogeny&#39;
                    Cluster ASVs based on their phylogenetic similarity. `n_clusters` required
                &#39;spearman&#39;, &#39;auto&#39;
                    Creates a distance matrix based on the spearman rank similarity
                    between two trajectories. We use the raw data. `n_clusters` required
                &#39;fixed-topology&#39;
                    Sets the clustering assignment to the most likely clustering configuration
                    specified in the graph at the location `value` (`value` is a str).
                    We take the mean coclusterings and do agglomerative clustering on that matrix
                    with the `mode` number of clusters.
        hyperparam_option : None
            Not used in this function - only here for API consistency
        value : list of list
            Cluster assingments for each of the ASVs
            Only necessary if `value_option` == &#39;manual&#39;
        n_clusters : int, str
            Necessary if `value_option` is not &#39;manual&#39; or &#39;no-clusters&#39;
            If str, options:
                &#39;expected&#39;, &#39;auto&#39;: log_2(n_asvs)
        run_every_n_iterations : int
            Only run the update every `run_every_n_iterations` iterations
        &#39;&#39;&#39;
        from sklearn.cluster import AgglomerativeClustering
        asvs = self.G.data.asvs

        if not pl.isint(run_every_n_iterations):
            raise TypeError(&#39;`run_every_n_iterations` ({}) must be an int&#39;.format(
                type(run_every_n_iterations)))
        if run_every_n_iterations &lt;= 0:
            raise ValueError(&#39;`run_every_n_iterations` ({}) must be &gt;= 0&#39;.format(
                run_every_n_iterations))

        self.run_every_n_iterations = run_every_n_iterations
        self.delay = delay

        if not pl.isstr(value_option):
            raise TypeError(&#39;`value_option` ({}) must be a str&#39;.format(type(value_option)))
        if value_option not in [&#39;manual&#39;, &#39;no-clusters&#39;, &#39;fixed-topology&#39;]:
            if pl.isstr(n_clusters):
                if n_clusters in [&#39;expected&#39;, &#39;auto&#39;]:
                    n_clusters = int(round(np.log(len(asvs))/np.log(2)))
                else:
                    raise ValueError(&#39;`n_clusters` ({}) not recognized&#39;.format(n_clusters))
            if not pl.isint(n_clusters):
                raise TypeError(&#39;`n_clusters` ({}) must be a str or an int&#39;.format(type(n_clusters)))
            if n_clusters &lt;= 0:
                raise ValueError(&#39;`n_clusters` ({}) must be &gt; 0&#39;.format(n_clusters))
            if n_clusters &gt; self.G.data.n_asvs:
                raise ValueError(&#39;`n_clusters` ({}) must be &lt;= than the number of ASVs ({})&#39;.format(
                    n_clusters, self.G.data.n_asvs))

        if value_option == &#39;manual&#39;:
            # Check that all of the ASVs are in the init and that it is in the right
            # structure
            if not pl.isarray(value):
                raise ValueError(&#39;if `value_option` is &#34;manual&#34;, value ({}) must &#39; \
                    &#39;be of type array&#39;.format(value.__class__))
            clusters = list(value)

            idxs_to_delete = []
            for idx, cluster in enumerate(clusters):
                if not pl.isarray(cluster):
                    raise ValueError(&#39;cluster at index `{}` ({}) is not an array&#39;.format(
                        idx, cluster))
                cluster = list(cluster)
                if len(cluster) == 0:
                    logging.warning(&#39;Cluster index {} has 0 elements, deleting&#39;.format(
                        idx))
                    idxs_to_delete.append(idx)
            if len(idxs_to_delete) &gt; 0:
                clusters = np.delete(clusters, idxs_to_delete).tolist()

            all_oidxs = set()
            for cluster in clusters:
                for oidx in cluster:
                    if not pl.isint(oidx):
                        raise ValueError(&#39;`oidx` ({}) must be an int&#39;.format(oidx.__class__))
                    if oidx &gt;= len(asvs):
                        raise ValueError(&#39;oidx `{}` not in our ASVSet&#39;.format(oidx))
                    all_oidxs.add(oidx)

            for oidx in range(len(asvs)):
                if oidx not in all_oidxs:
                    raise ValueError(&#39;oidx `{}` in ASVSet not in `value` ({})&#39;.format(
                        oidx, value))
            # Now everything is checked and valid

        elif value_option == &#39;fixed-topology&#39;:
            logging.info(&#39;Fixed topology initialization&#39;)
            if not pl.isstr(value):
                raise TypeError(&#39;`value` ({}) must be a str&#39;.format(value))

            CHAIN2 = pl.inference.BaseMCMC.load(value)
            CLUSTERING2 = CHAIN2.graph[STRNAMES.CLUSTERING_OBJ]
            ASVS2 = CHAIN2.graph.data.asvs
            asvs_curr = self.G.data.asvs
            for asv in ASVS2:
                if asv.name not in asvs_curr:
                    raise ValueError(&#39;Cannot perform fixed topology because the ASV {} in &#39; \
                        &#39;the passed in clustering is not in this clustering: {}&#39;.format(
                            asv.name, asvs_curr.names.order))
            for asv in asvs_curr:
                if asv.name not in ASVS2:
                    raise ValueError(&#39;Cannot perform fixed topology because the ASV {} in &#39; \
                        &#39;the current clustering is not in the passed in clustering: {}&#39;.format(
                            asv.name, ASVS2.names.order))

            # Get the most likely cluster configuration and set as the value for the passed in cluster
            ret = generate_cluster_assignments_posthoc(CLUSTERING2, n_clusters=&#39;mode&#39;, set_as_value=True)
            ca = {}
            for aidx, cidx in enumerate(ret):
                if cidx not in ca:
                    ca[cidx] = []
                ca[cidx].append(aidx)
            ret = []
            for v in ca.values():
                ret.append(v)
            CLUSTERING2.from_array(ret)
            logging.info(&#39;Clustering set to:\n{}&#39;.format(str(CLUSTERING2)))

            # Set the passed in cluster assignment as the current cluster assignment
            # Need to be careful because the indices of the ASVs might not line up
            clusters = []
            for cluster in CLUSTERING2:
                anames = [asvs_curr[ASVS2.names.order[aidx]].name for aidx in cluster.members]
                aidxs = [asvs_curr[aname].idx for aname in anames]
                clusters.append(aidxs)

        elif value_option == &#39;no-clusters&#39;:
            clusters = []
            for oidx in range(len(asvs)):
                clusters.append([oidx])

        elif value_option == &#39;random&#39;:
            clusters = {}
            for oidx in range(len(asvs)):
                idx = npr.choice(n_clusters)
                if idx in clusters:
                    clusters[idx].append(oidx)
                else:
                    clusters[idx] = [oidx]
            c = []
            for cid in clusters.keys():
                c.append(clusters[cid])
            clusters = c

        elif value_option == &#39;taxonomy&#39;:
            # Create an affinity matrix, we can precompute the self-similarity to 1
            M = np.diag(np.ones(len(asvs), dtype=float))
            for i, oid1 in enumerate(asvs.ids.order):
                for j, oid2 in enumerate(asvs.ids.order):
                    if i == j:
                        continue
                    M[i,j] = asvs.taxonomic_similarity(oid1=oid1, oid2=oid2)

            c = AgglomerativeClustering(
                n_clusters=n_clusters,
                affinity=&#39;precomputed&#39;,
                linkage=&#39;complete&#39;)
            assignments = c.fit_predict(1-M)

            # Convert assignments into clusters
            clusters = {}
            for oidx,cidx in enumerate(assignments):
                if cidx not in clusters:
                    clusters[cidx] = []
                clusters[cidx].append(oidx)
            clusters = [val for val in clusters.values()]

        elif value_option == &#39;sequence&#39;:
            import diversity

            logging.info(&#39;Making affinity matrix from sequences&#39;)
            evenness = np.diag(np.ones(len(self.G.data.asvs), dtype=float))

            for i in range(len(self.G.data.asvs)):
                for j in range(len(self.G.data.asvs)):
                    if j &lt;= i:
                        continue
                    # Subtract because we want to make a similarity matrix
                    dist = 1-diversity.beta.hamming(
                        list(self.G.data.asvs[i].sequence),
                        list(self.G.data.asvs[j].sequence))
                    evenness[i,j] = dist
                    evenness[j,i] = dist

            c = AgglomerativeClustering(
                n_clusters=n_clusters,
                affinity=&#39;precomputed&#39;,
                linkage=&#39;average&#39;)
            assignments = c.fit_predict(evenness)
            clusters = {}
            for oidx,cidx in enumerate(assignments):
                if cidx not in clusters:
                    clusters[cidx] = []
                clusters[cidx].append(oidx)
            clusters = [val for val in clusters.values()]

        elif value_option == &#39;spearman&#39;:
            # Use spearman correlation to create a distance matrix
            # Use agglomerative clustering to make the clusters based
            # on distance matrix (distance = 1 - pearson(x,y))
            dm = np.zeros(shape=(len(asvs), len(asvs)))
            data = []
            for ridx in range(self.G.data.n_replicates):
                data.append(self.G.data.abs_data[ridx])
            data = np.hstack(data)
            for i in range(len(asvs)):
                for j in range(i+1):
                    distance = (1 - scipy.stats.spearmanr(data[i, :], data[j, :])[0])/2
                    dm[i,j] = distance
                    dm[j,i] = distance

            c = AgglomerativeClustering(
                n_clusters=n_clusters,
                affinity=&#39;precomputed&#39;,
                linkage=&#39;complete&#39;)
            assignments = c.fit_predict(dm)

            # convert into clusters
            clusters = {}
            for oidx, cidx in enumerate(assignments):
                if cidx not in clusters:
                    clusters[cidx] = []
                clusters[cidx].append(oidx)
            clusters = [val for val in clusters.values()]

        elif value_option == &#39;phylogeny&#39;:
            raise NotImplementedError(&#39;`phylogeny` not implemented yet&#39;)

        else:
            raise ValueError(&#39;`value_option` &#34;{}&#34; not recognized&#39;.format(value_option))

        # Move all the ASVs into their assigned clusters
        for cluster in clusters:
            cid = None
            for oidx in cluster:
                if cid is None:
                    # make new cluster
                    cid = self.clustering.make_new_cluster_with(idx=oidx)
                else:
                    self.clustering.move_item(idx=oidx, cid=cid)
        logging.info(&#39;Cluster Assingments initialization results:\n{}&#39;.format(
            str(self.clustering)))
        self._there_are_perturbations = self.G.perturbations is not None

        # Initialize the multiprocessors if necessary
        if self.mp is not None:
            if not pl.isstr(self.mp):
                raise TypeError(&#39;`mp` ({}) must be a str&#39;.format(type(self.mp)))
            if &#39;full&#39; in self.mp:
                n_cpus = self.mp.split(&#39;-&#39;)[1]
                if n_cpus == &#39;auto&#39;:
                    self.n_cpus = psutil.cpu_count(logical=False)
                else:
                    try:
                        self.n_cpus = int(n_cpus)
                    except:
                        raise ValueError(&#39;`mp` ({}) not recognized&#39;.format(self.mp))
                self.pool = pl.multiprocessing.PersistentPool(ptype=&#39;dasw&#39;, G=self.G)
            elif self.mp == &#39;debug&#39;:
                self.pool = None
            else:
                raise ValueError(&#39;`mp` ({}) not recognized&#39;.format(self.mp))
        else:
            self.pool = None

        self.ndts_bias = []
        self.n_asvs = len(self.G.data.asvs)
        self.n_replicates = self.G.data.n_replicates
        self.n_dts_for_replicate = self.G.data.n_dts_for_replicate
        self.total_dts = np.sum(self.n_dts_for_replicate)
        for ridx in range(self.G.data.n_replicates):
            self.ndts_bias.append(
                np.arange(0, self.G.data.n_dts_for_replicate[ridx] * self.n_asvs, self.n_asvs))
        self.replicate_bias = np.zeros(self.n_replicates, dtype=int)
        for ridx in range(1, self.n_replicates):
            self.replicate_bias[ridx] = self.replicate_bias[ridx-1] + \
                self.n_asvs * self.n_dts_for_replicate[ridx - 1]

    def visualize_posterior(self, basepath, f, section=&#39;posterior&#39;, asv_formatter=&#39;%(name)s&#39;,
        yticklabels=&#39;%(name)s %(index)s&#39;, xticklabels=&#39;%(index)s&#39;):
        &#39;&#39;&#39;Render the traces in the folder `basepath` and write the 
        learned values to the file `f`.

        Parameters
        ----------
        basepath : str
            This is the loction to write the files to
        f : _io.TextIOWrapper
            File that we are writing the values to
        section : str
            Section of the trace to compute on. Options:
                &#39;posterior&#39; : posterior samples
                &#39;burnin&#39; : burn-in samples
                &#39;entire&#39; : both burn-in and posterior samples

        Returns
        -------
        _io.TextIOWrapper
        &#39;&#39;&#39;
        asvs = self.G.data.asvs
        f.write(&#39;\n\n###################################\n&#39;)
        f.write(self.name)
        f.write(&#39;\n###################################\n&#39;)
        if not self.G.inference.is_in_inference_order(self):
            f.write(&#39;`{}` not learned. These were the fixed cluster assignments\n&#39;.format(self.name))
            for cidx, cluster in enumerate(self.clustering):
                f.write(&#39;Cluster {}:\n&#39;.format(cidx+1))
                for aidx in cluster.members:
                    label = pl.asvname_formatter(format=asv_formatter, asv=asvs[aidx], asvs=asvs)
                    f.write(&#39;\t- {}\n&#39;.format(label))

            return f

        # Coclusters
        cocluster_trace = self.clustering.coclusters.get_trace_from_disk(section=section)
        coclusters = pl.variables.summary(cocluster_trace, section=section)[&#39;mean&#39;]
        for i in range(coclusters.shape[0]):
            coclusters[i,i] = np.nan

        visualization.render_cocluster_proportions(
            coclusters=coclusters, asvs=self.G.data.asvs, clustering=self.clustering,
            yticklabels=yticklabels, include_tick_marks=False, xticklabels=xticklabels,
            title=&#39;Cluster Assignments&#39;)
        fig = plt.gcf()
        fig.tight_layout()
        plt.savefig(basepath + &#39;coclusters.pdf&#39;)
        plt.close()

        # N clusters
        visualization.render_trace(var=self.clustering.n_clusters, plt_type=&#39;both&#39;, 
            section=section, include_burnin=True, rasterized=True)
        fig = plt.gcf()
        fig.suptitle(&#39;Number of Clusters&#39;)
        plt.savefig(basepath + &#39;n_clusters.pdf&#39;)
        plt.close()

        ca = generate_cluster_assignments_posthoc(clustering=self.clustering, n_clusters=&#39;mode&#39;, 
            section=section)
        cluster_assignments = {}
        for idx, assignment in enumerate(ca):
            if assignment in cluster_assignments:
                cluster_assignments[assignment].append(idx)
            else:
                cluster_assignments[assignment] = [idx]

        f.write(&#39;Mode number of clusters: {}\n&#39;.format(len(self.clustering)))
        for idx,lst in enumerate(cluster_assignments.values()):
            f.write(&#39;Cluster {} - Size {}\n&#39;.format(idx+1, len(lst)))
            for oidx in lst:
                # Get rid of index because that does not really make sense here
                label = pl.asvname_formatter(format=asv_formatter, asv=asvs[oidx], asvs=asvs)
                f.write(&#39;\t- {}\n&#39;.format(label))
        
        return f

    def set_trace(self):
        self.clustering.set_trace()

    def add_trace(self):
        self.clustering.add_trace()

    def add_init_value(self):
        self.clustering.add_init_value()

    def kill(self):
        if pl.ispersistentpool(self.pool):
            # For pylab multiprocessing, explicitly kill them
            self.pool.kill()
        return

    # Update super safe - meant to be used during debugging
    # =====================================================
    def update_slow(self):
        &#39;&#39;&#39; This is updating the new cluster. Depending on the iteration you do
        either split-merge Metropolis-Hasting update or a regular Gibbs update. To
        get highest mixing we alternate between each.
        &#39;&#39;&#39;
        if self.clustering.n_clusters.sample_iter &lt; self.delay:
            return

        if self.clustering.n_clusters.sample_iter % self.run_every_n_iterations != 0:
           return

        # print(&#39;in clustering&#39;)
        start_time = time.time()
        oidxs = npr.permutation(np.arange(len(self.G.data.asvs)))

        for oidx in oidxs:
            self.gibbs_update_single_asv_slow(oidx=oidx)
        self._strtime = time.time() - start_time

    def gibbs_update_single_asv_slow(self, oidx):
        &#39;&#39;&#39;The update function is based off of Algorithm 8 in &#39;Markov Chain
        Sampling Methods for Dirichlet Process Mixture Models&#39; by Radford M.
        Neal, 2000.

        Calculate the marginal likelihood of the asv in every cluster
        and a new cluster then sample from `self.sample_categorical_log`
        to get the cluster assignment.

        Parameters
        ----------
        oidx : int
            ASV index that we are updating the cluster assignment of
        &#39;&#39;&#39;
        curr_cluster = self.clustering.idx2cid[oidx]
        concentration = self.concentration.value

        # start as a dictionary then send values to `sample_categorical_log`
        LOG_P = []
        LOG_KEYS = []

        # Calculate current cluster
        # =========================
        # If the element is already in its own cluster, use the new cluster case
        if self.clustering.clusters[curr_cluster].size == 1:
            a = np.log(concentration/self.m)
        else:
            a = np.log(self.clustering.clusters[curr_cluster].size - 1)
        LOG_P.append(a + self.calculate_marginal_loglikelihood_slow()[&#39;ret&#39;])
        LOG_KEYS.append(curr_cluster)

        # Calculate going to every other cluster
        # ======================================
        for cid in self.clustering.order:
            if curr_cluster == cid:
                continue

            # Move ASV and recompute the matrices
            self.clustering.move_item(idx=oidx,cid=cid)
            self.G[REPRNAMES.CLUSTER_INTERACTION_INDICATOR].update_cnt_indicators()
            self.G.data.design_matrices[REPRNAMES.CLUSTER_INTERACTION_VALUE].M.build()
            if self._there_are_perturbations:
                self.G.data.design_matrices[REPRNAMES.PERT_VALUE].M.build()

            LOG_P.append(np.log(self.clustering.clusters[cid].size - 1) + \
                self.calculate_marginal_loglikelihood_slow()[&#39;ret&#39;])
            LOG_KEYS.append(cid)


        # Calculate new cluster
        # =====================
        cid=self.clustering.make_new_cluster_with(idx=oidx)
        self.G[REPRNAMES.CLUSTER_INTERACTION_INDICATOR].update_cnt_indicators()
        self.G.data.design_matrices[REPRNAMES.CLUSTER_INTERACTION_VALUE].M.build()
        if self._there_are_perturbations:
            self.G.data.design_matrices[REPRNAMES.PERT_VALUE].M.build()
        
        LOG_KEYS.append(cid)
        LOG_P.append(np.log(concentration/self.m) + \
            self.calculate_marginal_loglikelihood_slow()[&#39;ret&#39;])

        # Sample the assignment
        # =====================
        idx = sample_categorical_log(LOG_P)
        assigned_cid = LOG_KEYS[idx]
        curr_clus = self.clustering.idx2cid[oidx]

        if assigned_cid != curr_clus:
            self.clustering.move_item(idx=oidx,cid=assigned_cid)
            self.G[REPRNAMES.CLUSTER_INTERACTION_INDICATOR].update_cnt_indicators()

            # Change the mixing matrix for the interactions and (potentially) perturbations
            self.G.data.design_matrices[REPRNAMES.CLUSTER_INTERACTION_VALUE].M.build()
            if self._there_are_perturbations:
                self.G.data.design_matrices[REPRNAMES.PERT_VALUE].M.build()

    def calculate_marginal_loglikelihood_slow(self):
        &#39;&#39;&#39;Marginalizes out the interactions and the perturbations
        &#39;&#39;&#39;
        # Build the parameters
        # ====================
        self.G[REPRNAMES.CLUSTER_INTERACTION_INDICATOR].update_cnt_indicators()
        lhs = [REPRNAMES.GROWTH_VALUE, REPRNAMES.SELF_INTERACTION_VALUE]
        if self._there_are_perturbations:
            rhs = [REPRNAMES.PERT_VALUE, REPRNAMES.CLUSTER_INTERACTION_VALUE]
        else:
            rhs = [REPRNAMES.CLUSTER_INTERACTION_VALUE]

        # reconstruct the X matrices
        for v in rhs:
            self.G.data.design_matrices[v].M.build()
        
        y = self.G.data.construct_lhs(lhs, 
            kwargs_dict={REPRNAMES.GROWTH_VALUE:{&#39;with_perturbations&#39;: False}})
        X = self.G.data.construct_rhs(keys=rhs, toarray=True)
        process_prec = self.G[REPRNAMES.PROCESSVAR].build_matrix(cov=False, sparse=False)
        prior_prec = build_prior_covariance(G=self.G, cov=False, order=rhs, sparse=False)
        prior_var = build_prior_covariance(G=self.G, cov=True, order=rhs, sparse=False)
        prior_mean = build_prior_mean(G=self.G, order=rhs, shape=(-1,1))

        # If nothing is on, return 0
        if X.shape[1] == 0:
            return {
                &#39;a&#39;: 0,
                &#39;beta_prec&#39;: 0,
                &#39;process_prec&#39;: prior_prec,
                &#39;ret&#39;: 0,
                &#39;beta_logdet&#39;: 0,
                &#39;priorvar_logdet&#39;: 0,
                &#39;bEb&#39;: 0,
                &#39;bEbprior&#39;: 0}

        # Calculate the marginalization
        # =============================
        beta_prec = X.T @ process_prec @ X + prior_prec
        beta_cov = pinv(beta_prec, self)
        beta_mean = beta_cov @ ( X.T @ process_prec @ y + prior_prec @ prior_mean )
        beta_mean = np.asarray(beta_mean).reshape(-1,1)

        try:
            beta_logdet = log_det(beta_cov, self)
        except:
            logging.critical(&#39;Crashed in log_det&#39;)
            logging.critical(&#39;beta_cov:\n{}&#39;.format(beta_cov))
            logging.critical(&#39;prior_prec\n{}&#39;.format(prior_prec))
            raise
        priorvar_logdet = log_det(prior_var, self)
        ll2 = 0.5 * (beta_logdet - priorvar_logdet)

        bEbprior = np.asarray(prior_mean.T @ prior_prec @ prior_mean)[0,0]
        bEb = np.asarray(beta_mean.T @ beta_prec @ beta_mean)[0,0]
        ll3 = 0.5 * (bEb  - bEbprior)

        # print(&#39;prior_prec truth:\n&#39;, prior_prec)

        return {
            &#39;a&#39;: X.T @ process_prec,
            &#39;beta_prec&#39;: beta_prec,
            &#39;process_prec&#39;: prior_prec,
            &#39;ret&#39;: ll2+ll3,
            &#39;beta_logdet&#39;: beta_logdet,
            &#39;priorvar_logdet&#39;: priorvar_logdet,
            &#39;bEb&#39;: bEb,
            &#39;bEbprior&#39;: bEbprior}

    # Update regular - meant to be used during inference
    # ==================================================
    # @profile
    def update_slow_fast(self):
        &#39;&#39;&#39;Much faster than `update_slow`
        &#39;&#39;&#39;

        if self.clustering.n_clusters.sample_iter &lt; self.delay:
            return

        if self.clustering.n_clusters.sample_iter % self.run_every_n_iterations != 0:
           return

        start_time = time.time()

        self.process_prec = self.G[REPRNAMES.PROCESSVAR].prec.ravel() #.build_matrix(cov=False, sparse=False)
        self.process_prec_matrix = self.G[REPRNAMES.PROCESSVAR].build_matrix(sparse=True, cov=False)
        lhs = [REPRNAMES.GROWTH_VALUE, REPRNAMES.SELF_INTERACTION_VALUE]
        self.y = self.G.data.construct_lhs(lhs, 
            kwargs_dict={REPRNAMES.GROWTH_VALUE:{&#39;with_perturbations&#39;: False}})

        oidxs = npr.permutation(np.arange(len(self.G.data.asvs)))
        iii = 0
        for oidx in oidxs:
            logging.info(&#39;{}/{}: {}&#39;.format(iii, len(oidxs), oidx))
            self.gibbs_update_single_asv_slow_fast(oidx=oidx)
            iii += 1
        self._strtime = time.time() - start_time
    
    # @profile
    def gibbs_update_single_asv_slow_fast(self, oidx):
        &#39;&#39;&#39;The update function is based off of Algorithm 8 in &#39;Markov Chain
        Sampling Methods for Dirichlet Process Mixture Models&#39; by Radford M.
        Neal, 2000.

        Calculate the marginal likelihood of the asv in every cluster
        and a new cluster then sample from `self.sample_categorical_log`
        to get the cluster assignment.

        Parameters
        ----------
        oidx : int
            ASV index that we are updating the cluster assignment of
        &#39;&#39;&#39;
        curr_cluster = self.clustering.idx2cid[oidx]
        concentration = self.concentration.value

        # start as a dictionary then send values to `sample_categorical_log`
        LOG_P = []
        LOG_KEYS = []

        # Calculate current cluster
        # =========================
        # If the element is already in its own cluster, use the new cluster case
        if self.clustering.clusters[curr_cluster].size == 1:
            a = np.log(concentration/self.m)
        else:
            a = np.log(self.clustering.clusters[curr_cluster].size - 1)
        LOG_P.append(a + self.calculate_marginal_loglikelihood_slow_fast_sparse())
        LOG_KEYS.append(curr_cluster)

        # Calculate going to every other cluster
        # ======================================
        for cid in self.clustering.order:
            if curr_cluster == cid:
                continue

            # Move ASV and recompute the matrices
            self.clustering.move_item(idx=oidx,cid=cid)
            self.G[REPRNAMES.CLUSTER_INTERACTION_INDICATOR].update_cnt_indicators()
            self.G.data.design_matrices[REPRNAMES.CLUSTER_INTERACTION_VALUE].M.build()
            if self._there_are_perturbations:
                self.G.data.design_matrices[REPRNAMES.PERT_VALUE].M.build()

            LOG_P.append(np.log(self.clustering.clusters[cid].size - 1) + \
                self.calculate_marginal_loglikelihood_slow_fast_sparse())
            LOG_KEYS.append(cid)


        # Calculate new cluster
        # =====================
        cid=self.clustering.make_new_cluster_with(idx=oidx)
        self.G[REPRNAMES.CLUSTER_INTERACTION_INDICATOR].update_cnt_indicators()
        self.G.data.design_matrices[REPRNAMES.CLUSTER_INTERACTION_VALUE].M.build()
        if self._there_are_perturbations:
            self.G.data.design_matrices[REPRNAMES.PERT_VALUE].M.build()
        
        LOG_KEYS.append(cid)
        LOG_P.append(np.log(concentration/self.m) + \
            self.calculate_marginal_loglikelihood_slow_fast_sparse())

        # Sample the assignment
        # =====================
        idx = sample_categorical_log(LOG_P)
        assigned_cid = LOG_KEYS[idx]
        curr_clus = self.clustering.idx2cid[oidx]

        if assigned_cid != curr_clus:
            self.clustering.move_item(idx=oidx,cid=assigned_cid)
            self.G[REPRNAMES.CLUSTER_INTERACTION_INDICATOR].update_cnt_indicators()

            # Change the mixing matrix for the interactions and (potentially) perturbations
            self.G.data.design_matrices[REPRNAMES.CLUSTER_INTERACTION_VALUE].M.build()
            if self._there_are_perturbations:
                self.G.data.design_matrices[REPRNAMES.PERT_VALUE].M.build()

    # @profile
    def calculate_marginal_loglikelihood_slow_fast(self):
        &#39;&#39;&#39;Marginalizes out the interactions and the perturbations
        &#39;&#39;&#39;
        # Build the parameters
        # ====================
        if self._there_are_perturbations:
            rhs = [REPRNAMES.PERT_VALUE, REPRNAMES.CLUSTER_INTERACTION_VALUE]
        else:
            rhs = [REPRNAMES.CLUSTER_INTERACTION_VALUE]
        
        
        y = self.y
        X = self.G.data.construct_rhs(keys=rhs, toarray=True)

        process_prec = self.process_prec
        prior_prec = build_prior_covariance(G=self.G, cov=False, order=rhs, sparse=False)
        prior_prec_diag = np.diag(prior_prec)
        prior_var = build_prior_covariance(G=self.G, cov=True, order=rhs, sparse=False)
        prior_mean = build_prior_mean(G=self.G, order=rhs, shape=(-1,1))

        # Calculate the marginalization
        # =============================
        a = X.T * process_prec

        beta_prec = a @ X + prior_prec
        beta_cov = pinv(beta_prec, self)
        beta_mean = beta_cov @ ( a @ y + prior_prec @ prior_mean )
        beta_mean = np.asarray(beta_mean).reshape(-1,1)

        try:
            beta_logdet = log_det(beta_cov, self)
        except:
            logging.critical(&#39;Crashed in log_det&#39;)
            logging.critical(&#39;beta_cov:\n{}&#39;.format(beta_cov))
            logging.critical(&#39;prior_prec\n{}&#39;.format(prior_prec))
            raise
        priorvar_logdet = log_det(prior_var, self)
        ll2 = 0.5 * (beta_logdet - priorvar_logdet)

        a = np.sum((prior_mean.ravel() ** 2) *prior_prec_diag)
        # np.asarray(prior_mean.T @ prior_prec @ prior_mean)[0,0]
        b = np.asarray(beta_mean.T @ beta_prec @ beta_mean)[0,0]
        ll3 = -0.5 * (a  - b)

        return ll2+ll3

    # @profile
    def calculate_marginal_loglikelihood_slow_fast_sparse(self):
        &#39;&#39;&#39;Marginalizes out the interactions and the perturbations
        &#39;&#39;&#39;
        # Build the parameters
        # ====================
        if self._there_are_perturbations:
            rhs = [REPRNAMES.PERT_VALUE, REPRNAMES.CLUSTER_INTERACTION_VALUE]
        else:
            rhs = [REPRNAMES.CLUSTER_INTERACTION_VALUE]
        
        
        y = self.y
        X = self.G.data.construct_rhs(keys=rhs, toarray=False)

        process_prec = self.process_prec_matrix
        prior_prec = build_prior_covariance(G=self.G, cov=False, order=rhs, sparse=True)  
        prior_prec_diag = build_prior_covariance(G=self.G, cov=False, order=rhs, diag=True)        
        prior_var = build_prior_covariance(G=self.G, cov=True, order=rhs, sparse=True)
        prior_mean = build_prior_mean(G=self.G, order=rhs, shape=(-1,1))

        # Calculate the marginalization
        # =============================

        # print(&#39;X&#39;)
        # print(type(X))
        # print(X.shape)

        # print(&#39;process_prec&#39;)
        # print(type(process_prec))
        # print(process_prec.shape)

        # print(&#39;prior_prec&#39;)
        # print(type(prior_prec))
        # print(prior_prec.shape)

        # print(&#39;prior mean&#39;)
        # print(type(prior_mean))
        # print(prior_mean.shape)

        a = X.T.dot(process_prec)
        beta_prec = a.dot(X) + prior_prec
        beta_cov = pinv(beta_prec, self)
        beta_mean = beta_cov @ ( a.dot(y) + prior_prec.dot(prior_mean))
        beta_mean = np.asarray(beta_mean).reshape(-1,1)

        try:
            beta_logdet = log_det(beta_cov, self)
        except:
            logging.critical(&#39;Crashed in log_det&#39;)
            logging.critical(&#39;beta_cov:\n{}&#39;.format(beta_cov))
            logging.critical(&#39;prior_prec\n{}&#39;.format(prior_prec))
            raise
        priorvar_logdet = log_det(prior_var, self)
        ll2 = 0.5 * (beta_logdet - priorvar_logdet)

        a = np.sum((prior_mean.ravel() ** 2) *prior_prec_diag)
        # np.asarray(prior_mean.T @ prior_prec @ prior_mean)[0,0]

        # print(&#39;beta_prec.shape&#39;, beta_prec.shape)
        # print(&#39;beta_mean.shape&#39;, beta_mean.shape)

        b = np.asarray(beta_mean.T @ beta_prec.dot(beta_mean))[0,0]
        ll3 = -0.5 * (a  - b)

        return ll2+ll3

    # Update MP - meant to be used during inference
    # =============================================
    def update_mp(self):
        &#39;&#39;&#39;Implements `update_slow` but parallelizes calculating the likelihood
        of being in a cluster. NOTE that this does not parallelize on the ASV level.

        On the first gibb step with initialize the workers that we implement with DASW (
        different arguments, single worker). For more information what this means look
        at pylab.multiprocessing documentation.

        If we initialized our pool as a pylab.multiprocessing.PersistentPool, then we 
        multiprocess the likelihood calculations for each asv. If we didnt then this 
        implementation has the same performance as `ClusterAssignments.update_slow_fast`.
        &#39;&#39;&#39;
        if self.G.data.zero_inflation_transition_policy is not None:
            raise NotImplementedError(&#39;Multiprocessing for zero inflation data is not implemented yet.&#39; \
                &#39; Use `mp=None`&#39;)
        DMI = self.G.data.design_matrices[REPRNAMES.CLUSTER_INTERACTION_VALUE]
        DMP = self.G.data.design_matrices[REPRNAMES.PERT_VALUE]
        if self.clustering.n_clusters.sample_iter == 0 or self.pool == []:
            kwargs = {
                &#39;n_asvs&#39;: len(self.G.data.asvs),
                &#39;total_n_dts_per_asv&#39;: self.G.data.total_n_dts_per_asv,
                &#39;n_replicates&#39;: self.G.data.n_replicates,
                &#39;n_dts_for_replicate&#39;: self.G.data.n_dts_for_replicate,
                &#39;there_are_perturbations&#39;: self._there_are_perturbations,
                &#39;keypair2col_interactions&#39;: DMI.M.keypair2col,
                &#39;keypair2col_perturbations&#39;: DMP.M.keypair2col,
                &#39;n_perturbations&#39;: len(self.G.perturbations) if self._there_are_perturbations else None,
                &#39;base_Xrows&#39;: DMI.base.rows,
                &#39;base_Xcols&#39;: DMI.base.cols,
                &#39;base_Xshape&#39;: DMI.base.shape,
                &#39;base_Xpertrows&#39;: DMP.base.rows,
                &#39;base_Xpertcols&#39;: DMP.base.cols,
                &#39;base_Xpertshape&#39;: DMP.base.shape,
                &#39;n_rowsM&#39;: DMI.M.n_rows,
                &#39;n_rowsMpert&#39;: DMP.M.n_rows}

            if pl.ispersistentpool(self.pool):
                for _ in range(self.n_cpus):
                    self.pool.add_worker(SingleClusterFullParallelization(**kwargs))
            else:
                self.pool = SingleClusterFullParallelization(**kwargs)

        if self.clustering.n_clusters.sample_iter &lt; self.delay:
            return

        if self.clustering.n_clusters.sample_iter % self.run_every_n_iterations != 0:
            return

        # Send in arguments for the start of the gibbs step
        start_time = time.time()
        base_Xdata = DMI.base.data
        self.concentration = self.G[REPRNAMES.CONCENTRATION].value
        y = self.G.data.construct_lhs(keys=[REPRNAMES.GROWTH_VALUE, REPRNAMES.SELF_INTERACTION_VALUE],
            kwargs_dict={REPRNAMES.GROWTH_VALUE: {&#39;with_perturbations&#39;:False}})
        prior_var_interactions = self.G[REPRNAMES.PRIOR_VAR_INTERACTIONS].value
        prior_mean_interactions = self.G[REPRNAMES.PRIOR_MEAN_INTERACTIONS].value
        process_prec_diag = self.G[REPRNAMES.PROCESSVAR].prec

        if self._there_are_perturbations:
            prior_var_pert = self.G[REPRNAMES.PRIOR_VAR_PERT].get_single_value_of_perts()
            prior_mean_pert = self.G[REPRNAMES.PRIOR_MEAN_PERT].get_single_value_of_perts()
            base_Xpertdata = DMP.base.data
        else:
            prior_var_pert = None
            prior_mean_pert = None
            base_Xpertdata = None

        kwargs = {
            &#39;base_Xdata&#39;: base_Xdata,
            &#39;base_Xpertdata&#39;: base_Xpertdata,
            &#39;concentration&#39;: self.concentration,
            &#39;m&#39;: self.m,
            &#39;y&#39;: y,
            &#39;process_prec_diag&#39;: process_prec_diag,
            &#39;prior_var_interactions&#39;: prior_var_interactions,
            &#39;prior_var_pert&#39;: prior_var_pert,
            &#39;prior_mean_interactions&#39;: prior_mean_interactions,
            &#39;prior_mean_pert&#39;: prior_mean_pert}
        
        if pl.ispersistentpool(self.pool):
            self.pool.map(&#39;initialize_gibbs&#39;, [kwargs]*self.pool.num_workers)
        else:
            self.pool.initialize_gibbs(**kwargs)

        oidxs = npr.permutation(np.arange(len(self.G.data.asvs)))
        for iii, oidx in enumerate(oidxs):
            logging.info(&#39;{}/{} - {}&#39;.format(iii, len(self.G.data.asvs), oidx))
            self.oidx = oidx
            self.gibbs_update_single_asv_parallel()

        self._strtime = time.time() - start_time

    def gibbs_update_single_asv_parallel(self):
        &#39;&#39;&#39;Update for a single asvs
        &#39;&#39;&#39;
        self.original_cluster = self.clustering.idx2cid[self.oidx]
        self.curr_cluster = self.original_cluster

        interactions = self.G[REPRNAMES.INTERACTIONS_OBJ]
        interaction_on_idxs = interactions.get_indicators(return_idxs=True)
        if self._there_are_perturbations:
            perturbation_on_idxs = [p.indicator.cluster_arg_array() for p in self.G.perturbations]
        else:
            perturbation_on_idxs = None

        # # Send topology parameters if the topology wont change
        # TODO: this works on windows but not on the cluster when dispatching?
        # if self.clustering.clusters[self.original_cluster].size &gt; 1:
        #     use_saved_params = True
        #     kwargs = {
        #         &#39;interaction_on_idxs&#39;: interaction_on_idxs,
        #         &#39;perturbation_on_idxs&#39;: perturbation_on_idxs}
        #     if pl.ispersistentpool(self.pool):
        #         self.pool.map(&#39;initialize_oidx&#39;, [kwargs]*self.pool.num_workers)
        #     else:
        #         use_saved_params = False
        # else:
        #     use_saved_params = False
        use_saved_params = False

        if pl.ispersistentpool(self.pool):
            self.pool.staged_map_start(&#39;run&#39;)
        else:
            notpool_ret = []

        # Get the likelihood of the current configuration
        if self.clustering.clusters[self.original_cluster].size == 1:
            log_mult_factor = math.log(self.concentration/self.m)
        else:
            log_mult_factor = math.log(self.clustering.clusters[self.original_cluster].size - 1)

        if use_saved_params and pl.ispersistentpool(self.pool):
            interaction_on_idxs = None
            perturbation_on_idxs = None

        cluster_config = np.asarray([self.clustering.cid2cidx[self.clustering.idx2cid[i]] \
            for i in range(len(self.G.data.asvs))])

        kwargs = {
            &#39;interaction_on_idxs&#39;: interaction_on_idxs,
            &#39;perturbation_on_idxs&#39;: perturbation_on_idxs,
            &#39;cluster_config&#39;: cluster_config,
            &#39;log_mult_factor&#39;: log_mult_factor,
            &#39;cid&#39;: self.original_cluster,
            &#39;use_saved_params&#39;: use_saved_params}

        if pl.ispersistentpool(self.pool):
            self.pool.staged_map_put(kwargs)
        else:
            notpool_ret.append(self.pool.run(**kwargs))

        # Check every cluster
        for cid in self.clustering.order:
            if cid == self.original_cluster:
                continue
            self.clustering.move_item(idx=self.oidx, cid=cid)
            self.curr_cluster = cid

            if not use_saved_params:
                interaction_on_idxs = interactions.get_indicators(return_idxs=True)
                if self._there_are_perturbations:
                    perturbation_on_idxs = [p.indicator.cluster_arg_array() for p in self.G.perturbations]
                else:
                    perturbation_on_idxs = None

            cluster_config = np.asarray([self.clustering.cid2cidx[self.clustering.idx2cid[i]] \
                for i in range(len(self.G.data.asvs))])
            log_mult_factor = np.log(self.clustering.clusters[self.curr_cluster].size - 1)

            kwargs = {
                &#39;interaction_on_idxs&#39;: interaction_on_idxs,
                &#39;perturbation_on_idxs&#39;: perturbation_on_idxs,
                &#39;cluster_config&#39;: cluster_config,
                &#39;log_mult_factor&#39;: log_mult_factor,
                &#39;cid&#39;: self.curr_cluster,
                &#39;use_saved_params&#39;: use_saved_params}

            if pl.ispersistentpool(self.pool):
                self.pool.staged_map_put(kwargs)
            else:
                notpool_ret.append(self.pool.run(**kwargs))

        # Make a new cluster
        self.curr_cluster = self.clustering.make_new_cluster_with(idx=self.oidx)
        cluster_config = np.asarray([self.clustering.cid2cidx[self.clustering.idx2cid[i]] \
            for i in range(len(self.G.data.asvs))])
        interaction_on_idxs = interactions.get_indicators(return_idxs=True)
        if self._there_are_perturbations:
            perturbation_on_idxs = [p.indicator.cluster_arg_array() for p in self.G.perturbations]
        else:
            perturbation_on_idxs = None
        log_mult_factor = np.log(self.concentration/self.m)

        kwargs = {
            &#39;interaction_on_idxs&#39;: interaction_on_idxs,
            &#39;perturbation_on_idxs&#39;: perturbation_on_idxs,
            &#39;cluster_config&#39;: cluster_config,
            &#39;log_mult_factor&#39;: log_mult_factor,
            &#39;cid&#39;: self.curr_cluster,
            &#39;use_saved_params&#39;: False}

        # Put the values and get if necessary
        KEYS = []
        LOG_P = []
        if pl.ispersistentpool(self.pool):
            self.pool.staged_map_put(kwargs)
            ret = self.pool.staged_map_get()
        else:
            notpool_ret.append(self.pool.run(**kwargs))
            ret = notpool_ret
        for c, p in ret:
            KEYS.append(c)
            LOG_P.append(p)

        idx = sample_categorical_log(LOG_P)
        assigned_cid = KEYS[idx]

        if assigned_cid != self.original_cluster:
            logging.info(&#39;cluster changed&#39;)

        self.clustering.move_item(idx=self.oidx, cid=assigned_cid)

        self.G[REPRNAMES.CLUSTER_INTERACTION_INDICATOR].update_cnt_indicators()
        self.G.data.design_matrices[REPRNAMES.CLUSTER_INTERACTION_VALUE].M.build()
        if self._there_are_perturbations:
            self.G.data.design_matrices[REPRNAMES.PERT_VALUE].M.build()

        
class SingleClusterFullParallelization(pl.multiprocessing.PersistentWorker):
    &#39;&#39;&#39;Make the full parallelization
        - Mixture matricies for interactions and perturbations
        - calculating the marginalization for the sent in cluster

    Parameters
    ----------
    n_asvs : int
        Total number of OTUs
    total_n_dts_per_asv : int
        Total number of time changes for each OTU
    n_replicates : int
        Total number of replicates
    n_dts_for_replicate : np.ndarray
        Total number of time changes for each replicate
    there_are_perturbations : bool
        If True, there are perturbations
    keypair2col_interactions : np.ndarray
        These map the OTU indices of the pairs of OTUs to their column index
        in `big_X`
    keypair2col_perturbations : np.ndarray, None
        These map the OTU indices nad perturbation index to the column in
        `big_Xpert`. If there are no perturbations then this is None
    n_perturbations : int, None
        Number of perturbations. None if there are no perturbations
    base_Xrows, base_Xcols, base_Xpertrows, base_Xpertcols : np.ndarray
        These are the rows and columns necessary to build the interaction and perturbation 
        matrices, respectively. Whats passed in is the data vector and then we build
        it using sparse matrices
    n_rowsM, n_rowsMpert : int
        These are the number of rows for the mixing matrix for the interactions and
        perturbations respectively.
    &#39;&#39;&#39;
    def __init__(self, n_asvs, total_n_dts_per_asv, n_replicates, n_dts_for_replicate,
        there_are_perturbations, keypair2col_interactions, keypair2col_perturbations,
        n_perturbations, base_Xrows, base_Xcols, base_Xshape, base_Xpertrows, base_Xpertcols,
        base_Xpertshape, n_rowsM, n_rowsMpert):
        self.n_asvs = n_asvs
        self.total_n_dts_per_asv = total_n_dts_per_asv
        self.n_replicates = n_replicates
        self.n_dts_for_replicate = n_dts_for_replicate
        self.there_are_perturbations = there_are_perturbations
        self.keypair2col_interactions = keypair2col_interactions
        if self.there_are_perturbations:
            self.keypair2col_perturbations = keypair2col_perturbations
            self.n_perturbations = n_perturbations

        self.base_Xrows = base_Xrows
        self.base_Xcols = base_Xcols
        self.base_Xshape = base_Xshape
        self.base_Xpertrows = base_Xpertrows
        self.base_Xpertcols = base_Xpertcols
        self.base_Xpertshape = base_Xpertshape

        self.n_rowsM = n_rowsM
        self.n_rowsMpert = n_rowsMpert

    def initialize_gibbs(self, base_Xdata, base_Xpertdata, concentration, m, y,
        process_prec_diag, prior_var_interactions, prior_var_pert, 
        prior_mean_interactions, prior_mean_pert):
        &#39;&#39;&#39;Pass in the information that changes every Gibbs step

        Parameters
        ----------
        base_X : scipy.sparse.csc_matrix
            Sparse matrix for the interaction terms
        base_Xpert : scipy.sparse.csc_matrix, None
            Sparse matrix for the perturbation terms
            If None, there are no perturbations
        concentration : float
            This is the concentration of the system
        m : int
            This is the auxiliary variable for the marginalization
        y : np.ndarray
            This is the observation array
        process_prec_diag : np.ndarray
            This is the process precision diagonal
        &#39;&#39;&#39;
        self.base_X = scipy.sparse.coo_matrix(
            (base_Xdata,(self.base_Xrows,self.base_Xcols)),
            shape=self.base_Xshape).tocsc()
        
        self.concentration = concentration
        self.m = m
        self.y = y.reshape(-1,1)
        self.n_rows = len(y)
        self.n_cols_X = self.base_X.shape[1]
        self.prior_var_interactions = prior_var_interactions
        self.prior_prec_interactions = 1/prior_var_interactions
        self.prior_mean_interactions = prior_mean_interactions

        if self.there_are_perturbations:
            self.base_Xpert = scipy.sparse.coo_matrix(
                (base_Xpertdata,(self.base_Xpertrows,self.base_Xpertcols)),
                shape=self.base_Xpertshape).tocsc()
            self.prior_var_pert = prior_var_pert
            self.prior_prec_pert = 1/prior_var_pert
            self.prior_mean_pert = prior_mean_pert

        self.process_prec_matrix = scipy.sparse.dia_matrix(
            (process_prec_diag,[0]), shape=(len(process_prec_diag),len(process_prec_diag))).tocsc()

    def initialize_oidx(self, interaction_on_idxs, perturbation_on_idxs):
        &#39;&#39;&#39;Pass in the parameters that change for every OTU - potentially

        Parameters
        ----------
        
        &#39;&#39;&#39;
        self.saved_interaction_on_idxs = interaction_on_idxs
        self.saved_perturbation_on_idxs = perturbation_on_idxs

    # @profile
    def run(self, interaction_on_idxs, perturbation_on_idxs, cluster_config, log_mult_factor, cid, 
        use_saved_params):
        &#39;&#39;&#39;Pass in the parameters for the specific cluster assignment for
        the OTU and run the marginalization


        Parameters
        ----------
        interaction_on_idxs : np.array(int)
            An array of indices for the interactions that are on. Assumes that the
            clustering is in the order specified in `cluster_config`
        perturbation_on_idxs : list(np.ndarray(int)), None
            If there are perturbations, then we set the perturbation idxs on
            Each element in the list are the indices of that perturbation that are on
        cluster_config : list(list(int))
            This is the cluster configuration and in cluster order.
        log_mult_factor : float
            This is the log multiplication factor that we add onto the marginalization
        use_saved_params : bool
            If True, passed in `interaction_on_idxs` and `perturbation_on_idxs` are None
            and we can use `saved_interaction_on_idxs` and `saved_perturbation_on_idxs`
        &#39;&#39;&#39;
        if use_saved_params:
            interaction_on_idxs = self.saved_interaction_on_idxs
            perturbation_on_idxs = self.saved_perturbation_on_idxs

        # We need to make the arrays for interactions and perturbations
        self.set_clustering(cluster_config=cluster_config)
        Xinteractions = self.build_interactions_matrix(on_columns=interaction_on_idxs)

        if self.there_are_perturbations:
            Xperturbations = self.build_perturbations_matrix(on_columns=perturbation_on_idxs)
            X = scipy.sparse.hstack([Xperturbations, Xinteractions])
        else:
            X = Xinteractions
        self.X = X
        self.prior_mean = self.build_prior_mean(on_interactions=interaction_on_idxs,
            on_perturbations=perturbation_on_idxs)
        self.prior_cov, self.prior_prec, self.prior_prec_diag = self.build_prior_cov_and_prec_and_diag(
            on_interactions=interaction_on_idxs, on_perturbations=perturbation_on_idxs)
        
        return cid, self.calculate_marginal_loglikelihood_slow_fast_sparse()

    def set_clustering(self, cluster_config):
        self.clustering = CondensedClustering(oidx2cidx=cluster_config)
        self.iidx2cidxpair = np.zeros(shape=(len(self.clustering)*(len(self.clustering)-1), 2), 
            dtype=int)
        self.iidx2cidxpair = SingleClusterFullParallelization.make_iidx2cidxpair(
            ret=self.iidx2cidxpair,
            n_clusters=len(self.clustering))

    def build_interactions_matrix(self, on_columns):
        &#39;&#39;&#39;Build the interaction matrix

        First we make the rows and columns for the mixing matrix,
        then we multiple the base matrix and the mixing matrix.
        &#39;&#39;&#39;
        rows = []
        cols = []

        # c2ciidx = Cluster-to-Cluster Interaction InDeX
        c2ciidx = 0
        for ccc in on_columns:
            tcidx = self.iidx2cidxpair[ccc, 0]
            scidx = self.iidx2cidxpair[ccc, 1]
            
            smems = self.clustering.clusters[scidx]
            tmems = self.clustering.clusters[tcidx]
            
            a = np.zeros(len(smems)*len(tmems), dtype=int)
            rows.append(SingleClusterFullParallelization.get_indices(a,
                self.keypair2col_interactions, tmems, smems))
            cols.append(np.full(len(tmems)*len(smems), fill_value=c2ciidx))
            c2ciidx += 1

        rows = np.asarray(list(itertools.chain.from_iterable(rows)))
        cols = np.asarray(list(itertools.chain.from_iterable(cols)))
        data = np.ones(len(rows), dtype=int)

        # print(&#39;rows&#39;, rows)
        # print(cols)
        # print(data)
        # print((self.n_rowsM, c2ciidx))

        M = scipy.sparse.coo_matrix((data,(rows,cols)),
            shape=(self.n_rowsM, c2ciidx)).tocsc()
        ret = self.base_X @ M
        return ret

    def build_perturbations_matrix(self, on_columns):
        if not self.there_are_perturbations:
            raise ValueError(&#39;You should not be here&#39;)
        
        keypair2col = self.keypair2col_perturbations
        rows = []
        cols = []

        col = 0
        for pidx, pert_ind_idxs in enumerate(on_columns):
            for cidx in pert_ind_idxs:
                for oidx in self.clustering.clusters[cidx]:
                    rows.append(keypair2col[oidx, pidx])
                    cols.append(col)
                col += 1
        
        data = np.ones(len(rows), dtype=np.float64)
        M = scipy.sparse.coo_matrix((data,(rows,cols)),
            shape=(self.n_rowsMpert, col)).tocsc()
        ret = self.base_Xpert @ M
        return ret

    def build_prior_mean(self, on_interactions, on_perturbations):
        &#39;&#39;&#39;Build the prior mean array

        Perturbations go first and then interactions
        &#39;&#39;&#39;
        ret = []
        for pidx, pert in enumerate(on_perturbations):
            ret = np.append(ret, 
                np.full(len(pert), fill_value=self.prior_mean_pert[pidx]))
        ret = np.append(ret, np.full(len(on_interactions), fill_value=self.prior_mean_interactions))
        return ret.reshape(-1,1)

    def build_prior_cov_and_prec_and_diag(self, on_interactions, on_perturbations):
        &#39;&#39;&#39;Build the prior covariance matrices and others
        &#39;&#39;&#39;
        ret = []
        for pidx, pert in enumerate(on_perturbations):
            ret = np.append(ret, 
                np.full(len(pert), fill_value=self.prior_var_pert[pidx]))
        prior_var_diag = np.append(ret, np.full(len(on_interactions), fill_value=self.prior_var_interactions))
        prior_prec_diag = 1/prior_var_diag

        prior_var = scipy.sparse.dia_matrix((prior_var_diag,[0]), 
            shape=(len(prior_var_diag),len(prior_var_diag))).tocsc()
        prior_prec = scipy.sparse.dia_matrix((prior_prec_diag,[0]), 
            shape=(len(prior_prec_diag),len(prior_prec_diag))).tocsc()

        return prior_var, prior_prec, prior_prec_diag

    # @profile
    def calculate_marginal_loglikelihood_slow_fast_sparse(self):
        y = self.y
        X = self.X
        process_prec = self.process_prec_matrix
        prior_mean = self.prior_mean
        prior_cov = self.prior_cov
        prior_prec = self.prior_prec
        prior_prec_diag = self.prior_prec_diag

        a = X.T.dot(process_prec)
        beta_prec = a.dot(X) + prior_prec
        beta_cov = pinv(beta_prec, self)
        beta_mean = beta_cov @ (a.dot(y) + prior_prec.dot(prior_mean))
        beta_mean = np.asarray(beta_mean).reshape(-1,1)

        try:
            beta_logdet = log_det(beta_cov, self)
        except:
            logging.critical(&#39;Crashed in log_det&#39;)
            logging.critical(&#39;beta_cov:\n{}&#39;.format(beta_cov))
            logging.critical(&#39;prior_prec\n{}&#39;.format(prior_prec))
            raise
        
        priorvar_logdet = log_det(prior_cov, self)
        ll2 = 0.5 * (beta_logdet - priorvar_logdet)

        bEbprior = np.asarray(prior_mean.T @ prior_prec.dot(prior_mean))[0,0]
        bEb = np.asarray(beta_mean.T @ beta_prec.dot(beta_mean) )[0,0]
        ll3 = 0.5 * (bEb - bEbprior)

        self.a = a
        self.beta_prec = beta_prec

        return ll2 + ll3

    @staticmethod
    @numba.jit(nopython=True, cache=True)
    def get_indices(a, keypair2col, tmems, smems):
        &#39;&#39;&#39;Use Just in Time compilation to reduce the &#39;getting&#39; time
        by about 95%

        Parameters
        ----------
        keypair2col : np.ndarray
            Maps (target_oidx, source_oidx) pair to the place the interaction
            index would be on a full interactio design matrix on the OTU level
        tmems, smems : np.ndarray
            These are the OTU indices in the target cluster and the source cluster
            respectively
        &#39;&#39;&#39;
        i = 0
        for tidx in tmems:
            for sidx in smems:
                a[i] = keypair2col[tidx, sidx]
                i += 1
        return a

    @staticmethod
    # @numba.jit(nopython=True, cache=True)
    def make_iidx2cidxpair(ret, n_clusters):
        &#39;&#39;&#39;Map the index of a cluster interaction to (dst,src) of clusters

        Parameters
        ----------
        n_clusters : int
            Number of clusters

        Returns
        -------
        np.ndarray(n_interactions,2)
            First column is the destination cluster index, second column is the source
            cluster index
        &#39;&#39;&#39;
        i = 0
        for dst_cidx in range(n_clusters):
            for src_cidx in range(n_clusters):
                if dst_cidx == src_cidx:
                    continue
                ret[i,0] = dst_cidx
                ret[i,1] = src_cidx
                i += 1
        
        return ret


class CondensedClustering:
    &#39;&#39;&#39;Condensed clustering object that is not associated with the graph

    Parameters
    ----------
    oidx2cidx : np.ndarray
        Maps the cluster assignment to each asv.
        index -&gt; ASV index
        output -&gt; cluster index

    &#39;&#39;&#39;
    def __init__(self, oidx2cidx):

        self.clusters = []
        self.oidx2cidx = oidx2cidx
        a = {}
        for oidx, cidx in enumerate(self.oidx2cidx):
            if cidx not in a:
                a[cidx] = [oidx]
            else:
                a[cidx].append(oidx)
        
        cidx = 0
        while cidx in a:
            self.clusters.append(np.asarray(a[cidx], dtype=int))
            cidx += 1

    def __len__(self):
        return len(self.clusters)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="mdsine2.clustering.ClusterAssignments"><code class="flex name class">
<span>class <span class="ident">ClusterAssignments</span></span>
<span>(</span><span>clustering, concentration, m=1, mp=None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>This is the posterior of the cluster assignments for each ASV.</p>
<p>To calculate the loglikelihood of an ASV being in a cluster, we have to
marginalize out the cluster that the ASV belongs to - this means marginalizing
out the interactions going in and out of the cluster in question, along with all
the perturbations associated with it.</p>
<p>Parameters</p>
<p>clustering (pylab.cluster.Clustering)
- Defines the clusters
concentration (pylab.Variable or subclass of pylab.Variable)
- Defines the concentration parameter for the base distribution
m (int, Optional)
- Number of auxiliary variables defined in the model
- Default is 1
mp : str, None
This is the type of multiprocessing it is going to be. Options:
None
No multiprocessing
'full-#'
Send out to the different processors, where '#' is the number of
processors to make
'debug'
Send out to the different classes but stay on a single core. This
is necessary for benchmarking and easier debugging.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ClusterAssignments(pl.graph.Node):
    &#39;&#39;&#39;This is the posterior of the cluster assignments for each ASV.

    To calculate the loglikelihood of an ASV being in a cluster, we have to
    marginalize out the cluster that the ASV belongs to - this means marginalizing
    out the interactions going in and out of the cluster in question, along with all
    the perturbations associated with it.
    &#39;&#39;&#39;
    def __init__(self, clustering, concentration, m=1, mp=None, **kwargs):
        &#39;&#39;&#39;Parameters

        clustering (pylab.cluster.Clustering)
            - Defines the clusters
        concentration (pylab.Variable or subclass of pylab.Variable)
            - Defines the concentration parameter for the base distribution
        m (int, Optional)
            - Number of auxiliary variables defined in the model
            - Default is 1
        mp : str, None
            This is the type of multiprocessing it is going to be. Options:
                None
                    No multiprocessing
                &#39;full-#&#39;
                    Send out to the different processors, where &#39;#&#39; is the number of
                    processors to make
                &#39;debug&#39;
                    Send out to the different classes but stay on a single core. This
                    is necessary for benchmarking and easier debugging.
        &#39;&#39;&#39;
        self.clustering = clustering
        self.concentration = concentration
        self.m = m
        self.mp = mp
        self._strtime = -1
        # self.compute_device = &#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;

        if self.mp is not None:
            self.update = self.update_mp
        else:
            self.update = self.update_slow_fast

        kwargs[&#39;name&#39;] = STRNAMES.CLUSTERING
        pl.graph.Node.__init__(self, **kwargs)

    def __str__(self):
        return str(self.clustering) + &#39;\nTotal time: {}&#39;.format(self._strtime)

    def __getstate__(self):
        state = self.__dict__.copy()
        state.pop(&#39;pool&#39;, None)
        state.pop(&#39;actors&#39;, None)
        return state

    def __setstate__(self, state):
        self.__dict__.update(state)
        self.pool = []
        self.actors = None

    @property
    def value(self):
        &#39;&#39;&#39;This is so that the cluster assignments get printed in inference.MCMC
        &#39;&#39;&#39;
        return self.clustering

    @property
    def sample_iter(self):
        return self.clustering.n_clusters.sample_iter

    def initialize(self, value_option, hyperparam_option=None, value=None, n_clusters=None,
        delay=0, run_every_n_iterations=1):
        &#39;&#39;&#39;Initialize the cluster assingments - there are no hyperparamters to
        initialize because the concentration is initialized somewhere else

        Note - if `n_clusters` is not specified and the cluster initialization
        method requires it - it will be set to the expected number of clusters
        which = log(n_asvs)/log(2)

        Parameters
        ----------
        value_option : str
            The different methods to initialize the clusters
            Options
                &#39;manual&#39;
                    Manually set the cluster assignments
                &#39;no-clusters&#39;
                    Every ASV in their own cluster
                &#39;random&#39;
                    Every ASV is randomly assigned to the number of clusters. `n_clusters` required
                &#39;taxonomy&#39;
                    Cluster ASVs based on their taxonomic similarity. `n_clusters` required
                &#39;sequence&#39;
                    Cluster ASVs based on their sequence similarity. `n_clusters` required
                &#39;phylogeny&#39;
                    Cluster ASVs based on their phylogenetic similarity. `n_clusters` required
                &#39;spearman&#39;, &#39;auto&#39;
                    Creates a distance matrix based on the spearman rank similarity
                    between two trajectories. We use the raw data. `n_clusters` required
                &#39;fixed-topology&#39;
                    Sets the clustering assignment to the most likely clustering configuration
                    specified in the graph at the location `value` (`value` is a str).
                    We take the mean coclusterings and do agglomerative clustering on that matrix
                    with the `mode` number of clusters.
        hyperparam_option : None
            Not used in this function - only here for API consistency
        value : list of list
            Cluster assingments for each of the ASVs
            Only necessary if `value_option` == &#39;manual&#39;
        n_clusters : int, str
            Necessary if `value_option` is not &#39;manual&#39; or &#39;no-clusters&#39;
            If str, options:
                &#39;expected&#39;, &#39;auto&#39;: log_2(n_asvs)
        run_every_n_iterations : int
            Only run the update every `run_every_n_iterations` iterations
        &#39;&#39;&#39;
        from sklearn.cluster import AgglomerativeClustering
        asvs = self.G.data.asvs

        if not pl.isint(run_every_n_iterations):
            raise TypeError(&#39;`run_every_n_iterations` ({}) must be an int&#39;.format(
                type(run_every_n_iterations)))
        if run_every_n_iterations &lt;= 0:
            raise ValueError(&#39;`run_every_n_iterations` ({}) must be &gt;= 0&#39;.format(
                run_every_n_iterations))

        self.run_every_n_iterations = run_every_n_iterations
        self.delay = delay

        if not pl.isstr(value_option):
            raise TypeError(&#39;`value_option` ({}) must be a str&#39;.format(type(value_option)))
        if value_option not in [&#39;manual&#39;, &#39;no-clusters&#39;, &#39;fixed-topology&#39;]:
            if pl.isstr(n_clusters):
                if n_clusters in [&#39;expected&#39;, &#39;auto&#39;]:
                    n_clusters = int(round(np.log(len(asvs))/np.log(2)))
                else:
                    raise ValueError(&#39;`n_clusters` ({}) not recognized&#39;.format(n_clusters))
            if not pl.isint(n_clusters):
                raise TypeError(&#39;`n_clusters` ({}) must be a str or an int&#39;.format(type(n_clusters)))
            if n_clusters &lt;= 0:
                raise ValueError(&#39;`n_clusters` ({}) must be &gt; 0&#39;.format(n_clusters))
            if n_clusters &gt; self.G.data.n_asvs:
                raise ValueError(&#39;`n_clusters` ({}) must be &lt;= than the number of ASVs ({})&#39;.format(
                    n_clusters, self.G.data.n_asvs))

        if value_option == &#39;manual&#39;:
            # Check that all of the ASVs are in the init and that it is in the right
            # structure
            if not pl.isarray(value):
                raise ValueError(&#39;if `value_option` is &#34;manual&#34;, value ({}) must &#39; \
                    &#39;be of type array&#39;.format(value.__class__))
            clusters = list(value)

            idxs_to_delete = []
            for idx, cluster in enumerate(clusters):
                if not pl.isarray(cluster):
                    raise ValueError(&#39;cluster at index `{}` ({}) is not an array&#39;.format(
                        idx, cluster))
                cluster = list(cluster)
                if len(cluster) == 0:
                    logging.warning(&#39;Cluster index {} has 0 elements, deleting&#39;.format(
                        idx))
                    idxs_to_delete.append(idx)
            if len(idxs_to_delete) &gt; 0:
                clusters = np.delete(clusters, idxs_to_delete).tolist()

            all_oidxs = set()
            for cluster in clusters:
                for oidx in cluster:
                    if not pl.isint(oidx):
                        raise ValueError(&#39;`oidx` ({}) must be an int&#39;.format(oidx.__class__))
                    if oidx &gt;= len(asvs):
                        raise ValueError(&#39;oidx `{}` not in our ASVSet&#39;.format(oidx))
                    all_oidxs.add(oidx)

            for oidx in range(len(asvs)):
                if oidx not in all_oidxs:
                    raise ValueError(&#39;oidx `{}` in ASVSet not in `value` ({})&#39;.format(
                        oidx, value))
            # Now everything is checked and valid

        elif value_option == &#39;fixed-topology&#39;:
            logging.info(&#39;Fixed topology initialization&#39;)
            if not pl.isstr(value):
                raise TypeError(&#39;`value` ({}) must be a str&#39;.format(value))

            CHAIN2 = pl.inference.BaseMCMC.load(value)
            CLUSTERING2 = CHAIN2.graph[STRNAMES.CLUSTERING_OBJ]
            ASVS2 = CHAIN2.graph.data.asvs
            asvs_curr = self.G.data.asvs
            for asv in ASVS2:
                if asv.name not in asvs_curr:
                    raise ValueError(&#39;Cannot perform fixed topology because the ASV {} in &#39; \
                        &#39;the passed in clustering is not in this clustering: {}&#39;.format(
                            asv.name, asvs_curr.names.order))
            for asv in asvs_curr:
                if asv.name not in ASVS2:
                    raise ValueError(&#39;Cannot perform fixed topology because the ASV {} in &#39; \
                        &#39;the current clustering is not in the passed in clustering: {}&#39;.format(
                            asv.name, ASVS2.names.order))

            # Get the most likely cluster configuration and set as the value for the passed in cluster
            ret = generate_cluster_assignments_posthoc(CLUSTERING2, n_clusters=&#39;mode&#39;, set_as_value=True)
            ca = {}
            for aidx, cidx in enumerate(ret):
                if cidx not in ca:
                    ca[cidx] = []
                ca[cidx].append(aidx)
            ret = []
            for v in ca.values():
                ret.append(v)
            CLUSTERING2.from_array(ret)
            logging.info(&#39;Clustering set to:\n{}&#39;.format(str(CLUSTERING2)))

            # Set the passed in cluster assignment as the current cluster assignment
            # Need to be careful because the indices of the ASVs might not line up
            clusters = []
            for cluster in CLUSTERING2:
                anames = [asvs_curr[ASVS2.names.order[aidx]].name for aidx in cluster.members]
                aidxs = [asvs_curr[aname].idx for aname in anames]
                clusters.append(aidxs)

        elif value_option == &#39;no-clusters&#39;:
            clusters = []
            for oidx in range(len(asvs)):
                clusters.append([oidx])

        elif value_option == &#39;random&#39;:
            clusters = {}
            for oidx in range(len(asvs)):
                idx = npr.choice(n_clusters)
                if idx in clusters:
                    clusters[idx].append(oidx)
                else:
                    clusters[idx] = [oidx]
            c = []
            for cid in clusters.keys():
                c.append(clusters[cid])
            clusters = c

        elif value_option == &#39;taxonomy&#39;:
            # Create an affinity matrix, we can precompute the self-similarity to 1
            M = np.diag(np.ones(len(asvs), dtype=float))
            for i, oid1 in enumerate(asvs.ids.order):
                for j, oid2 in enumerate(asvs.ids.order):
                    if i == j:
                        continue
                    M[i,j] = asvs.taxonomic_similarity(oid1=oid1, oid2=oid2)

            c = AgglomerativeClustering(
                n_clusters=n_clusters,
                affinity=&#39;precomputed&#39;,
                linkage=&#39;complete&#39;)
            assignments = c.fit_predict(1-M)

            # Convert assignments into clusters
            clusters = {}
            for oidx,cidx in enumerate(assignments):
                if cidx not in clusters:
                    clusters[cidx] = []
                clusters[cidx].append(oidx)
            clusters = [val for val in clusters.values()]

        elif value_option == &#39;sequence&#39;:
            import diversity

            logging.info(&#39;Making affinity matrix from sequences&#39;)
            evenness = np.diag(np.ones(len(self.G.data.asvs), dtype=float))

            for i in range(len(self.G.data.asvs)):
                for j in range(len(self.G.data.asvs)):
                    if j &lt;= i:
                        continue
                    # Subtract because we want to make a similarity matrix
                    dist = 1-diversity.beta.hamming(
                        list(self.G.data.asvs[i].sequence),
                        list(self.G.data.asvs[j].sequence))
                    evenness[i,j] = dist
                    evenness[j,i] = dist

            c = AgglomerativeClustering(
                n_clusters=n_clusters,
                affinity=&#39;precomputed&#39;,
                linkage=&#39;average&#39;)
            assignments = c.fit_predict(evenness)
            clusters = {}
            for oidx,cidx in enumerate(assignments):
                if cidx not in clusters:
                    clusters[cidx] = []
                clusters[cidx].append(oidx)
            clusters = [val for val in clusters.values()]

        elif value_option == &#39;spearman&#39;:
            # Use spearman correlation to create a distance matrix
            # Use agglomerative clustering to make the clusters based
            # on distance matrix (distance = 1 - pearson(x,y))
            dm = np.zeros(shape=(len(asvs), len(asvs)))
            data = []
            for ridx in range(self.G.data.n_replicates):
                data.append(self.G.data.abs_data[ridx])
            data = np.hstack(data)
            for i in range(len(asvs)):
                for j in range(i+1):
                    distance = (1 - scipy.stats.spearmanr(data[i, :], data[j, :])[0])/2
                    dm[i,j] = distance
                    dm[j,i] = distance

            c = AgglomerativeClustering(
                n_clusters=n_clusters,
                affinity=&#39;precomputed&#39;,
                linkage=&#39;complete&#39;)
            assignments = c.fit_predict(dm)

            # convert into clusters
            clusters = {}
            for oidx, cidx in enumerate(assignments):
                if cidx not in clusters:
                    clusters[cidx] = []
                clusters[cidx].append(oidx)
            clusters = [val for val in clusters.values()]

        elif value_option == &#39;phylogeny&#39;:
            raise NotImplementedError(&#39;`phylogeny` not implemented yet&#39;)

        else:
            raise ValueError(&#39;`value_option` &#34;{}&#34; not recognized&#39;.format(value_option))

        # Move all the ASVs into their assigned clusters
        for cluster in clusters:
            cid = None
            for oidx in cluster:
                if cid is None:
                    # make new cluster
                    cid = self.clustering.make_new_cluster_with(idx=oidx)
                else:
                    self.clustering.move_item(idx=oidx, cid=cid)
        logging.info(&#39;Cluster Assingments initialization results:\n{}&#39;.format(
            str(self.clustering)))
        self._there_are_perturbations = self.G.perturbations is not None

        # Initialize the multiprocessors if necessary
        if self.mp is not None:
            if not pl.isstr(self.mp):
                raise TypeError(&#39;`mp` ({}) must be a str&#39;.format(type(self.mp)))
            if &#39;full&#39; in self.mp:
                n_cpus = self.mp.split(&#39;-&#39;)[1]
                if n_cpus == &#39;auto&#39;:
                    self.n_cpus = psutil.cpu_count(logical=False)
                else:
                    try:
                        self.n_cpus = int(n_cpus)
                    except:
                        raise ValueError(&#39;`mp` ({}) not recognized&#39;.format(self.mp))
                self.pool = pl.multiprocessing.PersistentPool(ptype=&#39;dasw&#39;, G=self.G)
            elif self.mp == &#39;debug&#39;:
                self.pool = None
            else:
                raise ValueError(&#39;`mp` ({}) not recognized&#39;.format(self.mp))
        else:
            self.pool = None

        self.ndts_bias = []
        self.n_asvs = len(self.G.data.asvs)
        self.n_replicates = self.G.data.n_replicates
        self.n_dts_for_replicate = self.G.data.n_dts_for_replicate
        self.total_dts = np.sum(self.n_dts_for_replicate)
        for ridx in range(self.G.data.n_replicates):
            self.ndts_bias.append(
                np.arange(0, self.G.data.n_dts_for_replicate[ridx] * self.n_asvs, self.n_asvs))
        self.replicate_bias = np.zeros(self.n_replicates, dtype=int)
        for ridx in range(1, self.n_replicates):
            self.replicate_bias[ridx] = self.replicate_bias[ridx-1] + \
                self.n_asvs * self.n_dts_for_replicate[ridx - 1]

    def visualize_posterior(self, basepath, f, section=&#39;posterior&#39;, asv_formatter=&#39;%(name)s&#39;,
        yticklabels=&#39;%(name)s %(index)s&#39;, xticklabels=&#39;%(index)s&#39;):
        &#39;&#39;&#39;Render the traces in the folder `basepath` and write the 
        learned values to the file `f`.

        Parameters
        ----------
        basepath : str
            This is the loction to write the files to
        f : _io.TextIOWrapper
            File that we are writing the values to
        section : str
            Section of the trace to compute on. Options:
                &#39;posterior&#39; : posterior samples
                &#39;burnin&#39; : burn-in samples
                &#39;entire&#39; : both burn-in and posterior samples

        Returns
        -------
        _io.TextIOWrapper
        &#39;&#39;&#39;
        asvs = self.G.data.asvs
        f.write(&#39;\n\n###################################\n&#39;)
        f.write(self.name)
        f.write(&#39;\n###################################\n&#39;)
        if not self.G.inference.is_in_inference_order(self):
            f.write(&#39;`{}` not learned. These were the fixed cluster assignments\n&#39;.format(self.name))
            for cidx, cluster in enumerate(self.clustering):
                f.write(&#39;Cluster {}:\n&#39;.format(cidx+1))
                for aidx in cluster.members:
                    label = pl.asvname_formatter(format=asv_formatter, asv=asvs[aidx], asvs=asvs)
                    f.write(&#39;\t- {}\n&#39;.format(label))

            return f

        # Coclusters
        cocluster_trace = self.clustering.coclusters.get_trace_from_disk(section=section)
        coclusters = pl.variables.summary(cocluster_trace, section=section)[&#39;mean&#39;]
        for i in range(coclusters.shape[0]):
            coclusters[i,i] = np.nan

        visualization.render_cocluster_proportions(
            coclusters=coclusters, asvs=self.G.data.asvs, clustering=self.clustering,
            yticklabels=yticklabels, include_tick_marks=False, xticklabels=xticklabels,
            title=&#39;Cluster Assignments&#39;)
        fig = plt.gcf()
        fig.tight_layout()
        plt.savefig(basepath + &#39;coclusters.pdf&#39;)
        plt.close()

        # N clusters
        visualization.render_trace(var=self.clustering.n_clusters, plt_type=&#39;both&#39;, 
            section=section, include_burnin=True, rasterized=True)
        fig = plt.gcf()
        fig.suptitle(&#39;Number of Clusters&#39;)
        plt.savefig(basepath + &#39;n_clusters.pdf&#39;)
        plt.close()

        ca = generate_cluster_assignments_posthoc(clustering=self.clustering, n_clusters=&#39;mode&#39;, 
            section=section)
        cluster_assignments = {}
        for idx, assignment in enumerate(ca):
            if assignment in cluster_assignments:
                cluster_assignments[assignment].append(idx)
            else:
                cluster_assignments[assignment] = [idx]

        f.write(&#39;Mode number of clusters: {}\n&#39;.format(len(self.clustering)))
        for idx,lst in enumerate(cluster_assignments.values()):
            f.write(&#39;Cluster {} - Size {}\n&#39;.format(idx+1, len(lst)))
            for oidx in lst:
                # Get rid of index because that does not really make sense here
                label = pl.asvname_formatter(format=asv_formatter, asv=asvs[oidx], asvs=asvs)
                f.write(&#39;\t- {}\n&#39;.format(label))
        
        return f

    def set_trace(self):
        self.clustering.set_trace()

    def add_trace(self):
        self.clustering.add_trace()

    def add_init_value(self):
        self.clustering.add_init_value()

    def kill(self):
        if pl.ispersistentpool(self.pool):
            # For pylab multiprocessing, explicitly kill them
            self.pool.kill()
        return

    # Update super safe - meant to be used during debugging
    # =====================================================
    def update_slow(self):
        &#39;&#39;&#39; This is updating the new cluster. Depending on the iteration you do
        either split-merge Metropolis-Hasting update or a regular Gibbs update. To
        get highest mixing we alternate between each.
        &#39;&#39;&#39;
        if self.clustering.n_clusters.sample_iter &lt; self.delay:
            return

        if self.clustering.n_clusters.sample_iter % self.run_every_n_iterations != 0:
           return

        # print(&#39;in clustering&#39;)
        start_time = time.time()
        oidxs = npr.permutation(np.arange(len(self.G.data.asvs)))

        for oidx in oidxs:
            self.gibbs_update_single_asv_slow(oidx=oidx)
        self._strtime = time.time() - start_time

    def gibbs_update_single_asv_slow(self, oidx):
        &#39;&#39;&#39;The update function is based off of Algorithm 8 in &#39;Markov Chain
        Sampling Methods for Dirichlet Process Mixture Models&#39; by Radford M.
        Neal, 2000.

        Calculate the marginal likelihood of the asv in every cluster
        and a new cluster then sample from `self.sample_categorical_log`
        to get the cluster assignment.

        Parameters
        ----------
        oidx : int
            ASV index that we are updating the cluster assignment of
        &#39;&#39;&#39;
        curr_cluster = self.clustering.idx2cid[oidx]
        concentration = self.concentration.value

        # start as a dictionary then send values to `sample_categorical_log`
        LOG_P = []
        LOG_KEYS = []

        # Calculate current cluster
        # =========================
        # If the element is already in its own cluster, use the new cluster case
        if self.clustering.clusters[curr_cluster].size == 1:
            a = np.log(concentration/self.m)
        else:
            a = np.log(self.clustering.clusters[curr_cluster].size - 1)
        LOG_P.append(a + self.calculate_marginal_loglikelihood_slow()[&#39;ret&#39;])
        LOG_KEYS.append(curr_cluster)

        # Calculate going to every other cluster
        # ======================================
        for cid in self.clustering.order:
            if curr_cluster == cid:
                continue

            # Move ASV and recompute the matrices
            self.clustering.move_item(idx=oidx,cid=cid)
            self.G[REPRNAMES.CLUSTER_INTERACTION_INDICATOR].update_cnt_indicators()
            self.G.data.design_matrices[REPRNAMES.CLUSTER_INTERACTION_VALUE].M.build()
            if self._there_are_perturbations:
                self.G.data.design_matrices[REPRNAMES.PERT_VALUE].M.build()

            LOG_P.append(np.log(self.clustering.clusters[cid].size - 1) + \
                self.calculate_marginal_loglikelihood_slow()[&#39;ret&#39;])
            LOG_KEYS.append(cid)


        # Calculate new cluster
        # =====================
        cid=self.clustering.make_new_cluster_with(idx=oidx)
        self.G[REPRNAMES.CLUSTER_INTERACTION_INDICATOR].update_cnt_indicators()
        self.G.data.design_matrices[REPRNAMES.CLUSTER_INTERACTION_VALUE].M.build()
        if self._there_are_perturbations:
            self.G.data.design_matrices[REPRNAMES.PERT_VALUE].M.build()
        
        LOG_KEYS.append(cid)
        LOG_P.append(np.log(concentration/self.m) + \
            self.calculate_marginal_loglikelihood_slow()[&#39;ret&#39;])

        # Sample the assignment
        # =====================
        idx = sample_categorical_log(LOG_P)
        assigned_cid = LOG_KEYS[idx]
        curr_clus = self.clustering.idx2cid[oidx]

        if assigned_cid != curr_clus:
            self.clustering.move_item(idx=oidx,cid=assigned_cid)
            self.G[REPRNAMES.CLUSTER_INTERACTION_INDICATOR].update_cnt_indicators()

            # Change the mixing matrix for the interactions and (potentially) perturbations
            self.G.data.design_matrices[REPRNAMES.CLUSTER_INTERACTION_VALUE].M.build()
            if self._there_are_perturbations:
                self.G.data.design_matrices[REPRNAMES.PERT_VALUE].M.build()

    def calculate_marginal_loglikelihood_slow(self):
        &#39;&#39;&#39;Marginalizes out the interactions and the perturbations
        &#39;&#39;&#39;
        # Build the parameters
        # ====================
        self.G[REPRNAMES.CLUSTER_INTERACTION_INDICATOR].update_cnt_indicators()
        lhs = [REPRNAMES.GROWTH_VALUE, REPRNAMES.SELF_INTERACTION_VALUE]
        if self._there_are_perturbations:
            rhs = [REPRNAMES.PERT_VALUE, REPRNAMES.CLUSTER_INTERACTION_VALUE]
        else:
            rhs = [REPRNAMES.CLUSTER_INTERACTION_VALUE]

        # reconstruct the X matrices
        for v in rhs:
            self.G.data.design_matrices[v].M.build()
        
        y = self.G.data.construct_lhs(lhs, 
            kwargs_dict={REPRNAMES.GROWTH_VALUE:{&#39;with_perturbations&#39;: False}})
        X = self.G.data.construct_rhs(keys=rhs, toarray=True)
        process_prec = self.G[REPRNAMES.PROCESSVAR].build_matrix(cov=False, sparse=False)
        prior_prec = build_prior_covariance(G=self.G, cov=False, order=rhs, sparse=False)
        prior_var = build_prior_covariance(G=self.G, cov=True, order=rhs, sparse=False)
        prior_mean = build_prior_mean(G=self.G, order=rhs, shape=(-1,1))

        # If nothing is on, return 0
        if X.shape[1] == 0:
            return {
                &#39;a&#39;: 0,
                &#39;beta_prec&#39;: 0,
                &#39;process_prec&#39;: prior_prec,
                &#39;ret&#39;: 0,
                &#39;beta_logdet&#39;: 0,
                &#39;priorvar_logdet&#39;: 0,
                &#39;bEb&#39;: 0,
                &#39;bEbprior&#39;: 0}

        # Calculate the marginalization
        # =============================
        beta_prec = X.T @ process_prec @ X + prior_prec
        beta_cov = pinv(beta_prec, self)
        beta_mean = beta_cov @ ( X.T @ process_prec @ y + prior_prec @ prior_mean )
        beta_mean = np.asarray(beta_mean).reshape(-1,1)

        try:
            beta_logdet = log_det(beta_cov, self)
        except:
            logging.critical(&#39;Crashed in log_det&#39;)
            logging.critical(&#39;beta_cov:\n{}&#39;.format(beta_cov))
            logging.critical(&#39;prior_prec\n{}&#39;.format(prior_prec))
            raise
        priorvar_logdet = log_det(prior_var, self)
        ll2 = 0.5 * (beta_logdet - priorvar_logdet)

        bEbprior = np.asarray(prior_mean.T @ prior_prec @ prior_mean)[0,0]
        bEb = np.asarray(beta_mean.T @ beta_prec @ beta_mean)[0,0]
        ll3 = 0.5 * (bEb  - bEbprior)

        # print(&#39;prior_prec truth:\n&#39;, prior_prec)

        return {
            &#39;a&#39;: X.T @ process_prec,
            &#39;beta_prec&#39;: beta_prec,
            &#39;process_prec&#39;: prior_prec,
            &#39;ret&#39;: ll2+ll3,
            &#39;beta_logdet&#39;: beta_logdet,
            &#39;priorvar_logdet&#39;: priorvar_logdet,
            &#39;bEb&#39;: bEb,
            &#39;bEbprior&#39;: bEbprior}

    # Update regular - meant to be used during inference
    # ==================================================
    # @profile
    def update_slow_fast(self):
        &#39;&#39;&#39;Much faster than `update_slow`
        &#39;&#39;&#39;

        if self.clustering.n_clusters.sample_iter &lt; self.delay:
            return

        if self.clustering.n_clusters.sample_iter % self.run_every_n_iterations != 0:
           return

        start_time = time.time()

        self.process_prec = self.G[REPRNAMES.PROCESSVAR].prec.ravel() #.build_matrix(cov=False, sparse=False)
        self.process_prec_matrix = self.G[REPRNAMES.PROCESSVAR].build_matrix(sparse=True, cov=False)
        lhs = [REPRNAMES.GROWTH_VALUE, REPRNAMES.SELF_INTERACTION_VALUE]
        self.y = self.G.data.construct_lhs(lhs, 
            kwargs_dict={REPRNAMES.GROWTH_VALUE:{&#39;with_perturbations&#39;: False}})

        oidxs = npr.permutation(np.arange(len(self.G.data.asvs)))
        iii = 0
        for oidx in oidxs:
            logging.info(&#39;{}/{}: {}&#39;.format(iii, len(oidxs), oidx))
            self.gibbs_update_single_asv_slow_fast(oidx=oidx)
            iii += 1
        self._strtime = time.time() - start_time
    
    # @profile
    def gibbs_update_single_asv_slow_fast(self, oidx):
        &#39;&#39;&#39;The update function is based off of Algorithm 8 in &#39;Markov Chain
        Sampling Methods for Dirichlet Process Mixture Models&#39; by Radford M.
        Neal, 2000.

        Calculate the marginal likelihood of the asv in every cluster
        and a new cluster then sample from `self.sample_categorical_log`
        to get the cluster assignment.

        Parameters
        ----------
        oidx : int
            ASV index that we are updating the cluster assignment of
        &#39;&#39;&#39;
        curr_cluster = self.clustering.idx2cid[oidx]
        concentration = self.concentration.value

        # start as a dictionary then send values to `sample_categorical_log`
        LOG_P = []
        LOG_KEYS = []

        # Calculate current cluster
        # =========================
        # If the element is already in its own cluster, use the new cluster case
        if self.clustering.clusters[curr_cluster].size == 1:
            a = np.log(concentration/self.m)
        else:
            a = np.log(self.clustering.clusters[curr_cluster].size - 1)
        LOG_P.append(a + self.calculate_marginal_loglikelihood_slow_fast_sparse())
        LOG_KEYS.append(curr_cluster)

        # Calculate going to every other cluster
        # ======================================
        for cid in self.clustering.order:
            if curr_cluster == cid:
                continue

            # Move ASV and recompute the matrices
            self.clustering.move_item(idx=oidx,cid=cid)
            self.G[REPRNAMES.CLUSTER_INTERACTION_INDICATOR].update_cnt_indicators()
            self.G.data.design_matrices[REPRNAMES.CLUSTER_INTERACTION_VALUE].M.build()
            if self._there_are_perturbations:
                self.G.data.design_matrices[REPRNAMES.PERT_VALUE].M.build()

            LOG_P.append(np.log(self.clustering.clusters[cid].size - 1) + \
                self.calculate_marginal_loglikelihood_slow_fast_sparse())
            LOG_KEYS.append(cid)


        # Calculate new cluster
        # =====================
        cid=self.clustering.make_new_cluster_with(idx=oidx)
        self.G[REPRNAMES.CLUSTER_INTERACTION_INDICATOR].update_cnt_indicators()
        self.G.data.design_matrices[REPRNAMES.CLUSTER_INTERACTION_VALUE].M.build()
        if self._there_are_perturbations:
            self.G.data.design_matrices[REPRNAMES.PERT_VALUE].M.build()
        
        LOG_KEYS.append(cid)
        LOG_P.append(np.log(concentration/self.m) + \
            self.calculate_marginal_loglikelihood_slow_fast_sparse())

        # Sample the assignment
        # =====================
        idx = sample_categorical_log(LOG_P)
        assigned_cid = LOG_KEYS[idx]
        curr_clus = self.clustering.idx2cid[oidx]

        if assigned_cid != curr_clus:
            self.clustering.move_item(idx=oidx,cid=assigned_cid)
            self.G[REPRNAMES.CLUSTER_INTERACTION_INDICATOR].update_cnt_indicators()

            # Change the mixing matrix for the interactions and (potentially) perturbations
            self.G.data.design_matrices[REPRNAMES.CLUSTER_INTERACTION_VALUE].M.build()
            if self._there_are_perturbations:
                self.G.data.design_matrices[REPRNAMES.PERT_VALUE].M.build()

    # @profile
    def calculate_marginal_loglikelihood_slow_fast(self):
        &#39;&#39;&#39;Marginalizes out the interactions and the perturbations
        &#39;&#39;&#39;
        # Build the parameters
        # ====================
        if self._there_are_perturbations:
            rhs = [REPRNAMES.PERT_VALUE, REPRNAMES.CLUSTER_INTERACTION_VALUE]
        else:
            rhs = [REPRNAMES.CLUSTER_INTERACTION_VALUE]
        
        
        y = self.y
        X = self.G.data.construct_rhs(keys=rhs, toarray=True)

        process_prec = self.process_prec
        prior_prec = build_prior_covariance(G=self.G, cov=False, order=rhs, sparse=False)
        prior_prec_diag = np.diag(prior_prec)
        prior_var = build_prior_covariance(G=self.G, cov=True, order=rhs, sparse=False)
        prior_mean = build_prior_mean(G=self.G, order=rhs, shape=(-1,1))

        # Calculate the marginalization
        # =============================
        a = X.T * process_prec

        beta_prec = a @ X + prior_prec
        beta_cov = pinv(beta_prec, self)
        beta_mean = beta_cov @ ( a @ y + prior_prec @ prior_mean )
        beta_mean = np.asarray(beta_mean).reshape(-1,1)

        try:
            beta_logdet = log_det(beta_cov, self)
        except:
            logging.critical(&#39;Crashed in log_det&#39;)
            logging.critical(&#39;beta_cov:\n{}&#39;.format(beta_cov))
            logging.critical(&#39;prior_prec\n{}&#39;.format(prior_prec))
            raise
        priorvar_logdet = log_det(prior_var, self)
        ll2 = 0.5 * (beta_logdet - priorvar_logdet)

        a = np.sum((prior_mean.ravel() ** 2) *prior_prec_diag)
        # np.asarray(prior_mean.T @ prior_prec @ prior_mean)[0,0]
        b = np.asarray(beta_mean.T @ beta_prec @ beta_mean)[0,0]
        ll3 = -0.5 * (a  - b)

        return ll2+ll3

    # @profile
    def calculate_marginal_loglikelihood_slow_fast_sparse(self):
        &#39;&#39;&#39;Marginalizes out the interactions and the perturbations
        &#39;&#39;&#39;
        # Build the parameters
        # ====================
        if self._there_are_perturbations:
            rhs = [REPRNAMES.PERT_VALUE, REPRNAMES.CLUSTER_INTERACTION_VALUE]
        else:
            rhs = [REPRNAMES.CLUSTER_INTERACTION_VALUE]
        
        
        y = self.y
        X = self.G.data.construct_rhs(keys=rhs, toarray=False)

        process_prec = self.process_prec_matrix
        prior_prec = build_prior_covariance(G=self.G, cov=False, order=rhs, sparse=True)  
        prior_prec_diag = build_prior_covariance(G=self.G, cov=False, order=rhs, diag=True)        
        prior_var = build_prior_covariance(G=self.G, cov=True, order=rhs, sparse=True)
        prior_mean = build_prior_mean(G=self.G, order=rhs, shape=(-1,1))

        # Calculate the marginalization
        # =============================

        # print(&#39;X&#39;)
        # print(type(X))
        # print(X.shape)

        # print(&#39;process_prec&#39;)
        # print(type(process_prec))
        # print(process_prec.shape)

        # print(&#39;prior_prec&#39;)
        # print(type(prior_prec))
        # print(prior_prec.shape)

        # print(&#39;prior mean&#39;)
        # print(type(prior_mean))
        # print(prior_mean.shape)

        a = X.T.dot(process_prec)
        beta_prec = a.dot(X) + prior_prec
        beta_cov = pinv(beta_prec, self)
        beta_mean = beta_cov @ ( a.dot(y) + prior_prec.dot(prior_mean))
        beta_mean = np.asarray(beta_mean).reshape(-1,1)

        try:
            beta_logdet = log_det(beta_cov, self)
        except:
            logging.critical(&#39;Crashed in log_det&#39;)
            logging.critical(&#39;beta_cov:\n{}&#39;.format(beta_cov))
            logging.critical(&#39;prior_prec\n{}&#39;.format(prior_prec))
            raise
        priorvar_logdet = log_det(prior_var, self)
        ll2 = 0.5 * (beta_logdet - priorvar_logdet)

        a = np.sum((prior_mean.ravel() ** 2) *prior_prec_diag)
        # np.asarray(prior_mean.T @ prior_prec @ prior_mean)[0,0]

        # print(&#39;beta_prec.shape&#39;, beta_prec.shape)
        # print(&#39;beta_mean.shape&#39;, beta_mean.shape)

        b = np.asarray(beta_mean.T @ beta_prec.dot(beta_mean))[0,0]
        ll3 = -0.5 * (a  - b)

        return ll2+ll3

    # Update MP - meant to be used during inference
    # =============================================
    def update_mp(self):
        &#39;&#39;&#39;Implements `update_slow` but parallelizes calculating the likelihood
        of being in a cluster. NOTE that this does not parallelize on the ASV level.

        On the first gibb step with initialize the workers that we implement with DASW (
        different arguments, single worker). For more information what this means look
        at pylab.multiprocessing documentation.

        If we initialized our pool as a pylab.multiprocessing.PersistentPool, then we 
        multiprocess the likelihood calculations for each asv. If we didnt then this 
        implementation has the same performance as `ClusterAssignments.update_slow_fast`.
        &#39;&#39;&#39;
        if self.G.data.zero_inflation_transition_policy is not None:
            raise NotImplementedError(&#39;Multiprocessing for zero inflation data is not implemented yet.&#39; \
                &#39; Use `mp=None`&#39;)
        DMI = self.G.data.design_matrices[REPRNAMES.CLUSTER_INTERACTION_VALUE]
        DMP = self.G.data.design_matrices[REPRNAMES.PERT_VALUE]
        if self.clustering.n_clusters.sample_iter == 0 or self.pool == []:
            kwargs = {
                &#39;n_asvs&#39;: len(self.G.data.asvs),
                &#39;total_n_dts_per_asv&#39;: self.G.data.total_n_dts_per_asv,
                &#39;n_replicates&#39;: self.G.data.n_replicates,
                &#39;n_dts_for_replicate&#39;: self.G.data.n_dts_for_replicate,
                &#39;there_are_perturbations&#39;: self._there_are_perturbations,
                &#39;keypair2col_interactions&#39;: DMI.M.keypair2col,
                &#39;keypair2col_perturbations&#39;: DMP.M.keypair2col,
                &#39;n_perturbations&#39;: len(self.G.perturbations) if self._there_are_perturbations else None,
                &#39;base_Xrows&#39;: DMI.base.rows,
                &#39;base_Xcols&#39;: DMI.base.cols,
                &#39;base_Xshape&#39;: DMI.base.shape,
                &#39;base_Xpertrows&#39;: DMP.base.rows,
                &#39;base_Xpertcols&#39;: DMP.base.cols,
                &#39;base_Xpertshape&#39;: DMP.base.shape,
                &#39;n_rowsM&#39;: DMI.M.n_rows,
                &#39;n_rowsMpert&#39;: DMP.M.n_rows}

            if pl.ispersistentpool(self.pool):
                for _ in range(self.n_cpus):
                    self.pool.add_worker(SingleClusterFullParallelization(**kwargs))
            else:
                self.pool = SingleClusterFullParallelization(**kwargs)

        if self.clustering.n_clusters.sample_iter &lt; self.delay:
            return

        if self.clustering.n_clusters.sample_iter % self.run_every_n_iterations != 0:
            return

        # Send in arguments for the start of the gibbs step
        start_time = time.time()
        base_Xdata = DMI.base.data
        self.concentration = self.G[REPRNAMES.CONCENTRATION].value
        y = self.G.data.construct_lhs(keys=[REPRNAMES.GROWTH_VALUE, REPRNAMES.SELF_INTERACTION_VALUE],
            kwargs_dict={REPRNAMES.GROWTH_VALUE: {&#39;with_perturbations&#39;:False}})
        prior_var_interactions = self.G[REPRNAMES.PRIOR_VAR_INTERACTIONS].value
        prior_mean_interactions = self.G[REPRNAMES.PRIOR_MEAN_INTERACTIONS].value
        process_prec_diag = self.G[REPRNAMES.PROCESSVAR].prec

        if self._there_are_perturbations:
            prior_var_pert = self.G[REPRNAMES.PRIOR_VAR_PERT].get_single_value_of_perts()
            prior_mean_pert = self.G[REPRNAMES.PRIOR_MEAN_PERT].get_single_value_of_perts()
            base_Xpertdata = DMP.base.data
        else:
            prior_var_pert = None
            prior_mean_pert = None
            base_Xpertdata = None

        kwargs = {
            &#39;base_Xdata&#39;: base_Xdata,
            &#39;base_Xpertdata&#39;: base_Xpertdata,
            &#39;concentration&#39;: self.concentration,
            &#39;m&#39;: self.m,
            &#39;y&#39;: y,
            &#39;process_prec_diag&#39;: process_prec_diag,
            &#39;prior_var_interactions&#39;: prior_var_interactions,
            &#39;prior_var_pert&#39;: prior_var_pert,
            &#39;prior_mean_interactions&#39;: prior_mean_interactions,
            &#39;prior_mean_pert&#39;: prior_mean_pert}
        
        if pl.ispersistentpool(self.pool):
            self.pool.map(&#39;initialize_gibbs&#39;, [kwargs]*self.pool.num_workers)
        else:
            self.pool.initialize_gibbs(**kwargs)

        oidxs = npr.permutation(np.arange(len(self.G.data.asvs)))
        for iii, oidx in enumerate(oidxs):
            logging.info(&#39;{}/{} - {}&#39;.format(iii, len(self.G.data.asvs), oidx))
            self.oidx = oidx
            self.gibbs_update_single_asv_parallel()

        self._strtime = time.time() - start_time

    def gibbs_update_single_asv_parallel(self):
        &#39;&#39;&#39;Update for a single asvs
        &#39;&#39;&#39;
        self.original_cluster = self.clustering.idx2cid[self.oidx]
        self.curr_cluster = self.original_cluster

        interactions = self.G[REPRNAMES.INTERACTIONS_OBJ]
        interaction_on_idxs = interactions.get_indicators(return_idxs=True)
        if self._there_are_perturbations:
            perturbation_on_idxs = [p.indicator.cluster_arg_array() for p in self.G.perturbations]
        else:
            perturbation_on_idxs = None

        # # Send topology parameters if the topology wont change
        # TODO: this works on windows but not on the cluster when dispatching?
        # if self.clustering.clusters[self.original_cluster].size &gt; 1:
        #     use_saved_params = True
        #     kwargs = {
        #         &#39;interaction_on_idxs&#39;: interaction_on_idxs,
        #         &#39;perturbation_on_idxs&#39;: perturbation_on_idxs}
        #     if pl.ispersistentpool(self.pool):
        #         self.pool.map(&#39;initialize_oidx&#39;, [kwargs]*self.pool.num_workers)
        #     else:
        #         use_saved_params = False
        # else:
        #     use_saved_params = False
        use_saved_params = False

        if pl.ispersistentpool(self.pool):
            self.pool.staged_map_start(&#39;run&#39;)
        else:
            notpool_ret = []

        # Get the likelihood of the current configuration
        if self.clustering.clusters[self.original_cluster].size == 1:
            log_mult_factor = math.log(self.concentration/self.m)
        else:
            log_mult_factor = math.log(self.clustering.clusters[self.original_cluster].size - 1)

        if use_saved_params and pl.ispersistentpool(self.pool):
            interaction_on_idxs = None
            perturbation_on_idxs = None

        cluster_config = np.asarray([self.clustering.cid2cidx[self.clustering.idx2cid[i]] \
            for i in range(len(self.G.data.asvs))])

        kwargs = {
            &#39;interaction_on_idxs&#39;: interaction_on_idxs,
            &#39;perturbation_on_idxs&#39;: perturbation_on_idxs,
            &#39;cluster_config&#39;: cluster_config,
            &#39;log_mult_factor&#39;: log_mult_factor,
            &#39;cid&#39;: self.original_cluster,
            &#39;use_saved_params&#39;: use_saved_params}

        if pl.ispersistentpool(self.pool):
            self.pool.staged_map_put(kwargs)
        else:
            notpool_ret.append(self.pool.run(**kwargs))

        # Check every cluster
        for cid in self.clustering.order:
            if cid == self.original_cluster:
                continue
            self.clustering.move_item(idx=self.oidx, cid=cid)
            self.curr_cluster = cid

            if not use_saved_params:
                interaction_on_idxs = interactions.get_indicators(return_idxs=True)
                if self._there_are_perturbations:
                    perturbation_on_idxs = [p.indicator.cluster_arg_array() for p in self.G.perturbations]
                else:
                    perturbation_on_idxs = None

            cluster_config = np.asarray([self.clustering.cid2cidx[self.clustering.idx2cid[i]] \
                for i in range(len(self.G.data.asvs))])
            log_mult_factor = np.log(self.clustering.clusters[self.curr_cluster].size - 1)

            kwargs = {
                &#39;interaction_on_idxs&#39;: interaction_on_idxs,
                &#39;perturbation_on_idxs&#39;: perturbation_on_idxs,
                &#39;cluster_config&#39;: cluster_config,
                &#39;log_mult_factor&#39;: log_mult_factor,
                &#39;cid&#39;: self.curr_cluster,
                &#39;use_saved_params&#39;: use_saved_params}

            if pl.ispersistentpool(self.pool):
                self.pool.staged_map_put(kwargs)
            else:
                notpool_ret.append(self.pool.run(**kwargs))

        # Make a new cluster
        self.curr_cluster = self.clustering.make_new_cluster_with(idx=self.oidx)
        cluster_config = np.asarray([self.clustering.cid2cidx[self.clustering.idx2cid[i]] \
            for i in range(len(self.G.data.asvs))])
        interaction_on_idxs = interactions.get_indicators(return_idxs=True)
        if self._there_are_perturbations:
            perturbation_on_idxs = [p.indicator.cluster_arg_array() for p in self.G.perturbations]
        else:
            perturbation_on_idxs = None
        log_mult_factor = np.log(self.concentration/self.m)

        kwargs = {
            &#39;interaction_on_idxs&#39;: interaction_on_idxs,
            &#39;perturbation_on_idxs&#39;: perturbation_on_idxs,
            &#39;cluster_config&#39;: cluster_config,
            &#39;log_mult_factor&#39;: log_mult_factor,
            &#39;cid&#39;: self.curr_cluster,
            &#39;use_saved_params&#39;: False}

        # Put the values and get if necessary
        KEYS = []
        LOG_P = []
        if pl.ispersistentpool(self.pool):
            self.pool.staged_map_put(kwargs)
            ret = self.pool.staged_map_get()
        else:
            notpool_ret.append(self.pool.run(**kwargs))
            ret = notpool_ret
        for c, p in ret:
            KEYS.append(c)
            LOG_P.append(p)

        idx = sample_categorical_log(LOG_P)
        assigned_cid = KEYS[idx]

        if assigned_cid != self.original_cluster:
            logging.info(&#39;cluster changed&#39;)

        self.clustering.move_item(idx=self.oidx, cid=assigned_cid)

        self.G[REPRNAMES.CLUSTER_INTERACTION_INDICATOR].update_cnt_indicators()
        self.G.data.design_matrices[REPRNAMES.CLUSTER_INTERACTION_VALUE].M.build()
        if self._there_are_perturbations:
            self.G.data.design_matrices[REPRNAMES.PERT_VALUE].M.build()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mdsine2.pylab.graph.Node" href="pylab/graph.html#mdsine2.pylab.graph.Node">Node</a></li>
<li><a title="mdsine2.pylab.graph.BaseNode" href="pylab/graph.html#mdsine2.pylab.graph.BaseNode">BaseNode</a></li>
<li><a title="mdsine2.pylab.base.Saveable" href="pylab/base.html#mdsine2.pylab.base.Saveable">Saveable</a></li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="mdsine2.clustering.ClusterAssignments.sample_iter"><code class="name">var <span class="ident">sample_iter</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def sample_iter(self):
    return self.clustering.n_clusters.sample_iter</code></pre>
</details>
</dd>
<dt id="mdsine2.clustering.ClusterAssignments.value"><code class="name">var <span class="ident">value</span></code></dt>
<dd>
<div class="desc"><p>This is so that the cluster assignments get printed in inference.MCMC</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def value(self):
    &#39;&#39;&#39;This is so that the cluster assignments get printed in inference.MCMC
    &#39;&#39;&#39;
    return self.clustering</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="mdsine2.clustering.ClusterAssignments.add_init_value"><code class="name flex">
<span>def <span class="ident">add_init_value</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_init_value(self):
    self.clustering.add_init_value()</code></pre>
</details>
</dd>
<dt id="mdsine2.clustering.ClusterAssignments.add_trace"><code class="name flex">
<span>def <span class="ident">add_trace</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_trace(self):
    self.clustering.add_trace()</code></pre>
</details>
</dd>
<dt id="mdsine2.clustering.ClusterAssignments.calculate_marginal_loglikelihood_slow"><code class="name flex">
<span>def <span class="ident">calculate_marginal_loglikelihood_slow</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Marginalizes out the interactions and the perturbations</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calculate_marginal_loglikelihood_slow(self):
    &#39;&#39;&#39;Marginalizes out the interactions and the perturbations
    &#39;&#39;&#39;
    # Build the parameters
    # ====================
    self.G[REPRNAMES.CLUSTER_INTERACTION_INDICATOR].update_cnt_indicators()
    lhs = [REPRNAMES.GROWTH_VALUE, REPRNAMES.SELF_INTERACTION_VALUE]
    if self._there_are_perturbations:
        rhs = [REPRNAMES.PERT_VALUE, REPRNAMES.CLUSTER_INTERACTION_VALUE]
    else:
        rhs = [REPRNAMES.CLUSTER_INTERACTION_VALUE]

    # reconstruct the X matrices
    for v in rhs:
        self.G.data.design_matrices[v].M.build()
    
    y = self.G.data.construct_lhs(lhs, 
        kwargs_dict={REPRNAMES.GROWTH_VALUE:{&#39;with_perturbations&#39;: False}})
    X = self.G.data.construct_rhs(keys=rhs, toarray=True)
    process_prec = self.G[REPRNAMES.PROCESSVAR].build_matrix(cov=False, sparse=False)
    prior_prec = build_prior_covariance(G=self.G, cov=False, order=rhs, sparse=False)
    prior_var = build_prior_covariance(G=self.G, cov=True, order=rhs, sparse=False)
    prior_mean = build_prior_mean(G=self.G, order=rhs, shape=(-1,1))

    # If nothing is on, return 0
    if X.shape[1] == 0:
        return {
            &#39;a&#39;: 0,
            &#39;beta_prec&#39;: 0,
            &#39;process_prec&#39;: prior_prec,
            &#39;ret&#39;: 0,
            &#39;beta_logdet&#39;: 0,
            &#39;priorvar_logdet&#39;: 0,
            &#39;bEb&#39;: 0,
            &#39;bEbprior&#39;: 0}

    # Calculate the marginalization
    # =============================
    beta_prec = X.T @ process_prec @ X + prior_prec
    beta_cov = pinv(beta_prec, self)
    beta_mean = beta_cov @ ( X.T @ process_prec @ y + prior_prec @ prior_mean )
    beta_mean = np.asarray(beta_mean).reshape(-1,1)

    try:
        beta_logdet = log_det(beta_cov, self)
    except:
        logging.critical(&#39;Crashed in log_det&#39;)
        logging.critical(&#39;beta_cov:\n{}&#39;.format(beta_cov))
        logging.critical(&#39;prior_prec\n{}&#39;.format(prior_prec))
        raise
    priorvar_logdet = log_det(prior_var, self)
    ll2 = 0.5 * (beta_logdet - priorvar_logdet)

    bEbprior = np.asarray(prior_mean.T @ prior_prec @ prior_mean)[0,0]
    bEb = np.asarray(beta_mean.T @ beta_prec @ beta_mean)[0,0]
    ll3 = 0.5 * (bEb  - bEbprior)

    # print(&#39;prior_prec truth:\n&#39;, prior_prec)

    return {
        &#39;a&#39;: X.T @ process_prec,
        &#39;beta_prec&#39;: beta_prec,
        &#39;process_prec&#39;: prior_prec,
        &#39;ret&#39;: ll2+ll3,
        &#39;beta_logdet&#39;: beta_logdet,
        &#39;priorvar_logdet&#39;: priorvar_logdet,
        &#39;bEb&#39;: bEb,
        &#39;bEbprior&#39;: bEbprior}</code></pre>
</details>
</dd>
<dt id="mdsine2.clustering.ClusterAssignments.calculate_marginal_loglikelihood_slow_fast"><code class="name flex">
<span>def <span class="ident">calculate_marginal_loglikelihood_slow_fast</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Marginalizes out the interactions and the perturbations</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calculate_marginal_loglikelihood_slow_fast(self):
    &#39;&#39;&#39;Marginalizes out the interactions and the perturbations
    &#39;&#39;&#39;
    # Build the parameters
    # ====================
    if self._there_are_perturbations:
        rhs = [REPRNAMES.PERT_VALUE, REPRNAMES.CLUSTER_INTERACTION_VALUE]
    else:
        rhs = [REPRNAMES.CLUSTER_INTERACTION_VALUE]
    
    
    y = self.y
    X = self.G.data.construct_rhs(keys=rhs, toarray=True)

    process_prec = self.process_prec
    prior_prec = build_prior_covariance(G=self.G, cov=False, order=rhs, sparse=False)
    prior_prec_diag = np.diag(prior_prec)
    prior_var = build_prior_covariance(G=self.G, cov=True, order=rhs, sparse=False)
    prior_mean = build_prior_mean(G=self.G, order=rhs, shape=(-1,1))

    # Calculate the marginalization
    # =============================
    a = X.T * process_prec

    beta_prec = a @ X + prior_prec
    beta_cov = pinv(beta_prec, self)
    beta_mean = beta_cov @ ( a @ y + prior_prec @ prior_mean )
    beta_mean = np.asarray(beta_mean).reshape(-1,1)

    try:
        beta_logdet = log_det(beta_cov, self)
    except:
        logging.critical(&#39;Crashed in log_det&#39;)
        logging.critical(&#39;beta_cov:\n{}&#39;.format(beta_cov))
        logging.critical(&#39;prior_prec\n{}&#39;.format(prior_prec))
        raise
    priorvar_logdet = log_det(prior_var, self)
    ll2 = 0.5 * (beta_logdet - priorvar_logdet)

    a = np.sum((prior_mean.ravel() ** 2) *prior_prec_diag)
    # np.asarray(prior_mean.T @ prior_prec @ prior_mean)[0,0]
    b = np.asarray(beta_mean.T @ beta_prec @ beta_mean)[0,0]
    ll3 = -0.5 * (a  - b)

    return ll2+ll3</code></pre>
</details>
</dd>
<dt id="mdsine2.clustering.ClusterAssignments.calculate_marginal_loglikelihood_slow_fast_sparse"><code class="name flex">
<span>def <span class="ident">calculate_marginal_loglikelihood_slow_fast_sparse</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Marginalizes out the interactions and the perturbations</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calculate_marginal_loglikelihood_slow_fast_sparse(self):
    &#39;&#39;&#39;Marginalizes out the interactions and the perturbations
    &#39;&#39;&#39;
    # Build the parameters
    # ====================
    if self._there_are_perturbations:
        rhs = [REPRNAMES.PERT_VALUE, REPRNAMES.CLUSTER_INTERACTION_VALUE]
    else:
        rhs = [REPRNAMES.CLUSTER_INTERACTION_VALUE]
    
    
    y = self.y
    X = self.G.data.construct_rhs(keys=rhs, toarray=False)

    process_prec = self.process_prec_matrix
    prior_prec = build_prior_covariance(G=self.G, cov=False, order=rhs, sparse=True)  
    prior_prec_diag = build_prior_covariance(G=self.G, cov=False, order=rhs, diag=True)        
    prior_var = build_prior_covariance(G=self.G, cov=True, order=rhs, sparse=True)
    prior_mean = build_prior_mean(G=self.G, order=rhs, shape=(-1,1))

    # Calculate the marginalization
    # =============================

    # print(&#39;X&#39;)
    # print(type(X))
    # print(X.shape)

    # print(&#39;process_prec&#39;)
    # print(type(process_prec))
    # print(process_prec.shape)

    # print(&#39;prior_prec&#39;)
    # print(type(prior_prec))
    # print(prior_prec.shape)

    # print(&#39;prior mean&#39;)
    # print(type(prior_mean))
    # print(prior_mean.shape)

    a = X.T.dot(process_prec)
    beta_prec = a.dot(X) + prior_prec
    beta_cov = pinv(beta_prec, self)
    beta_mean = beta_cov @ ( a.dot(y) + prior_prec.dot(prior_mean))
    beta_mean = np.asarray(beta_mean).reshape(-1,1)

    try:
        beta_logdet = log_det(beta_cov, self)
    except:
        logging.critical(&#39;Crashed in log_det&#39;)
        logging.critical(&#39;beta_cov:\n{}&#39;.format(beta_cov))
        logging.critical(&#39;prior_prec\n{}&#39;.format(prior_prec))
        raise
    priorvar_logdet = log_det(prior_var, self)
    ll2 = 0.5 * (beta_logdet - priorvar_logdet)

    a = np.sum((prior_mean.ravel() ** 2) *prior_prec_diag)
    # np.asarray(prior_mean.T @ prior_prec @ prior_mean)[0,0]

    # print(&#39;beta_prec.shape&#39;, beta_prec.shape)
    # print(&#39;beta_mean.shape&#39;, beta_mean.shape)

    b = np.asarray(beta_mean.T @ beta_prec.dot(beta_mean))[0,0]
    ll3 = -0.5 * (a  - b)

    return ll2+ll3</code></pre>
</details>
</dd>
<dt id="mdsine2.clustering.ClusterAssignments.gibbs_update_single_asv_parallel"><code class="name flex">
<span>def <span class="ident">gibbs_update_single_asv_parallel</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Update for a single asvs</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def gibbs_update_single_asv_parallel(self):
    &#39;&#39;&#39;Update for a single asvs
    &#39;&#39;&#39;
    self.original_cluster = self.clustering.idx2cid[self.oidx]
    self.curr_cluster = self.original_cluster

    interactions = self.G[REPRNAMES.INTERACTIONS_OBJ]
    interaction_on_idxs = interactions.get_indicators(return_idxs=True)
    if self._there_are_perturbations:
        perturbation_on_idxs = [p.indicator.cluster_arg_array() for p in self.G.perturbations]
    else:
        perturbation_on_idxs = None

    # # Send topology parameters if the topology wont change
    # TODO: this works on windows but not on the cluster when dispatching?
    # if self.clustering.clusters[self.original_cluster].size &gt; 1:
    #     use_saved_params = True
    #     kwargs = {
    #         &#39;interaction_on_idxs&#39;: interaction_on_idxs,
    #         &#39;perturbation_on_idxs&#39;: perturbation_on_idxs}
    #     if pl.ispersistentpool(self.pool):
    #         self.pool.map(&#39;initialize_oidx&#39;, [kwargs]*self.pool.num_workers)
    #     else:
    #         use_saved_params = False
    # else:
    #     use_saved_params = False
    use_saved_params = False

    if pl.ispersistentpool(self.pool):
        self.pool.staged_map_start(&#39;run&#39;)
    else:
        notpool_ret = []

    # Get the likelihood of the current configuration
    if self.clustering.clusters[self.original_cluster].size == 1:
        log_mult_factor = math.log(self.concentration/self.m)
    else:
        log_mult_factor = math.log(self.clustering.clusters[self.original_cluster].size - 1)

    if use_saved_params and pl.ispersistentpool(self.pool):
        interaction_on_idxs = None
        perturbation_on_idxs = None

    cluster_config = np.asarray([self.clustering.cid2cidx[self.clustering.idx2cid[i]] \
        for i in range(len(self.G.data.asvs))])

    kwargs = {
        &#39;interaction_on_idxs&#39;: interaction_on_idxs,
        &#39;perturbation_on_idxs&#39;: perturbation_on_idxs,
        &#39;cluster_config&#39;: cluster_config,
        &#39;log_mult_factor&#39;: log_mult_factor,
        &#39;cid&#39;: self.original_cluster,
        &#39;use_saved_params&#39;: use_saved_params}

    if pl.ispersistentpool(self.pool):
        self.pool.staged_map_put(kwargs)
    else:
        notpool_ret.append(self.pool.run(**kwargs))

    # Check every cluster
    for cid in self.clustering.order:
        if cid == self.original_cluster:
            continue
        self.clustering.move_item(idx=self.oidx, cid=cid)
        self.curr_cluster = cid

        if not use_saved_params:
            interaction_on_idxs = interactions.get_indicators(return_idxs=True)
            if self._there_are_perturbations:
                perturbation_on_idxs = [p.indicator.cluster_arg_array() for p in self.G.perturbations]
            else:
                perturbation_on_idxs = None

        cluster_config = np.asarray([self.clustering.cid2cidx[self.clustering.idx2cid[i]] \
            for i in range(len(self.G.data.asvs))])
        log_mult_factor = np.log(self.clustering.clusters[self.curr_cluster].size - 1)

        kwargs = {
            &#39;interaction_on_idxs&#39;: interaction_on_idxs,
            &#39;perturbation_on_idxs&#39;: perturbation_on_idxs,
            &#39;cluster_config&#39;: cluster_config,
            &#39;log_mult_factor&#39;: log_mult_factor,
            &#39;cid&#39;: self.curr_cluster,
            &#39;use_saved_params&#39;: use_saved_params}

        if pl.ispersistentpool(self.pool):
            self.pool.staged_map_put(kwargs)
        else:
            notpool_ret.append(self.pool.run(**kwargs))

    # Make a new cluster
    self.curr_cluster = self.clustering.make_new_cluster_with(idx=self.oidx)
    cluster_config = np.asarray([self.clustering.cid2cidx[self.clustering.idx2cid[i]] \
        for i in range(len(self.G.data.asvs))])
    interaction_on_idxs = interactions.get_indicators(return_idxs=True)
    if self._there_are_perturbations:
        perturbation_on_idxs = [p.indicator.cluster_arg_array() for p in self.G.perturbations]
    else:
        perturbation_on_idxs = None
    log_mult_factor = np.log(self.concentration/self.m)

    kwargs = {
        &#39;interaction_on_idxs&#39;: interaction_on_idxs,
        &#39;perturbation_on_idxs&#39;: perturbation_on_idxs,
        &#39;cluster_config&#39;: cluster_config,
        &#39;log_mult_factor&#39;: log_mult_factor,
        &#39;cid&#39;: self.curr_cluster,
        &#39;use_saved_params&#39;: False}

    # Put the values and get if necessary
    KEYS = []
    LOG_P = []
    if pl.ispersistentpool(self.pool):
        self.pool.staged_map_put(kwargs)
        ret = self.pool.staged_map_get()
    else:
        notpool_ret.append(self.pool.run(**kwargs))
        ret = notpool_ret
    for c, p in ret:
        KEYS.append(c)
        LOG_P.append(p)

    idx = sample_categorical_log(LOG_P)
    assigned_cid = KEYS[idx]

    if assigned_cid != self.original_cluster:
        logging.info(&#39;cluster changed&#39;)

    self.clustering.move_item(idx=self.oidx, cid=assigned_cid)

    self.G[REPRNAMES.CLUSTER_INTERACTION_INDICATOR].update_cnt_indicators()
    self.G.data.design_matrices[REPRNAMES.CLUSTER_INTERACTION_VALUE].M.build()
    if self._there_are_perturbations:
        self.G.data.design_matrices[REPRNAMES.PERT_VALUE].M.build()</code></pre>
</details>
</dd>
<dt id="mdsine2.clustering.ClusterAssignments.gibbs_update_single_asv_slow"><code class="name flex">
<span>def <span class="ident">gibbs_update_single_asv_slow</span></span>(<span>self, oidx)</span>
</code></dt>
<dd>
<div class="desc"><p>The update function is based off of Algorithm 8 in 'Markov Chain
Sampling Methods for Dirichlet Process Mixture Models' by Radford M.
Neal, 2000.</p>
<p>Calculate the marginal likelihood of the asv in every cluster
and a new cluster then sample from <code>self.sample_categorical_log</code>
to get the cluster assignment.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>oidx</code></strong> :&ensp;<code>int</code></dt>
<dd>ASV index that we are updating the cluster assignment of</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def gibbs_update_single_asv_slow(self, oidx):
    &#39;&#39;&#39;The update function is based off of Algorithm 8 in &#39;Markov Chain
    Sampling Methods for Dirichlet Process Mixture Models&#39; by Radford M.
    Neal, 2000.

    Calculate the marginal likelihood of the asv in every cluster
    and a new cluster then sample from `self.sample_categorical_log`
    to get the cluster assignment.

    Parameters
    ----------
    oidx : int
        ASV index that we are updating the cluster assignment of
    &#39;&#39;&#39;
    curr_cluster = self.clustering.idx2cid[oidx]
    concentration = self.concentration.value

    # start as a dictionary then send values to `sample_categorical_log`
    LOG_P = []
    LOG_KEYS = []

    # Calculate current cluster
    # =========================
    # If the element is already in its own cluster, use the new cluster case
    if self.clustering.clusters[curr_cluster].size == 1:
        a = np.log(concentration/self.m)
    else:
        a = np.log(self.clustering.clusters[curr_cluster].size - 1)
    LOG_P.append(a + self.calculate_marginal_loglikelihood_slow()[&#39;ret&#39;])
    LOG_KEYS.append(curr_cluster)

    # Calculate going to every other cluster
    # ======================================
    for cid in self.clustering.order:
        if curr_cluster == cid:
            continue

        # Move ASV and recompute the matrices
        self.clustering.move_item(idx=oidx,cid=cid)
        self.G[REPRNAMES.CLUSTER_INTERACTION_INDICATOR].update_cnt_indicators()
        self.G.data.design_matrices[REPRNAMES.CLUSTER_INTERACTION_VALUE].M.build()
        if self._there_are_perturbations:
            self.G.data.design_matrices[REPRNAMES.PERT_VALUE].M.build()

        LOG_P.append(np.log(self.clustering.clusters[cid].size - 1) + \
            self.calculate_marginal_loglikelihood_slow()[&#39;ret&#39;])
        LOG_KEYS.append(cid)


    # Calculate new cluster
    # =====================
    cid=self.clustering.make_new_cluster_with(idx=oidx)
    self.G[REPRNAMES.CLUSTER_INTERACTION_INDICATOR].update_cnt_indicators()
    self.G.data.design_matrices[REPRNAMES.CLUSTER_INTERACTION_VALUE].M.build()
    if self._there_are_perturbations:
        self.G.data.design_matrices[REPRNAMES.PERT_VALUE].M.build()
    
    LOG_KEYS.append(cid)
    LOG_P.append(np.log(concentration/self.m) + \
        self.calculate_marginal_loglikelihood_slow()[&#39;ret&#39;])

    # Sample the assignment
    # =====================
    idx = sample_categorical_log(LOG_P)
    assigned_cid = LOG_KEYS[idx]
    curr_clus = self.clustering.idx2cid[oidx]

    if assigned_cid != curr_clus:
        self.clustering.move_item(idx=oidx,cid=assigned_cid)
        self.G[REPRNAMES.CLUSTER_INTERACTION_INDICATOR].update_cnt_indicators()

        # Change the mixing matrix for the interactions and (potentially) perturbations
        self.G.data.design_matrices[REPRNAMES.CLUSTER_INTERACTION_VALUE].M.build()
        if self._there_are_perturbations:
            self.G.data.design_matrices[REPRNAMES.PERT_VALUE].M.build()</code></pre>
</details>
</dd>
<dt id="mdsine2.clustering.ClusterAssignments.gibbs_update_single_asv_slow_fast"><code class="name flex">
<span>def <span class="ident">gibbs_update_single_asv_slow_fast</span></span>(<span>self, oidx)</span>
</code></dt>
<dd>
<div class="desc"><p>The update function is based off of Algorithm 8 in 'Markov Chain
Sampling Methods for Dirichlet Process Mixture Models' by Radford M.
Neal, 2000.</p>
<p>Calculate the marginal likelihood of the asv in every cluster
and a new cluster then sample from <code>self.sample_categorical_log</code>
to get the cluster assignment.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>oidx</code></strong> :&ensp;<code>int</code></dt>
<dd>ASV index that we are updating the cluster assignment of</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def gibbs_update_single_asv_slow_fast(self, oidx):
    &#39;&#39;&#39;The update function is based off of Algorithm 8 in &#39;Markov Chain
    Sampling Methods for Dirichlet Process Mixture Models&#39; by Radford M.
    Neal, 2000.

    Calculate the marginal likelihood of the asv in every cluster
    and a new cluster then sample from `self.sample_categorical_log`
    to get the cluster assignment.

    Parameters
    ----------
    oidx : int
        ASV index that we are updating the cluster assignment of
    &#39;&#39;&#39;
    curr_cluster = self.clustering.idx2cid[oidx]
    concentration = self.concentration.value

    # start as a dictionary then send values to `sample_categorical_log`
    LOG_P = []
    LOG_KEYS = []

    # Calculate current cluster
    # =========================
    # If the element is already in its own cluster, use the new cluster case
    if self.clustering.clusters[curr_cluster].size == 1:
        a = np.log(concentration/self.m)
    else:
        a = np.log(self.clustering.clusters[curr_cluster].size - 1)
    LOG_P.append(a + self.calculate_marginal_loglikelihood_slow_fast_sparse())
    LOG_KEYS.append(curr_cluster)

    # Calculate going to every other cluster
    # ======================================
    for cid in self.clustering.order:
        if curr_cluster == cid:
            continue

        # Move ASV and recompute the matrices
        self.clustering.move_item(idx=oidx,cid=cid)
        self.G[REPRNAMES.CLUSTER_INTERACTION_INDICATOR].update_cnt_indicators()
        self.G.data.design_matrices[REPRNAMES.CLUSTER_INTERACTION_VALUE].M.build()
        if self._there_are_perturbations:
            self.G.data.design_matrices[REPRNAMES.PERT_VALUE].M.build()

        LOG_P.append(np.log(self.clustering.clusters[cid].size - 1) + \
            self.calculate_marginal_loglikelihood_slow_fast_sparse())
        LOG_KEYS.append(cid)


    # Calculate new cluster
    # =====================
    cid=self.clustering.make_new_cluster_with(idx=oidx)
    self.G[REPRNAMES.CLUSTER_INTERACTION_INDICATOR].update_cnt_indicators()
    self.G.data.design_matrices[REPRNAMES.CLUSTER_INTERACTION_VALUE].M.build()
    if self._there_are_perturbations:
        self.G.data.design_matrices[REPRNAMES.PERT_VALUE].M.build()
    
    LOG_KEYS.append(cid)
    LOG_P.append(np.log(concentration/self.m) + \
        self.calculate_marginal_loglikelihood_slow_fast_sparse())

    # Sample the assignment
    # =====================
    idx = sample_categorical_log(LOG_P)
    assigned_cid = LOG_KEYS[idx]
    curr_clus = self.clustering.idx2cid[oidx]

    if assigned_cid != curr_clus:
        self.clustering.move_item(idx=oidx,cid=assigned_cid)
        self.G[REPRNAMES.CLUSTER_INTERACTION_INDICATOR].update_cnt_indicators()

        # Change the mixing matrix for the interactions and (potentially) perturbations
        self.G.data.design_matrices[REPRNAMES.CLUSTER_INTERACTION_VALUE].M.build()
        if self._there_are_perturbations:
            self.G.data.design_matrices[REPRNAMES.PERT_VALUE].M.build()</code></pre>
</details>
</dd>
<dt id="mdsine2.clustering.ClusterAssignments.initialize"><code class="name flex">
<span>def <span class="ident">initialize</span></span>(<span>self, value_option, hyperparam_option=None, value=None, n_clusters=None, delay=0, run_every_n_iterations=1)</span>
</code></dt>
<dd>
<div class="desc"><p>Initialize the cluster assingments - there are no hyperparamters to
initialize because the concentration is initialized somewhere else</p>
<p>Note - if <code>n_clusters</code> is not specified and the cluster initialization
method requires it - it will be set to the expected number of clusters
which = log(n_asvs)/log(2)</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>value_option</code></strong> :&ensp;<code>str</code></dt>
<dd>The different methods to initialize the clusters
Options
'manual'
Manually set the cluster assignments
'no-clusters'
Every ASV in their own cluster
'random'
Every ASV is randomly assigned to the number of clusters. <code>n_clusters</code> required
'taxonomy'
Cluster ASVs based on their taxonomic similarity. <code>n_clusters</code> required
'sequence'
Cluster ASVs based on their sequence similarity. <code>n_clusters</code> required
'phylogeny'
Cluster ASVs based on their phylogenetic similarity. <code>n_clusters</code> required
'spearman', 'auto'
Creates a distance matrix based on the spearman rank similarity
between two trajectories. We use the raw data. <code>n_clusters</code> required
'fixed-topology'
Sets the clustering assignment to the most likely clustering configuration
specified in the graph at the location <code>value</code> (<code>value</code> is a str).
We take the mean coclusterings and do agglomerative clustering on that matrix
with the <code>mode</code> number of clusters.</dd>
<dt><strong><code>hyperparam_option</code></strong> :&ensp;<code>None</code></dt>
<dd>Not used in this function - only here for API consistency</dd>
<dt><strong><code>value</code></strong> :&ensp;<code>list</code> of <code>list</code></dt>
<dd>Cluster assingments for each of the ASVs
Only necessary if <code>value_option</code> == 'manual'</dd>
<dt><strong><code>n_clusters</code></strong> :&ensp;<code>int, str</code></dt>
<dd>Necessary if <code>value_option</code> is not 'manual' or 'no-clusters'
If str, options:
'expected', 'auto': log_2(n_asvs)</dd>
<dt><strong><code>run_every_n_iterations</code></strong> :&ensp;<code>int</code></dt>
<dd>Only run the update every <code>run_every_n_iterations</code> iterations</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def initialize(self, value_option, hyperparam_option=None, value=None, n_clusters=None,
    delay=0, run_every_n_iterations=1):
    &#39;&#39;&#39;Initialize the cluster assingments - there are no hyperparamters to
    initialize because the concentration is initialized somewhere else

    Note - if `n_clusters` is not specified and the cluster initialization
    method requires it - it will be set to the expected number of clusters
    which = log(n_asvs)/log(2)

    Parameters
    ----------
    value_option : str
        The different methods to initialize the clusters
        Options
            &#39;manual&#39;
                Manually set the cluster assignments
            &#39;no-clusters&#39;
                Every ASV in their own cluster
            &#39;random&#39;
                Every ASV is randomly assigned to the number of clusters. `n_clusters` required
            &#39;taxonomy&#39;
                Cluster ASVs based on their taxonomic similarity. `n_clusters` required
            &#39;sequence&#39;
                Cluster ASVs based on their sequence similarity. `n_clusters` required
            &#39;phylogeny&#39;
                Cluster ASVs based on their phylogenetic similarity. `n_clusters` required
            &#39;spearman&#39;, &#39;auto&#39;
                Creates a distance matrix based on the spearman rank similarity
                between two trajectories. We use the raw data. `n_clusters` required
            &#39;fixed-topology&#39;
                Sets the clustering assignment to the most likely clustering configuration
                specified in the graph at the location `value` (`value` is a str).
                We take the mean coclusterings and do agglomerative clustering on that matrix
                with the `mode` number of clusters.
    hyperparam_option : None
        Not used in this function - only here for API consistency
    value : list of list
        Cluster assingments for each of the ASVs
        Only necessary if `value_option` == &#39;manual&#39;
    n_clusters : int, str
        Necessary if `value_option` is not &#39;manual&#39; or &#39;no-clusters&#39;
        If str, options:
            &#39;expected&#39;, &#39;auto&#39;: log_2(n_asvs)
    run_every_n_iterations : int
        Only run the update every `run_every_n_iterations` iterations
    &#39;&#39;&#39;
    from sklearn.cluster import AgglomerativeClustering
    asvs = self.G.data.asvs

    if not pl.isint(run_every_n_iterations):
        raise TypeError(&#39;`run_every_n_iterations` ({}) must be an int&#39;.format(
            type(run_every_n_iterations)))
    if run_every_n_iterations &lt;= 0:
        raise ValueError(&#39;`run_every_n_iterations` ({}) must be &gt;= 0&#39;.format(
            run_every_n_iterations))

    self.run_every_n_iterations = run_every_n_iterations
    self.delay = delay

    if not pl.isstr(value_option):
        raise TypeError(&#39;`value_option` ({}) must be a str&#39;.format(type(value_option)))
    if value_option not in [&#39;manual&#39;, &#39;no-clusters&#39;, &#39;fixed-topology&#39;]:
        if pl.isstr(n_clusters):
            if n_clusters in [&#39;expected&#39;, &#39;auto&#39;]:
                n_clusters = int(round(np.log(len(asvs))/np.log(2)))
            else:
                raise ValueError(&#39;`n_clusters` ({}) not recognized&#39;.format(n_clusters))
        if not pl.isint(n_clusters):
            raise TypeError(&#39;`n_clusters` ({}) must be a str or an int&#39;.format(type(n_clusters)))
        if n_clusters &lt;= 0:
            raise ValueError(&#39;`n_clusters` ({}) must be &gt; 0&#39;.format(n_clusters))
        if n_clusters &gt; self.G.data.n_asvs:
            raise ValueError(&#39;`n_clusters` ({}) must be &lt;= than the number of ASVs ({})&#39;.format(
                n_clusters, self.G.data.n_asvs))

    if value_option == &#39;manual&#39;:
        # Check that all of the ASVs are in the init and that it is in the right
        # structure
        if not pl.isarray(value):
            raise ValueError(&#39;if `value_option` is &#34;manual&#34;, value ({}) must &#39; \
                &#39;be of type array&#39;.format(value.__class__))
        clusters = list(value)

        idxs_to_delete = []
        for idx, cluster in enumerate(clusters):
            if not pl.isarray(cluster):
                raise ValueError(&#39;cluster at index `{}` ({}) is not an array&#39;.format(
                    idx, cluster))
            cluster = list(cluster)
            if len(cluster) == 0:
                logging.warning(&#39;Cluster index {} has 0 elements, deleting&#39;.format(
                    idx))
                idxs_to_delete.append(idx)
        if len(idxs_to_delete) &gt; 0:
            clusters = np.delete(clusters, idxs_to_delete).tolist()

        all_oidxs = set()
        for cluster in clusters:
            for oidx in cluster:
                if not pl.isint(oidx):
                    raise ValueError(&#39;`oidx` ({}) must be an int&#39;.format(oidx.__class__))
                if oidx &gt;= len(asvs):
                    raise ValueError(&#39;oidx `{}` not in our ASVSet&#39;.format(oidx))
                all_oidxs.add(oidx)

        for oidx in range(len(asvs)):
            if oidx not in all_oidxs:
                raise ValueError(&#39;oidx `{}` in ASVSet not in `value` ({})&#39;.format(
                    oidx, value))
        # Now everything is checked and valid

    elif value_option == &#39;fixed-topology&#39;:
        logging.info(&#39;Fixed topology initialization&#39;)
        if not pl.isstr(value):
            raise TypeError(&#39;`value` ({}) must be a str&#39;.format(value))

        CHAIN2 = pl.inference.BaseMCMC.load(value)
        CLUSTERING2 = CHAIN2.graph[STRNAMES.CLUSTERING_OBJ]
        ASVS2 = CHAIN2.graph.data.asvs
        asvs_curr = self.G.data.asvs
        for asv in ASVS2:
            if asv.name not in asvs_curr:
                raise ValueError(&#39;Cannot perform fixed topology because the ASV {} in &#39; \
                    &#39;the passed in clustering is not in this clustering: {}&#39;.format(
                        asv.name, asvs_curr.names.order))
        for asv in asvs_curr:
            if asv.name not in ASVS2:
                raise ValueError(&#39;Cannot perform fixed topology because the ASV {} in &#39; \
                    &#39;the current clustering is not in the passed in clustering: {}&#39;.format(
                        asv.name, ASVS2.names.order))

        # Get the most likely cluster configuration and set as the value for the passed in cluster
        ret = generate_cluster_assignments_posthoc(CLUSTERING2, n_clusters=&#39;mode&#39;, set_as_value=True)
        ca = {}
        for aidx, cidx in enumerate(ret):
            if cidx not in ca:
                ca[cidx] = []
            ca[cidx].append(aidx)
        ret = []
        for v in ca.values():
            ret.append(v)
        CLUSTERING2.from_array(ret)
        logging.info(&#39;Clustering set to:\n{}&#39;.format(str(CLUSTERING2)))

        # Set the passed in cluster assignment as the current cluster assignment
        # Need to be careful because the indices of the ASVs might not line up
        clusters = []
        for cluster in CLUSTERING2:
            anames = [asvs_curr[ASVS2.names.order[aidx]].name for aidx in cluster.members]
            aidxs = [asvs_curr[aname].idx for aname in anames]
            clusters.append(aidxs)

    elif value_option == &#39;no-clusters&#39;:
        clusters = []
        for oidx in range(len(asvs)):
            clusters.append([oidx])

    elif value_option == &#39;random&#39;:
        clusters = {}
        for oidx in range(len(asvs)):
            idx = npr.choice(n_clusters)
            if idx in clusters:
                clusters[idx].append(oidx)
            else:
                clusters[idx] = [oidx]
        c = []
        for cid in clusters.keys():
            c.append(clusters[cid])
        clusters = c

    elif value_option == &#39;taxonomy&#39;:
        # Create an affinity matrix, we can precompute the self-similarity to 1
        M = np.diag(np.ones(len(asvs), dtype=float))
        for i, oid1 in enumerate(asvs.ids.order):
            for j, oid2 in enumerate(asvs.ids.order):
                if i == j:
                    continue
                M[i,j] = asvs.taxonomic_similarity(oid1=oid1, oid2=oid2)

        c = AgglomerativeClustering(
            n_clusters=n_clusters,
            affinity=&#39;precomputed&#39;,
            linkage=&#39;complete&#39;)
        assignments = c.fit_predict(1-M)

        # Convert assignments into clusters
        clusters = {}
        for oidx,cidx in enumerate(assignments):
            if cidx not in clusters:
                clusters[cidx] = []
            clusters[cidx].append(oidx)
        clusters = [val for val in clusters.values()]

    elif value_option == &#39;sequence&#39;:
        import diversity

        logging.info(&#39;Making affinity matrix from sequences&#39;)
        evenness = np.diag(np.ones(len(self.G.data.asvs), dtype=float))

        for i in range(len(self.G.data.asvs)):
            for j in range(len(self.G.data.asvs)):
                if j &lt;= i:
                    continue
                # Subtract because we want to make a similarity matrix
                dist = 1-diversity.beta.hamming(
                    list(self.G.data.asvs[i].sequence),
                    list(self.G.data.asvs[j].sequence))
                evenness[i,j] = dist
                evenness[j,i] = dist

        c = AgglomerativeClustering(
            n_clusters=n_clusters,
            affinity=&#39;precomputed&#39;,
            linkage=&#39;average&#39;)
        assignments = c.fit_predict(evenness)
        clusters = {}
        for oidx,cidx in enumerate(assignments):
            if cidx not in clusters:
                clusters[cidx] = []
            clusters[cidx].append(oidx)
        clusters = [val for val in clusters.values()]

    elif value_option == &#39;spearman&#39;:
        # Use spearman correlation to create a distance matrix
        # Use agglomerative clustering to make the clusters based
        # on distance matrix (distance = 1 - pearson(x,y))
        dm = np.zeros(shape=(len(asvs), len(asvs)))
        data = []
        for ridx in range(self.G.data.n_replicates):
            data.append(self.G.data.abs_data[ridx])
        data = np.hstack(data)
        for i in range(len(asvs)):
            for j in range(i+1):
                distance = (1 - scipy.stats.spearmanr(data[i, :], data[j, :])[0])/2
                dm[i,j] = distance
                dm[j,i] = distance

        c = AgglomerativeClustering(
            n_clusters=n_clusters,
            affinity=&#39;precomputed&#39;,
            linkage=&#39;complete&#39;)
        assignments = c.fit_predict(dm)

        # convert into clusters
        clusters = {}
        for oidx, cidx in enumerate(assignments):
            if cidx not in clusters:
                clusters[cidx] = []
            clusters[cidx].append(oidx)
        clusters = [val for val in clusters.values()]

    elif value_option == &#39;phylogeny&#39;:
        raise NotImplementedError(&#39;`phylogeny` not implemented yet&#39;)

    else:
        raise ValueError(&#39;`value_option` &#34;{}&#34; not recognized&#39;.format(value_option))

    # Move all the ASVs into their assigned clusters
    for cluster in clusters:
        cid = None
        for oidx in cluster:
            if cid is None:
                # make new cluster
                cid = self.clustering.make_new_cluster_with(idx=oidx)
            else:
                self.clustering.move_item(idx=oidx, cid=cid)
    logging.info(&#39;Cluster Assingments initialization results:\n{}&#39;.format(
        str(self.clustering)))
    self._there_are_perturbations = self.G.perturbations is not None

    # Initialize the multiprocessors if necessary
    if self.mp is not None:
        if not pl.isstr(self.mp):
            raise TypeError(&#39;`mp` ({}) must be a str&#39;.format(type(self.mp)))
        if &#39;full&#39; in self.mp:
            n_cpus = self.mp.split(&#39;-&#39;)[1]
            if n_cpus == &#39;auto&#39;:
                self.n_cpus = psutil.cpu_count(logical=False)
            else:
                try:
                    self.n_cpus = int(n_cpus)
                except:
                    raise ValueError(&#39;`mp` ({}) not recognized&#39;.format(self.mp))
            self.pool = pl.multiprocessing.PersistentPool(ptype=&#39;dasw&#39;, G=self.G)
        elif self.mp == &#39;debug&#39;:
            self.pool = None
        else:
            raise ValueError(&#39;`mp` ({}) not recognized&#39;.format(self.mp))
    else:
        self.pool = None

    self.ndts_bias = []
    self.n_asvs = len(self.G.data.asvs)
    self.n_replicates = self.G.data.n_replicates
    self.n_dts_for_replicate = self.G.data.n_dts_for_replicate
    self.total_dts = np.sum(self.n_dts_for_replicate)
    for ridx in range(self.G.data.n_replicates):
        self.ndts_bias.append(
            np.arange(0, self.G.data.n_dts_for_replicate[ridx] * self.n_asvs, self.n_asvs))
    self.replicate_bias = np.zeros(self.n_replicates, dtype=int)
    for ridx in range(1, self.n_replicates):
        self.replicate_bias[ridx] = self.replicate_bias[ridx-1] + \
            self.n_asvs * self.n_dts_for_replicate[ridx - 1]</code></pre>
</details>
</dd>
<dt id="mdsine2.clustering.ClusterAssignments.kill"><code class="name flex">
<span>def <span class="ident">kill</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def kill(self):
    if pl.ispersistentpool(self.pool):
        # For pylab multiprocessing, explicitly kill them
        self.pool.kill()
    return</code></pre>
</details>
</dd>
<dt id="mdsine2.clustering.ClusterAssignments.set_trace"><code class="name flex">
<span>def <span class="ident">set_trace</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_trace(self):
    self.clustering.set_trace()</code></pre>
</details>
</dd>
<dt id="mdsine2.clustering.ClusterAssignments.update_mp"><code class="name flex">
<span>def <span class="ident">update_mp</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Implements <code>update_slow</code> but parallelizes calculating the likelihood
of being in a cluster. NOTE that this does not parallelize on the ASV level.</p>
<p>On the first gibb step with initialize the workers that we implement with DASW (
different arguments, single worker). For more information what this means look
at pylab.multiprocessing documentation.</p>
<p>If we initialized our pool as a pylab.multiprocessing.PersistentPool, then we
multiprocess the likelihood calculations for each asv. If we didnt then this
implementation has the same performance as <code><a title="mdsine2.clustering.ClusterAssignments.update_slow_fast" href="#mdsine2.clustering.ClusterAssignments.update_slow_fast">ClusterAssignments.update_slow_fast()</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_mp(self):
    &#39;&#39;&#39;Implements `update_slow` but parallelizes calculating the likelihood
    of being in a cluster. NOTE that this does not parallelize on the ASV level.

    On the first gibb step with initialize the workers that we implement with DASW (
    different arguments, single worker). For more information what this means look
    at pylab.multiprocessing documentation.

    If we initialized our pool as a pylab.multiprocessing.PersistentPool, then we 
    multiprocess the likelihood calculations for each asv. If we didnt then this 
    implementation has the same performance as `ClusterAssignments.update_slow_fast`.
    &#39;&#39;&#39;
    if self.G.data.zero_inflation_transition_policy is not None:
        raise NotImplementedError(&#39;Multiprocessing for zero inflation data is not implemented yet.&#39; \
            &#39; Use `mp=None`&#39;)
    DMI = self.G.data.design_matrices[REPRNAMES.CLUSTER_INTERACTION_VALUE]
    DMP = self.G.data.design_matrices[REPRNAMES.PERT_VALUE]
    if self.clustering.n_clusters.sample_iter == 0 or self.pool == []:
        kwargs = {
            &#39;n_asvs&#39;: len(self.G.data.asvs),
            &#39;total_n_dts_per_asv&#39;: self.G.data.total_n_dts_per_asv,
            &#39;n_replicates&#39;: self.G.data.n_replicates,
            &#39;n_dts_for_replicate&#39;: self.G.data.n_dts_for_replicate,
            &#39;there_are_perturbations&#39;: self._there_are_perturbations,
            &#39;keypair2col_interactions&#39;: DMI.M.keypair2col,
            &#39;keypair2col_perturbations&#39;: DMP.M.keypair2col,
            &#39;n_perturbations&#39;: len(self.G.perturbations) if self._there_are_perturbations else None,
            &#39;base_Xrows&#39;: DMI.base.rows,
            &#39;base_Xcols&#39;: DMI.base.cols,
            &#39;base_Xshape&#39;: DMI.base.shape,
            &#39;base_Xpertrows&#39;: DMP.base.rows,
            &#39;base_Xpertcols&#39;: DMP.base.cols,
            &#39;base_Xpertshape&#39;: DMP.base.shape,
            &#39;n_rowsM&#39;: DMI.M.n_rows,
            &#39;n_rowsMpert&#39;: DMP.M.n_rows}

        if pl.ispersistentpool(self.pool):
            for _ in range(self.n_cpus):
                self.pool.add_worker(SingleClusterFullParallelization(**kwargs))
        else:
            self.pool = SingleClusterFullParallelization(**kwargs)

    if self.clustering.n_clusters.sample_iter &lt; self.delay:
        return

    if self.clustering.n_clusters.sample_iter % self.run_every_n_iterations != 0:
        return

    # Send in arguments for the start of the gibbs step
    start_time = time.time()
    base_Xdata = DMI.base.data
    self.concentration = self.G[REPRNAMES.CONCENTRATION].value
    y = self.G.data.construct_lhs(keys=[REPRNAMES.GROWTH_VALUE, REPRNAMES.SELF_INTERACTION_VALUE],
        kwargs_dict={REPRNAMES.GROWTH_VALUE: {&#39;with_perturbations&#39;:False}})
    prior_var_interactions = self.G[REPRNAMES.PRIOR_VAR_INTERACTIONS].value
    prior_mean_interactions = self.G[REPRNAMES.PRIOR_MEAN_INTERACTIONS].value
    process_prec_diag = self.G[REPRNAMES.PROCESSVAR].prec

    if self._there_are_perturbations:
        prior_var_pert = self.G[REPRNAMES.PRIOR_VAR_PERT].get_single_value_of_perts()
        prior_mean_pert = self.G[REPRNAMES.PRIOR_MEAN_PERT].get_single_value_of_perts()
        base_Xpertdata = DMP.base.data
    else:
        prior_var_pert = None
        prior_mean_pert = None
        base_Xpertdata = None

    kwargs = {
        &#39;base_Xdata&#39;: base_Xdata,
        &#39;base_Xpertdata&#39;: base_Xpertdata,
        &#39;concentration&#39;: self.concentration,
        &#39;m&#39;: self.m,
        &#39;y&#39;: y,
        &#39;process_prec_diag&#39;: process_prec_diag,
        &#39;prior_var_interactions&#39;: prior_var_interactions,
        &#39;prior_var_pert&#39;: prior_var_pert,
        &#39;prior_mean_interactions&#39;: prior_mean_interactions,
        &#39;prior_mean_pert&#39;: prior_mean_pert}
    
    if pl.ispersistentpool(self.pool):
        self.pool.map(&#39;initialize_gibbs&#39;, [kwargs]*self.pool.num_workers)
    else:
        self.pool.initialize_gibbs(**kwargs)

    oidxs = npr.permutation(np.arange(len(self.G.data.asvs)))
    for iii, oidx in enumerate(oidxs):
        logging.info(&#39;{}/{} - {}&#39;.format(iii, len(self.G.data.asvs), oidx))
        self.oidx = oidx
        self.gibbs_update_single_asv_parallel()

    self._strtime = time.time() - start_time</code></pre>
</details>
</dd>
<dt id="mdsine2.clustering.ClusterAssignments.update_slow"><code class="name flex">
<span>def <span class="ident">update_slow</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>This is updating the new cluster. Depending on the iteration you do
either split-merge Metropolis-Hasting update or a regular Gibbs update. To
get highest mixing we alternate between each.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_slow(self):
    &#39;&#39;&#39; This is updating the new cluster. Depending on the iteration you do
    either split-merge Metropolis-Hasting update or a regular Gibbs update. To
    get highest mixing we alternate between each.
    &#39;&#39;&#39;
    if self.clustering.n_clusters.sample_iter &lt; self.delay:
        return

    if self.clustering.n_clusters.sample_iter % self.run_every_n_iterations != 0:
       return

    # print(&#39;in clustering&#39;)
    start_time = time.time()
    oidxs = npr.permutation(np.arange(len(self.G.data.asvs)))

    for oidx in oidxs:
        self.gibbs_update_single_asv_slow(oidx=oidx)
    self._strtime = time.time() - start_time</code></pre>
</details>
</dd>
<dt id="mdsine2.clustering.ClusterAssignments.update_slow_fast"><code class="name flex">
<span>def <span class="ident">update_slow_fast</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Much faster than <code>update_slow</code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_slow_fast(self):
    &#39;&#39;&#39;Much faster than `update_slow`
    &#39;&#39;&#39;

    if self.clustering.n_clusters.sample_iter &lt; self.delay:
        return

    if self.clustering.n_clusters.sample_iter % self.run_every_n_iterations != 0:
       return

    start_time = time.time()

    self.process_prec = self.G[REPRNAMES.PROCESSVAR].prec.ravel() #.build_matrix(cov=False, sparse=False)
    self.process_prec_matrix = self.G[REPRNAMES.PROCESSVAR].build_matrix(sparse=True, cov=False)
    lhs = [REPRNAMES.GROWTH_VALUE, REPRNAMES.SELF_INTERACTION_VALUE]
    self.y = self.G.data.construct_lhs(lhs, 
        kwargs_dict={REPRNAMES.GROWTH_VALUE:{&#39;with_perturbations&#39;: False}})

    oidxs = npr.permutation(np.arange(len(self.G.data.asvs)))
    iii = 0
    for oidx in oidxs:
        logging.info(&#39;{}/{}: {}&#39;.format(iii, len(oidxs), oidx))
        self.gibbs_update_single_asv_slow_fast(oidx=oidx)
        iii += 1
    self._strtime = time.time() - start_time</code></pre>
</details>
</dd>
<dt id="mdsine2.clustering.ClusterAssignments.visualize_posterior"><code class="name flex">
<span>def <span class="ident">visualize_posterior</span></span>(<span>self, basepath, f, section='posterior', asv_formatter='%(name)s', yticklabels='%(name)s %(index)s', xticklabels='%(index)s')</span>
</code></dt>
<dd>
<div class="desc"><p>Render the traces in the folder <code>basepath</code> and write the
learned values to the file <code>f</code>.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>basepath</code></strong> :&ensp;<code>str</code></dt>
<dd>This is the loction to write the files to</dd>
<dt><strong><code>f</code></strong> :&ensp;<code>_io.TextIOWrapper</code></dt>
<dd>File that we are writing the values to</dd>
<dt><strong><code>section</code></strong> :&ensp;<code>str</code></dt>
<dd>Section of the trace to compute on. Options:
'posterior' : posterior samples
'burnin' : burn-in samples
'entire' : both burn-in and posterior samples</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>_io.TextIOWrapper</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def visualize_posterior(self, basepath, f, section=&#39;posterior&#39;, asv_formatter=&#39;%(name)s&#39;,
    yticklabels=&#39;%(name)s %(index)s&#39;, xticklabels=&#39;%(index)s&#39;):
    &#39;&#39;&#39;Render the traces in the folder `basepath` and write the 
    learned values to the file `f`.

    Parameters
    ----------
    basepath : str
        This is the loction to write the files to
    f : _io.TextIOWrapper
        File that we are writing the values to
    section : str
        Section of the trace to compute on. Options:
            &#39;posterior&#39; : posterior samples
            &#39;burnin&#39; : burn-in samples
            &#39;entire&#39; : both burn-in and posterior samples

    Returns
    -------
    _io.TextIOWrapper
    &#39;&#39;&#39;
    asvs = self.G.data.asvs
    f.write(&#39;\n\n###################################\n&#39;)
    f.write(self.name)
    f.write(&#39;\n###################################\n&#39;)
    if not self.G.inference.is_in_inference_order(self):
        f.write(&#39;`{}` not learned. These were the fixed cluster assignments\n&#39;.format(self.name))
        for cidx, cluster in enumerate(self.clustering):
            f.write(&#39;Cluster {}:\n&#39;.format(cidx+1))
            for aidx in cluster.members:
                label = pl.asvname_formatter(format=asv_formatter, asv=asvs[aidx], asvs=asvs)
                f.write(&#39;\t- {}\n&#39;.format(label))

        return f

    # Coclusters
    cocluster_trace = self.clustering.coclusters.get_trace_from_disk(section=section)
    coclusters = pl.variables.summary(cocluster_trace, section=section)[&#39;mean&#39;]
    for i in range(coclusters.shape[0]):
        coclusters[i,i] = np.nan

    visualization.render_cocluster_proportions(
        coclusters=coclusters, asvs=self.G.data.asvs, clustering=self.clustering,
        yticklabels=yticklabels, include_tick_marks=False, xticklabels=xticklabels,
        title=&#39;Cluster Assignments&#39;)
    fig = plt.gcf()
    fig.tight_layout()
    plt.savefig(basepath + &#39;coclusters.pdf&#39;)
    plt.close()

    # N clusters
    visualization.render_trace(var=self.clustering.n_clusters, plt_type=&#39;both&#39;, 
        section=section, include_burnin=True, rasterized=True)
    fig = plt.gcf()
    fig.suptitle(&#39;Number of Clusters&#39;)
    plt.savefig(basepath + &#39;n_clusters.pdf&#39;)
    plt.close()

    ca = generate_cluster_assignments_posthoc(clustering=self.clustering, n_clusters=&#39;mode&#39;, 
        section=section)
    cluster_assignments = {}
    for idx, assignment in enumerate(ca):
        if assignment in cluster_assignments:
            cluster_assignments[assignment].append(idx)
        else:
            cluster_assignments[assignment] = [idx]

    f.write(&#39;Mode number of clusters: {}\n&#39;.format(len(self.clustering)))
    for idx,lst in enumerate(cluster_assignments.values()):
        f.write(&#39;Cluster {} - Size {}\n&#39;.format(idx+1, len(lst)))
        for oidx in lst:
            # Get rid of index because that does not really make sense here
            label = pl.asvname_formatter(format=asv_formatter, asv=asvs[oidx], asvs=asvs)
            f.write(&#39;\t- {}\n&#39;.format(label))
    
    return f</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="mdsine2.pylab.graph.Node" href="pylab/graph.html#mdsine2.pylab.graph.Node">Node</a></b></code>:
<ul class="hlist">
<li><code><a title="mdsine2.pylab.graph.Node.add_child" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_child">add_child</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.add_parent" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_parent">add_parent</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.add_prior" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_prior">add_prior</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.add_undirected" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_undirected">add_undirected</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.degree" href="pylab/graph.html#mdsine2.pylab.graph.Node.degree">degree</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.delete" href="pylab/graph.html#mdsine2.pylab.graph.BaseNode.delete">delete</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.get_adjacent_keys" href="pylab/graph.html#mdsine2.pylab.graph.Node.get_adjacent_keys">get_adjacent_keys</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.load" href="pylab/base.html#mdsine2.pylab.base.Saveable.load">load</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.metropolis" href="pylab/graph.html#mdsine2.pylab.graph.Node.metropolis">metropolis</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.save" href="pylab/base.html#mdsine2.pylab.base.Saveable.save">save</a></code></li>
<li><code><a title="mdsine2.pylab.graph.Node.set_save_location" href="pylab/base.html#mdsine2.pylab.base.Saveable.set_save_location">set_save_location</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="mdsine2.clustering.Concentration"><code class="flex name class">
<span>class <span class="ident">Concentration</span></span>
<span>(</span><span>prior, value=None, n_iter=None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the posterior for the concentration parameter that is used
in learning the cluster assignments.
The posterior is implemented as it is describes in 'Bayesian Inference
for Density Estimation' by M. D. Escobar and M. West, 1995.</p>
<p>Parameters</p>
<p>value (float, int)
- Initial value of the concentration
- Default value is the mean of the prior</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Concentration(pl.variables.Gamma):
    &#39;&#39;&#39;Defines the posterior for the concentration parameter that is used
    in learning the cluster assignments.
    The posterior is implemented as it is describes in &#39;Bayesian Inference
    for Density Estimation&#39; by M. D. Escobar and M. West, 1995.
    &#39;&#39;&#39;
    def __init__(self, prior, value=None, n_iter=None, **kwargs):
        &#39;&#39;&#39;Parameters

        value (float, int)
            - Initial value of the concentration
            - Default value is the mean of the prior
        &#39;&#39;&#39;
        kwargs[&#39;name&#39;] = STRNAMES.CONCENTRATION
        # initialize shape and scale as the same as the priors
        # we will be updating this later
        pl.variables.Gamma.__init__(self, shape=prior.shape.value, scale=prior.scale.value,
            dtype=float, **kwargs)
        self.add_prior(prior)

    def initialize(self, value_option, hyperparam_option, n_iter=None, value=None,
        shape=None, scale=None, delay=0):
        &#39;&#39;&#39;Initialize the hyperparameters of the beta prior

        Parameters
        ----------
        value_option (str)
            - Options to initialize the value
            - &#39;manual&#39;
                - Set the value manually, `value` must also be specified
            - &#39;auto&#39;, &#39;prior-mean&#39;
                - Set to the mean of the prior
        hyperparam_option (str)
            - Options ot initialize the hyperparameters
            - Options
                - &#39;manual&#39;
                    - Set the values manually. `shape` and `scale` must also be specified
                - &#39;auto&#39;, &#39;diffuse&#39;
                    - shape = 1e-5, scale= 1e5
        shape, scale (int, float)
            - User specified values
            - Only necessary if `hyperparam_option` == &#39;manual&#39;
        &#39;&#39;&#39;
        if not pl.isint(delay):
            raise TypeError(&#39;`delay` ({}) must be an int&#39;.format(type(delay)))
        if delay &lt; 0:
            raise ValueError(&#39;`delay` ({}) must be &gt;= 0&#39;.format(delay))
        self.delay = delay

        if hyperparam_option == &#39;manual&#39;:
            if pl.isnumeric(shape) and pl.isnumeric(scale):
                self.prior.shape.override_value(shape)
                self.prior.scale.override_value(scale)
            else:
                raise ValueError(&#39;shape ({}) and scale ({}) must be numeric&#39; \
                    &#39; (float, int)&#39;.format(shape.__class__, scale.__class__))

            if not pl.isint(n_iter):
                raise ValueError(&#39;`n_iter` ({}) needs ot be an int&#39;.format(n_iter.__class__))
            self.n_iter = n_iter

        elif hyperparam_option == &#39;strong-few&#39;:
            self.prior.shape.override_value(1)
            self.prior.scale.override_value(1)
            self.n_iter = 20

        elif hyperparam_option in [&#39;diffuse&#39;, &#39;auto&#39;]:
            self.prior.shape.override_value(1e-5)
            self.prior.scale.override_value(1e5)
            self.n_iter = 20
        else:
            raise ValueError(&#39;hyperparam_option `{}` not recognized&#39;.format(hyperparam_option))

        if value_option == &#39;manual&#39;:
            if pl.isnumeric(value):
                self.value = value
            else:
                raise ValueError(&#39;`value` ({}) must be numeric (float, int)&#39;.format(
                    value.__class__))
        elif value_option in [&#39;auto&#39;, &#39;prior-mean&#39;]:
            self.value = self.prior.mean()
        else:
            raise ValueError(&#39;value_option `{}` not recognized&#39;.format(value_option))

        self.shape.value = self.prior.shape.value
        self.scale.value = self.prior.scale.value
        logging.info(&#39;Cluster Concentration initialization results:\n&#39; \
            &#39;\tprior shape: {}\n&#39; \
            &#39;\tprior scale: {}\n&#39; \
            &#39;\tvalue: {}&#39;.format(
                self.prior.shape.value, self.prior.scale.value, self.value))

    def update(self):
        &#39;&#39;&#39;Sample the posterior of the concentration parameter
        &#39;&#39;&#39;
        if self.sample_iter &lt; self.delay:
            return

        clustering = self.G[REPRNAMES.CLUSTER_INTERACTION_VALUE].clustering
        k = len(clustering)
        n = self.G.data.n_asvs
        # If there are 1 or 0 clusters, do not update
        # if k &lt;= 1:
        #     return
        for i in range(self.n_iter):
            #first sample eta from a beta distribution
            eta = npr.beta(self.value+1,n)
            #sample alpha from a mixture of gammas
            pi_eta = [0.0, n]
            pi_eta[0] = (self.prior.shape.value+k-1.)/(1/(self.prior.scale.value)-np.log(eta))
            self.scale.value =  1/(1/self.prior.scale.value - np.log(eta))
            self.shape.value = self.prior.shape.value + k
            if np.random.choice([0,1], p=pi_eta/np.sum(pi_eta)) != 0:
                self.shape.value -= 1
            self.sample()
            # print(&#39;pi_eta[0]&#39;,pi_eta[0])

    def visualize_posterior(self, path, f, section=&#39;posterior&#39;):
        &#39;&#39;&#39;Render the traces in the folder `basepath` and write the 
        learned values to the file `f`.

        Parameters
        ----------
        path : str
            This is the path to write the files to
        f : _io.TextIOWrapper
            File that we are writing the values to
        section : str
            Section of the trace to compute on. Options:
                &#39;posterior&#39; : posterior samples
                &#39;burnin&#39; : burn-in samples
                &#39;entire&#39; : both burn-in and posterior samples

        Returns
        -------
        _io.TextIOWrapper
        &#39;&#39;&#39;
        f.write(&#39;\n\n###################################\n&#39;)
        f.write(self.name)
        f.write(&#39;\n###################################\n&#39;)
        if not self.G.inference.is_being_traced(self):
            f.write(&#39;`{}` not learned\n\tValue: {}\n&#39;.format(self.name, self.value))
            return f

        summ = pl.summary(self, section=section)
        for k,v in summ.items():
            f.write(&#39;\t{}: {}\n&#39;.format(k,v))

        ax1, _ = visualization.render_trace(var=self, plt_type=&#39;both&#39;, 
            section=section, include_burnin=True, log_scale=True, rasterized=True)

        l,h = ax1.get_xlim()
        xs = np.arange(l,h,step=(h-l)/1000) 
        ys = []
        for x in xs:
            ys.append(self.prior.pdf(value=x))
        ax1.plot(xs, ys, label=&#39;prior&#39;, alpha=0.5, color=&#39;red&#39;)
        ax1.legend()

        fig = plt.gcf()
        fig.suptitle(&#39;Concentration&#39;)
        plt.savefig(path)
        plt.close()
        f.close()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mdsine2.pylab.variables.Gamma" href="pylab/variables.html#mdsine2.pylab.variables.Gamma">Gamma</a></li>
<li><a title="mdsine2.pylab.variables.Variable" href="pylab/variables.html#mdsine2.pylab.variables.Variable">Variable</a></li>
<li><a title="mdsine2.pylab.graph.Node" href="pylab/graph.html#mdsine2.pylab.graph.Node">Node</a></li>
<li><a title="mdsine2.pylab.graph.BaseNode" href="pylab/graph.html#mdsine2.pylab.graph.BaseNode">BaseNode</a></li>
<li><a title="mdsine2.pylab.base.Saveable" href="pylab/base.html#mdsine2.pylab.base.Saveable">Saveable</a></li>
<li>mdsine2.pylab.variables._BaseArithmeticClass</li>
<li><a title="mdsine2.pylab.base.Traceable" href="pylab/base.html#mdsine2.pylab.base.Traceable">Traceable</a></li>
<li>mdsine2.pylab.variables._RandomBase</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="mdsine2.clustering.Concentration.initialize"><code class="name flex">
<span>def <span class="ident">initialize</span></span>(<span>self, value_option, hyperparam_option, n_iter=None, value=None, shape=None, scale=None, delay=0)</span>
</code></dt>
<dd>
<div class="desc"><p>Initialize the hyperparameters of the beta prior</p>
<h2 id="parameters">Parameters</h2>
<p>value_option (str)
- Options to initialize the value
- 'manual'
- Set the value manually, <code>value</code> must also be specified
- 'auto', 'prior-mean'
- Set to the mean of the prior
hyperparam_option (str)
- Options ot initialize the hyperparameters
- Options
- 'manual'
- Set the values manually. <code>shape</code> and <code>scale</code> must also be specified
- 'auto', 'diffuse'
- shape = 1e-5, scale= 1e5
shape, scale (int, float)
- User specified values
- Only necessary if <code>hyperparam_option</code> == 'manual'</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def initialize(self, value_option, hyperparam_option, n_iter=None, value=None,
    shape=None, scale=None, delay=0):
    &#39;&#39;&#39;Initialize the hyperparameters of the beta prior

    Parameters
    ----------
    value_option (str)
        - Options to initialize the value
        - &#39;manual&#39;
            - Set the value manually, `value` must also be specified
        - &#39;auto&#39;, &#39;prior-mean&#39;
            - Set to the mean of the prior
    hyperparam_option (str)
        - Options ot initialize the hyperparameters
        - Options
            - &#39;manual&#39;
                - Set the values manually. `shape` and `scale` must also be specified
            - &#39;auto&#39;, &#39;diffuse&#39;
                - shape = 1e-5, scale= 1e5
    shape, scale (int, float)
        - User specified values
        - Only necessary if `hyperparam_option` == &#39;manual&#39;
    &#39;&#39;&#39;
    if not pl.isint(delay):
        raise TypeError(&#39;`delay` ({}) must be an int&#39;.format(type(delay)))
    if delay &lt; 0:
        raise ValueError(&#39;`delay` ({}) must be &gt;= 0&#39;.format(delay))
    self.delay = delay

    if hyperparam_option == &#39;manual&#39;:
        if pl.isnumeric(shape) and pl.isnumeric(scale):
            self.prior.shape.override_value(shape)
            self.prior.scale.override_value(scale)
        else:
            raise ValueError(&#39;shape ({}) and scale ({}) must be numeric&#39; \
                &#39; (float, int)&#39;.format(shape.__class__, scale.__class__))

        if not pl.isint(n_iter):
            raise ValueError(&#39;`n_iter` ({}) needs ot be an int&#39;.format(n_iter.__class__))
        self.n_iter = n_iter

    elif hyperparam_option == &#39;strong-few&#39;:
        self.prior.shape.override_value(1)
        self.prior.scale.override_value(1)
        self.n_iter = 20

    elif hyperparam_option in [&#39;diffuse&#39;, &#39;auto&#39;]:
        self.prior.shape.override_value(1e-5)
        self.prior.scale.override_value(1e5)
        self.n_iter = 20
    else:
        raise ValueError(&#39;hyperparam_option `{}` not recognized&#39;.format(hyperparam_option))

    if value_option == &#39;manual&#39;:
        if pl.isnumeric(value):
            self.value = value
        else:
            raise ValueError(&#39;`value` ({}) must be numeric (float, int)&#39;.format(
                value.__class__))
    elif value_option in [&#39;auto&#39;, &#39;prior-mean&#39;]:
        self.value = self.prior.mean()
    else:
        raise ValueError(&#39;value_option `{}` not recognized&#39;.format(value_option))

    self.shape.value = self.prior.shape.value
    self.scale.value = self.prior.scale.value
    logging.info(&#39;Cluster Concentration initialization results:\n&#39; \
        &#39;\tprior shape: {}\n&#39; \
        &#39;\tprior scale: {}\n&#39; \
        &#39;\tvalue: {}&#39;.format(
            self.prior.shape.value, self.prior.scale.value, self.value))</code></pre>
</details>
</dd>
<dt id="mdsine2.clustering.Concentration.update"><code class="name flex">
<span>def <span class="ident">update</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Sample the posterior of the concentration parameter</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update(self):
    &#39;&#39;&#39;Sample the posterior of the concentration parameter
    &#39;&#39;&#39;
    if self.sample_iter &lt; self.delay:
        return

    clustering = self.G[REPRNAMES.CLUSTER_INTERACTION_VALUE].clustering
    k = len(clustering)
    n = self.G.data.n_asvs
    # If there are 1 or 0 clusters, do not update
    # if k &lt;= 1:
    #     return
    for i in range(self.n_iter):
        #first sample eta from a beta distribution
        eta = npr.beta(self.value+1,n)
        #sample alpha from a mixture of gammas
        pi_eta = [0.0, n]
        pi_eta[0] = (self.prior.shape.value+k-1.)/(1/(self.prior.scale.value)-np.log(eta))
        self.scale.value =  1/(1/self.prior.scale.value - np.log(eta))
        self.shape.value = self.prior.shape.value + k
        if np.random.choice([0,1], p=pi_eta/np.sum(pi_eta)) != 0:
            self.shape.value -= 1
        self.sample()</code></pre>
</details>
</dd>
<dt id="mdsine2.clustering.Concentration.visualize_posterior"><code class="name flex">
<span>def <span class="ident">visualize_posterior</span></span>(<span>self, path, f, section='posterior')</span>
</code></dt>
<dd>
<div class="desc"><p>Render the traces in the folder <code>basepath</code> and write the
learned values to the file <code>f</code>.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>path</code></strong> :&ensp;<code>str</code></dt>
<dd>This is the path to write the files to</dd>
<dt><strong><code>f</code></strong> :&ensp;<code>_io.TextIOWrapper</code></dt>
<dd>File that we are writing the values to</dd>
<dt><strong><code>section</code></strong> :&ensp;<code>str</code></dt>
<dd>Section of the trace to compute on. Options:
'posterior' : posterior samples
'burnin' : burn-in samples
'entire' : both burn-in and posterior samples</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>_io.TextIOWrapper</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def visualize_posterior(self, path, f, section=&#39;posterior&#39;):
    &#39;&#39;&#39;Render the traces in the folder `basepath` and write the 
    learned values to the file `f`.

    Parameters
    ----------
    path : str
        This is the path to write the files to
    f : _io.TextIOWrapper
        File that we are writing the values to
    section : str
        Section of the trace to compute on. Options:
            &#39;posterior&#39; : posterior samples
            &#39;burnin&#39; : burn-in samples
            &#39;entire&#39; : both burn-in and posterior samples

    Returns
    -------
    _io.TextIOWrapper
    &#39;&#39;&#39;
    f.write(&#39;\n\n###################################\n&#39;)
    f.write(self.name)
    f.write(&#39;\n###################################\n&#39;)
    if not self.G.inference.is_being_traced(self):
        f.write(&#39;`{}` not learned\n\tValue: {}\n&#39;.format(self.name, self.value))
        return f

    summ = pl.summary(self, section=section)
    for k,v in summ.items():
        f.write(&#39;\t{}: {}\n&#39;.format(k,v))

    ax1, _ = visualization.render_trace(var=self, plt_type=&#39;both&#39;, 
        section=section, include_burnin=True, log_scale=True, rasterized=True)

    l,h = ax1.get_xlim()
    xs = np.arange(l,h,step=(h-l)/1000) 
    ys = []
    for x in xs:
        ys.append(self.prior.pdf(value=x))
    ax1.plot(xs, ys, label=&#39;prior&#39;, alpha=0.5, color=&#39;red&#39;)
    ax1.legend()

    fig = plt.gcf()
    fig.suptitle(&#39;Concentration&#39;)
    plt.savefig(path)
    plt.close()
    f.close()</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="mdsine2.pylab.variables.Gamma" href="pylab/variables.html#mdsine2.pylab.variables.Gamma">Gamma</a></b></code>:
<ul class="hlist">
<li><code><a title="mdsine2.pylab.variables.Gamma.T" href="pylab/variables.html#mdsine2.pylab.variables.Variable.T">T</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Gamma.add_child" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_child">add_child</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Gamma.add_init_value" href="pylab/variables.html#mdsine2.pylab.variables.Variable.add_init_value">add_init_value</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Gamma.add_parent" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_parent">add_parent</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Gamma.add_prior" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_prior">add_prior</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Gamma.add_trace" href="pylab/base.html#mdsine2.pylab.base.Traceable.add_trace">add_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Gamma.add_undirected" href="pylab/graph.html#mdsine2.pylab.graph.Node.add_undirected">add_undirected</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Gamma.degree" href="pylab/graph.html#mdsine2.pylab.graph.Node.degree">degree</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Gamma.delete" href="pylab/graph.html#mdsine2.pylab.graph.BaseNode.delete">delete</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Gamma.get_adjacent_keys" href="pylab/graph.html#mdsine2.pylab.graph.Node.get_adjacent_keys">get_adjacent_keys</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Gamma.get_iter" href="pylab/base.html#mdsine2.pylab.base.Traceable.get_iter">get_iter</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Gamma.get_trace_from_disk" href="pylab/base.html#mdsine2.pylab.base.Traceable.get_trace_from_disk">get_trace_from_disk</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Gamma.load" href="pylab/base.html#mdsine2.pylab.base.Saveable.load">load</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Gamma.metropolis" href="pylab/graph.html#mdsine2.pylab.graph.Node.metropolis">metropolis</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Gamma.overwrite_entire_trace_on_disk" href="pylab/base.html#mdsine2.pylab.base.Traceable.overwrite_entire_trace_on_disk">overwrite_entire_trace_on_disk</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Gamma.pdf" href="pylab/variables.html#mdsine2.pylab.variables.Gamma.pdf">pdf</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Gamma.sample" href="pylab/variables.html#mdsine2.pylab.variables.Gamma.sample">sample</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Gamma.save" href="pylab/base.html#mdsine2.pylab.base.Saveable.save">save</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Gamma.set_save_location" href="pylab/base.html#mdsine2.pylab.base.Saveable.set_save_location">set_save_location</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Gamma.set_trace" href="pylab/base.html#mdsine2.pylab.base.Traceable.set_trace">set_trace</a></code></li>
<li><code><a title="mdsine2.pylab.variables.Gamma.set_value_shape" href="pylab/variables.html#mdsine2.pylab.variables.Variable.set_value_shape">set_value_shape</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="mdsine2.clustering.CondensedClustering"><code class="flex name class">
<span>class <span class="ident">CondensedClustering</span></span>
<span>(</span><span>oidx2cidx)</span>
</code></dt>
<dd>
<div class="desc"><p>Condensed clustering object that is not associated with the graph</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>oidx2cidx</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Maps the cluster assignment to each asv.
index -&gt; ASV index
output -&gt; cluster index</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CondensedClustering:
    &#39;&#39;&#39;Condensed clustering object that is not associated with the graph

    Parameters
    ----------
    oidx2cidx : np.ndarray
        Maps the cluster assignment to each asv.
        index -&gt; ASV index
        output -&gt; cluster index

    &#39;&#39;&#39;
    def __init__(self, oidx2cidx):

        self.clusters = []
        self.oidx2cidx = oidx2cidx
        a = {}
        for oidx, cidx in enumerate(self.oidx2cidx):
            if cidx not in a:
                a[cidx] = [oidx]
            else:
                a[cidx].append(oidx)
        
        cidx = 0
        while cidx in a:
            self.clusters.append(np.asarray(a[cidx], dtype=int))
            cidx += 1

    def __len__(self):
        return len(self.clusters)</code></pre>
</details>
</dd>
<dt id="mdsine2.clustering.SingleClusterFullParallelization"><code class="flex name class">
<span>class <span class="ident">SingleClusterFullParallelization</span></span>
<span>(</span><span>n_asvs, total_n_dts_per_asv, n_replicates, n_dts_for_replicate, there_are_perturbations, keypair2col_interactions, keypair2col_perturbations, n_perturbations, base_Xrows, base_Xcols, base_Xshape, base_Xpertrows, base_Xpertcols, base_Xpertshape, n_rowsM, n_rowsMpert)</span>
</code></dt>
<dd>
<div class="desc"><p>Make the full parallelization
- Mixture matricies for interactions and perturbations
- calculating the marginalization for the sent in cluster</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>n_asvs</code></strong> :&ensp;<code>int</code></dt>
<dd>Total number of OTUs</dd>
<dt><strong><code>total_n_dts_per_asv</code></strong> :&ensp;<code>int</code></dt>
<dd>Total number of time changes for each OTU</dd>
<dt><strong><code>n_replicates</code></strong> :&ensp;<code>int</code></dt>
<dd>Total number of replicates</dd>
<dt><strong><code>n_dts_for_replicate</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Total number of time changes for each replicate</dd>
<dt><strong><code>there_are_perturbations</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, there are perturbations</dd>
<dt><strong><code>keypair2col_interactions</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>These map the OTU indices of the pairs of OTUs to their column index
in <code>big_X</code></dd>
<dt><strong><code>keypair2col_perturbations</code></strong> :&ensp;<code>np.ndarray, None</code></dt>
<dd>These map the OTU indices nad perturbation index to the column in
<code>big_Xpert</code>. If there are no perturbations then this is None</dd>
<dt><strong><code>n_perturbations</code></strong> :&ensp;<code>int, None</code></dt>
<dd>Number of perturbations. None if there are no perturbations</dd>
<dt><strong><code>base_Xrows</code></strong>, <strong><code>base_Xcols</code></strong>, <strong><code>base_Xpertrows</code></strong>, <strong><code>base_Xpertcols</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>These are the rows and columns necessary to build the interaction and perturbation
matrices, respectively. Whats passed in is the data vector and then we build
it using sparse matrices</dd>
<dt><strong><code>n_rowsM</code></strong>, <strong><code>n_rowsMpert</code></strong> :&ensp;<code>int</code></dt>
<dd>These are the number of rows for the mixing matrix for the interactions and
perturbations respectively.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SingleClusterFullParallelization(pl.multiprocessing.PersistentWorker):
    &#39;&#39;&#39;Make the full parallelization
        - Mixture matricies for interactions and perturbations
        - calculating the marginalization for the sent in cluster

    Parameters
    ----------
    n_asvs : int
        Total number of OTUs
    total_n_dts_per_asv : int
        Total number of time changes for each OTU
    n_replicates : int
        Total number of replicates
    n_dts_for_replicate : np.ndarray
        Total number of time changes for each replicate
    there_are_perturbations : bool
        If True, there are perturbations
    keypair2col_interactions : np.ndarray
        These map the OTU indices of the pairs of OTUs to their column index
        in `big_X`
    keypair2col_perturbations : np.ndarray, None
        These map the OTU indices nad perturbation index to the column in
        `big_Xpert`. If there are no perturbations then this is None
    n_perturbations : int, None
        Number of perturbations. None if there are no perturbations
    base_Xrows, base_Xcols, base_Xpertrows, base_Xpertcols : np.ndarray
        These are the rows and columns necessary to build the interaction and perturbation 
        matrices, respectively. Whats passed in is the data vector and then we build
        it using sparse matrices
    n_rowsM, n_rowsMpert : int
        These are the number of rows for the mixing matrix for the interactions and
        perturbations respectively.
    &#39;&#39;&#39;
    def __init__(self, n_asvs, total_n_dts_per_asv, n_replicates, n_dts_for_replicate,
        there_are_perturbations, keypair2col_interactions, keypair2col_perturbations,
        n_perturbations, base_Xrows, base_Xcols, base_Xshape, base_Xpertrows, base_Xpertcols,
        base_Xpertshape, n_rowsM, n_rowsMpert):
        self.n_asvs = n_asvs
        self.total_n_dts_per_asv = total_n_dts_per_asv
        self.n_replicates = n_replicates
        self.n_dts_for_replicate = n_dts_for_replicate
        self.there_are_perturbations = there_are_perturbations
        self.keypair2col_interactions = keypair2col_interactions
        if self.there_are_perturbations:
            self.keypair2col_perturbations = keypair2col_perturbations
            self.n_perturbations = n_perturbations

        self.base_Xrows = base_Xrows
        self.base_Xcols = base_Xcols
        self.base_Xshape = base_Xshape
        self.base_Xpertrows = base_Xpertrows
        self.base_Xpertcols = base_Xpertcols
        self.base_Xpertshape = base_Xpertshape

        self.n_rowsM = n_rowsM
        self.n_rowsMpert = n_rowsMpert

    def initialize_gibbs(self, base_Xdata, base_Xpertdata, concentration, m, y,
        process_prec_diag, prior_var_interactions, prior_var_pert, 
        prior_mean_interactions, prior_mean_pert):
        &#39;&#39;&#39;Pass in the information that changes every Gibbs step

        Parameters
        ----------
        base_X : scipy.sparse.csc_matrix
            Sparse matrix for the interaction terms
        base_Xpert : scipy.sparse.csc_matrix, None
            Sparse matrix for the perturbation terms
            If None, there are no perturbations
        concentration : float
            This is the concentration of the system
        m : int
            This is the auxiliary variable for the marginalization
        y : np.ndarray
            This is the observation array
        process_prec_diag : np.ndarray
            This is the process precision diagonal
        &#39;&#39;&#39;
        self.base_X = scipy.sparse.coo_matrix(
            (base_Xdata,(self.base_Xrows,self.base_Xcols)),
            shape=self.base_Xshape).tocsc()
        
        self.concentration = concentration
        self.m = m
        self.y = y.reshape(-1,1)
        self.n_rows = len(y)
        self.n_cols_X = self.base_X.shape[1]
        self.prior_var_interactions = prior_var_interactions
        self.prior_prec_interactions = 1/prior_var_interactions
        self.prior_mean_interactions = prior_mean_interactions

        if self.there_are_perturbations:
            self.base_Xpert = scipy.sparse.coo_matrix(
                (base_Xpertdata,(self.base_Xpertrows,self.base_Xpertcols)),
                shape=self.base_Xpertshape).tocsc()
            self.prior_var_pert = prior_var_pert
            self.prior_prec_pert = 1/prior_var_pert
            self.prior_mean_pert = prior_mean_pert

        self.process_prec_matrix = scipy.sparse.dia_matrix(
            (process_prec_diag,[0]), shape=(len(process_prec_diag),len(process_prec_diag))).tocsc()

    def initialize_oidx(self, interaction_on_idxs, perturbation_on_idxs):
        &#39;&#39;&#39;Pass in the parameters that change for every OTU - potentially

        Parameters
        ----------
        
        &#39;&#39;&#39;
        self.saved_interaction_on_idxs = interaction_on_idxs
        self.saved_perturbation_on_idxs = perturbation_on_idxs

    # @profile
    def run(self, interaction_on_idxs, perturbation_on_idxs, cluster_config, log_mult_factor, cid, 
        use_saved_params):
        &#39;&#39;&#39;Pass in the parameters for the specific cluster assignment for
        the OTU and run the marginalization


        Parameters
        ----------
        interaction_on_idxs : np.array(int)
            An array of indices for the interactions that are on. Assumes that the
            clustering is in the order specified in `cluster_config`
        perturbation_on_idxs : list(np.ndarray(int)), None
            If there are perturbations, then we set the perturbation idxs on
            Each element in the list are the indices of that perturbation that are on
        cluster_config : list(list(int))
            This is the cluster configuration and in cluster order.
        log_mult_factor : float
            This is the log multiplication factor that we add onto the marginalization
        use_saved_params : bool
            If True, passed in `interaction_on_idxs` and `perturbation_on_idxs` are None
            and we can use `saved_interaction_on_idxs` and `saved_perturbation_on_idxs`
        &#39;&#39;&#39;
        if use_saved_params:
            interaction_on_idxs = self.saved_interaction_on_idxs
            perturbation_on_idxs = self.saved_perturbation_on_idxs

        # We need to make the arrays for interactions and perturbations
        self.set_clustering(cluster_config=cluster_config)
        Xinteractions = self.build_interactions_matrix(on_columns=interaction_on_idxs)

        if self.there_are_perturbations:
            Xperturbations = self.build_perturbations_matrix(on_columns=perturbation_on_idxs)
            X = scipy.sparse.hstack([Xperturbations, Xinteractions])
        else:
            X = Xinteractions
        self.X = X
        self.prior_mean = self.build_prior_mean(on_interactions=interaction_on_idxs,
            on_perturbations=perturbation_on_idxs)
        self.prior_cov, self.prior_prec, self.prior_prec_diag = self.build_prior_cov_and_prec_and_diag(
            on_interactions=interaction_on_idxs, on_perturbations=perturbation_on_idxs)
        
        return cid, self.calculate_marginal_loglikelihood_slow_fast_sparse()

    def set_clustering(self, cluster_config):
        self.clustering = CondensedClustering(oidx2cidx=cluster_config)
        self.iidx2cidxpair = np.zeros(shape=(len(self.clustering)*(len(self.clustering)-1), 2), 
            dtype=int)
        self.iidx2cidxpair = SingleClusterFullParallelization.make_iidx2cidxpair(
            ret=self.iidx2cidxpair,
            n_clusters=len(self.clustering))

    def build_interactions_matrix(self, on_columns):
        &#39;&#39;&#39;Build the interaction matrix

        First we make the rows and columns for the mixing matrix,
        then we multiple the base matrix and the mixing matrix.
        &#39;&#39;&#39;
        rows = []
        cols = []

        # c2ciidx = Cluster-to-Cluster Interaction InDeX
        c2ciidx = 0
        for ccc in on_columns:
            tcidx = self.iidx2cidxpair[ccc, 0]
            scidx = self.iidx2cidxpair[ccc, 1]
            
            smems = self.clustering.clusters[scidx]
            tmems = self.clustering.clusters[tcidx]
            
            a = np.zeros(len(smems)*len(tmems), dtype=int)
            rows.append(SingleClusterFullParallelization.get_indices(a,
                self.keypair2col_interactions, tmems, smems))
            cols.append(np.full(len(tmems)*len(smems), fill_value=c2ciidx))
            c2ciidx += 1

        rows = np.asarray(list(itertools.chain.from_iterable(rows)))
        cols = np.asarray(list(itertools.chain.from_iterable(cols)))
        data = np.ones(len(rows), dtype=int)

        # print(&#39;rows&#39;, rows)
        # print(cols)
        # print(data)
        # print((self.n_rowsM, c2ciidx))

        M = scipy.sparse.coo_matrix((data,(rows,cols)),
            shape=(self.n_rowsM, c2ciidx)).tocsc()
        ret = self.base_X @ M
        return ret

    def build_perturbations_matrix(self, on_columns):
        if not self.there_are_perturbations:
            raise ValueError(&#39;You should not be here&#39;)
        
        keypair2col = self.keypair2col_perturbations
        rows = []
        cols = []

        col = 0
        for pidx, pert_ind_idxs in enumerate(on_columns):
            for cidx in pert_ind_idxs:
                for oidx in self.clustering.clusters[cidx]:
                    rows.append(keypair2col[oidx, pidx])
                    cols.append(col)
                col += 1
        
        data = np.ones(len(rows), dtype=np.float64)
        M = scipy.sparse.coo_matrix((data,(rows,cols)),
            shape=(self.n_rowsMpert, col)).tocsc()
        ret = self.base_Xpert @ M
        return ret

    def build_prior_mean(self, on_interactions, on_perturbations):
        &#39;&#39;&#39;Build the prior mean array

        Perturbations go first and then interactions
        &#39;&#39;&#39;
        ret = []
        for pidx, pert in enumerate(on_perturbations):
            ret = np.append(ret, 
                np.full(len(pert), fill_value=self.prior_mean_pert[pidx]))
        ret = np.append(ret, np.full(len(on_interactions), fill_value=self.prior_mean_interactions))
        return ret.reshape(-1,1)

    def build_prior_cov_and_prec_and_diag(self, on_interactions, on_perturbations):
        &#39;&#39;&#39;Build the prior covariance matrices and others
        &#39;&#39;&#39;
        ret = []
        for pidx, pert in enumerate(on_perturbations):
            ret = np.append(ret, 
                np.full(len(pert), fill_value=self.prior_var_pert[pidx]))
        prior_var_diag = np.append(ret, np.full(len(on_interactions), fill_value=self.prior_var_interactions))
        prior_prec_diag = 1/prior_var_diag

        prior_var = scipy.sparse.dia_matrix((prior_var_diag,[0]), 
            shape=(len(prior_var_diag),len(prior_var_diag))).tocsc()
        prior_prec = scipy.sparse.dia_matrix((prior_prec_diag,[0]), 
            shape=(len(prior_prec_diag),len(prior_prec_diag))).tocsc()

        return prior_var, prior_prec, prior_prec_diag

    # @profile
    def calculate_marginal_loglikelihood_slow_fast_sparse(self):
        y = self.y
        X = self.X
        process_prec = self.process_prec_matrix
        prior_mean = self.prior_mean
        prior_cov = self.prior_cov
        prior_prec = self.prior_prec
        prior_prec_diag = self.prior_prec_diag

        a = X.T.dot(process_prec)
        beta_prec = a.dot(X) + prior_prec
        beta_cov = pinv(beta_prec, self)
        beta_mean = beta_cov @ (a.dot(y) + prior_prec.dot(prior_mean))
        beta_mean = np.asarray(beta_mean).reshape(-1,1)

        try:
            beta_logdet = log_det(beta_cov, self)
        except:
            logging.critical(&#39;Crashed in log_det&#39;)
            logging.critical(&#39;beta_cov:\n{}&#39;.format(beta_cov))
            logging.critical(&#39;prior_prec\n{}&#39;.format(prior_prec))
            raise
        
        priorvar_logdet = log_det(prior_cov, self)
        ll2 = 0.5 * (beta_logdet - priorvar_logdet)

        bEbprior = np.asarray(prior_mean.T @ prior_prec.dot(prior_mean))[0,0]
        bEb = np.asarray(beta_mean.T @ beta_prec.dot(beta_mean) )[0,0]
        ll3 = 0.5 * (bEb - bEbprior)

        self.a = a
        self.beta_prec = beta_prec

        return ll2 + ll3

    @staticmethod
    @numba.jit(nopython=True, cache=True)
    def get_indices(a, keypair2col, tmems, smems):
        &#39;&#39;&#39;Use Just in Time compilation to reduce the &#39;getting&#39; time
        by about 95%

        Parameters
        ----------
        keypair2col : np.ndarray
            Maps (target_oidx, source_oidx) pair to the place the interaction
            index would be on a full interactio design matrix on the OTU level
        tmems, smems : np.ndarray
            These are the OTU indices in the target cluster and the source cluster
            respectively
        &#39;&#39;&#39;
        i = 0
        for tidx in tmems:
            for sidx in smems:
                a[i] = keypair2col[tidx, sidx]
                i += 1
        return a

    @staticmethod
    # @numba.jit(nopython=True, cache=True)
    def make_iidx2cidxpair(ret, n_clusters):
        &#39;&#39;&#39;Map the index of a cluster interaction to (dst,src) of clusters

        Parameters
        ----------
        n_clusters : int
            Number of clusters

        Returns
        -------
        np.ndarray(n_interactions,2)
            First column is the destination cluster index, second column is the source
            cluster index
        &#39;&#39;&#39;
        i = 0
        for dst_cidx in range(n_clusters):
            for src_cidx in range(n_clusters):
                if dst_cidx == src_cidx:
                    continue
                ret[i,0] = dst_cidx
                ret[i,1] = src_cidx
                i += 1
        
        return ret</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mdsine2.pylab.multiprocessing.PersistentWorker" href="pylab/multiprocessing.html#mdsine2.pylab.multiprocessing.PersistentWorker">PersistentWorker</a></li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="mdsine2.clustering.SingleClusterFullParallelization.get_indices"><code class="name flex">
<span>def <span class="ident">get_indices</span></span>(<span>a, keypair2col, tmems, smems)</span>
</code></dt>
<dd>
<div class="desc"><p>Use Just in Time compilation to reduce the 'getting' time
by about 95%</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>keypair2col</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Maps (target_oidx, source_oidx) pair to the place the interaction
index would be on a full interactio design matrix on the OTU level</dd>
<dt><strong><code>tmems</code></strong>, <strong><code>smems</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>These are the OTU indices in the target cluster and the source cluster
respectively</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
@numba.jit(nopython=True, cache=True)
def get_indices(a, keypair2col, tmems, smems):
    &#39;&#39;&#39;Use Just in Time compilation to reduce the &#39;getting&#39; time
    by about 95%

    Parameters
    ----------
    keypair2col : np.ndarray
        Maps (target_oidx, source_oidx) pair to the place the interaction
        index would be on a full interactio design matrix on the OTU level
    tmems, smems : np.ndarray
        These are the OTU indices in the target cluster and the source cluster
        respectively
    &#39;&#39;&#39;
    i = 0
    for tidx in tmems:
        for sidx in smems:
            a[i] = keypair2col[tidx, sidx]
            i += 1
    return a</code></pre>
</details>
</dd>
<dt id="mdsine2.clustering.SingleClusterFullParallelization.make_iidx2cidxpair"><code class="name flex">
<span>def <span class="ident">make_iidx2cidxpair</span></span>(<span>ret, n_clusters)</span>
</code></dt>
<dd>
<div class="desc"><p>Map the index of a cluster interaction to (dst,src) of clusters</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>n_clusters</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of clusters</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.ndarray(n_interactions,2)</code></dt>
<dd>First column is the destination cluster index, second column is the source
cluster index</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
# @numba.jit(nopython=True, cache=True)
def make_iidx2cidxpair(ret, n_clusters):
    &#39;&#39;&#39;Map the index of a cluster interaction to (dst,src) of clusters

    Parameters
    ----------
    n_clusters : int
        Number of clusters

    Returns
    -------
    np.ndarray(n_interactions,2)
        First column is the destination cluster index, second column is the source
        cluster index
    &#39;&#39;&#39;
    i = 0
    for dst_cidx in range(n_clusters):
        for src_cidx in range(n_clusters):
            if dst_cidx == src_cidx:
                continue
            ret[i,0] = dst_cidx
            ret[i,1] = src_cidx
            i += 1
    
    return ret</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="mdsine2.clustering.SingleClusterFullParallelization.build_interactions_matrix"><code class="name flex">
<span>def <span class="ident">build_interactions_matrix</span></span>(<span>self, on_columns)</span>
</code></dt>
<dd>
<div class="desc"><p>Build the interaction matrix</p>
<p>First we make the rows and columns for the mixing matrix,
then we multiple the base matrix and the mixing matrix.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build_interactions_matrix(self, on_columns):
    &#39;&#39;&#39;Build the interaction matrix

    First we make the rows and columns for the mixing matrix,
    then we multiple the base matrix and the mixing matrix.
    &#39;&#39;&#39;
    rows = []
    cols = []

    # c2ciidx = Cluster-to-Cluster Interaction InDeX
    c2ciidx = 0
    for ccc in on_columns:
        tcidx = self.iidx2cidxpair[ccc, 0]
        scidx = self.iidx2cidxpair[ccc, 1]
        
        smems = self.clustering.clusters[scidx]
        tmems = self.clustering.clusters[tcidx]
        
        a = np.zeros(len(smems)*len(tmems), dtype=int)
        rows.append(SingleClusterFullParallelization.get_indices(a,
            self.keypair2col_interactions, tmems, smems))
        cols.append(np.full(len(tmems)*len(smems), fill_value=c2ciidx))
        c2ciidx += 1

    rows = np.asarray(list(itertools.chain.from_iterable(rows)))
    cols = np.asarray(list(itertools.chain.from_iterable(cols)))
    data = np.ones(len(rows), dtype=int)

    # print(&#39;rows&#39;, rows)
    # print(cols)
    # print(data)
    # print((self.n_rowsM, c2ciidx))

    M = scipy.sparse.coo_matrix((data,(rows,cols)),
        shape=(self.n_rowsM, c2ciidx)).tocsc()
    ret = self.base_X @ M
    return ret</code></pre>
</details>
</dd>
<dt id="mdsine2.clustering.SingleClusterFullParallelization.build_perturbations_matrix"><code class="name flex">
<span>def <span class="ident">build_perturbations_matrix</span></span>(<span>self, on_columns)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build_perturbations_matrix(self, on_columns):
    if not self.there_are_perturbations:
        raise ValueError(&#39;You should not be here&#39;)
    
    keypair2col = self.keypair2col_perturbations
    rows = []
    cols = []

    col = 0
    for pidx, pert_ind_idxs in enumerate(on_columns):
        for cidx in pert_ind_idxs:
            for oidx in self.clustering.clusters[cidx]:
                rows.append(keypair2col[oidx, pidx])
                cols.append(col)
            col += 1
    
    data = np.ones(len(rows), dtype=np.float64)
    M = scipy.sparse.coo_matrix((data,(rows,cols)),
        shape=(self.n_rowsMpert, col)).tocsc()
    ret = self.base_Xpert @ M
    return ret</code></pre>
</details>
</dd>
<dt id="mdsine2.clustering.SingleClusterFullParallelization.build_prior_cov_and_prec_and_diag"><code class="name flex">
<span>def <span class="ident">build_prior_cov_and_prec_and_diag</span></span>(<span>self, on_interactions, on_perturbations)</span>
</code></dt>
<dd>
<div class="desc"><p>Build the prior covariance matrices and others</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build_prior_cov_and_prec_and_diag(self, on_interactions, on_perturbations):
    &#39;&#39;&#39;Build the prior covariance matrices and others
    &#39;&#39;&#39;
    ret = []
    for pidx, pert in enumerate(on_perturbations):
        ret = np.append(ret, 
            np.full(len(pert), fill_value=self.prior_var_pert[pidx]))
    prior_var_diag = np.append(ret, np.full(len(on_interactions), fill_value=self.prior_var_interactions))
    prior_prec_diag = 1/prior_var_diag

    prior_var = scipy.sparse.dia_matrix((prior_var_diag,[0]), 
        shape=(len(prior_var_diag),len(prior_var_diag))).tocsc()
    prior_prec = scipy.sparse.dia_matrix((prior_prec_diag,[0]), 
        shape=(len(prior_prec_diag),len(prior_prec_diag))).tocsc()

    return prior_var, prior_prec, prior_prec_diag</code></pre>
</details>
</dd>
<dt id="mdsine2.clustering.SingleClusterFullParallelization.build_prior_mean"><code class="name flex">
<span>def <span class="ident">build_prior_mean</span></span>(<span>self, on_interactions, on_perturbations)</span>
</code></dt>
<dd>
<div class="desc"><p>Build the prior mean array</p>
<p>Perturbations go first and then interactions</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build_prior_mean(self, on_interactions, on_perturbations):
    &#39;&#39;&#39;Build the prior mean array

    Perturbations go first and then interactions
    &#39;&#39;&#39;
    ret = []
    for pidx, pert in enumerate(on_perturbations):
        ret = np.append(ret, 
            np.full(len(pert), fill_value=self.prior_mean_pert[pidx]))
    ret = np.append(ret, np.full(len(on_interactions), fill_value=self.prior_mean_interactions))
    return ret.reshape(-1,1)</code></pre>
</details>
</dd>
<dt id="mdsine2.clustering.SingleClusterFullParallelization.calculate_marginal_loglikelihood_slow_fast_sparse"><code class="name flex">
<span>def <span class="ident">calculate_marginal_loglikelihood_slow_fast_sparse</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calculate_marginal_loglikelihood_slow_fast_sparse(self):
    y = self.y
    X = self.X
    process_prec = self.process_prec_matrix
    prior_mean = self.prior_mean
    prior_cov = self.prior_cov
    prior_prec = self.prior_prec
    prior_prec_diag = self.prior_prec_diag

    a = X.T.dot(process_prec)
    beta_prec = a.dot(X) + prior_prec
    beta_cov = pinv(beta_prec, self)
    beta_mean = beta_cov @ (a.dot(y) + prior_prec.dot(prior_mean))
    beta_mean = np.asarray(beta_mean).reshape(-1,1)

    try:
        beta_logdet = log_det(beta_cov, self)
    except:
        logging.critical(&#39;Crashed in log_det&#39;)
        logging.critical(&#39;beta_cov:\n{}&#39;.format(beta_cov))
        logging.critical(&#39;prior_prec\n{}&#39;.format(prior_prec))
        raise
    
    priorvar_logdet = log_det(prior_cov, self)
    ll2 = 0.5 * (beta_logdet - priorvar_logdet)

    bEbprior = np.asarray(prior_mean.T @ prior_prec.dot(prior_mean))[0,0]
    bEb = np.asarray(beta_mean.T @ beta_prec.dot(beta_mean) )[0,0]
    ll3 = 0.5 * (bEb - bEbprior)

    self.a = a
    self.beta_prec = beta_prec

    return ll2 + ll3</code></pre>
</details>
</dd>
<dt id="mdsine2.clustering.SingleClusterFullParallelization.initialize_gibbs"><code class="name flex">
<span>def <span class="ident">initialize_gibbs</span></span>(<span>self, base_Xdata, base_Xpertdata, concentration, m, y, process_prec_diag, prior_var_interactions, prior_var_pert, prior_mean_interactions, prior_mean_pert)</span>
</code></dt>
<dd>
<div class="desc"><p>Pass in the information that changes every Gibbs step</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>base_X</code></strong> :&ensp;<code>scipy.sparse.csc_matrix</code></dt>
<dd>Sparse matrix for the interaction terms</dd>
<dt><strong><code>base_Xpert</code></strong> :&ensp;<code>scipy.sparse.csc_matrix, None</code></dt>
<dd>Sparse matrix for the perturbation terms
If None, there are no perturbations</dd>
<dt><strong><code>concentration</code></strong> :&ensp;<code>float</code></dt>
<dd>This is the concentration of the system</dd>
<dt><strong><code>m</code></strong> :&ensp;<code>int</code></dt>
<dd>This is the auxiliary variable for the marginalization</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>This is the observation array</dd>
<dt><strong><code>process_prec_diag</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>This is the process precision diagonal</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def initialize_gibbs(self, base_Xdata, base_Xpertdata, concentration, m, y,
    process_prec_diag, prior_var_interactions, prior_var_pert, 
    prior_mean_interactions, prior_mean_pert):
    &#39;&#39;&#39;Pass in the information that changes every Gibbs step

    Parameters
    ----------
    base_X : scipy.sparse.csc_matrix
        Sparse matrix for the interaction terms
    base_Xpert : scipy.sparse.csc_matrix, None
        Sparse matrix for the perturbation terms
        If None, there are no perturbations
    concentration : float
        This is the concentration of the system
    m : int
        This is the auxiliary variable for the marginalization
    y : np.ndarray
        This is the observation array
    process_prec_diag : np.ndarray
        This is the process precision diagonal
    &#39;&#39;&#39;
    self.base_X = scipy.sparse.coo_matrix(
        (base_Xdata,(self.base_Xrows,self.base_Xcols)),
        shape=self.base_Xshape).tocsc()
    
    self.concentration = concentration
    self.m = m
    self.y = y.reshape(-1,1)
    self.n_rows = len(y)
    self.n_cols_X = self.base_X.shape[1]
    self.prior_var_interactions = prior_var_interactions
    self.prior_prec_interactions = 1/prior_var_interactions
    self.prior_mean_interactions = prior_mean_interactions

    if self.there_are_perturbations:
        self.base_Xpert = scipy.sparse.coo_matrix(
            (base_Xpertdata,(self.base_Xpertrows,self.base_Xpertcols)),
            shape=self.base_Xpertshape).tocsc()
        self.prior_var_pert = prior_var_pert
        self.prior_prec_pert = 1/prior_var_pert
        self.prior_mean_pert = prior_mean_pert

    self.process_prec_matrix = scipy.sparse.dia_matrix(
        (process_prec_diag,[0]), shape=(len(process_prec_diag),len(process_prec_diag))).tocsc()</code></pre>
</details>
</dd>
<dt id="mdsine2.clustering.SingleClusterFullParallelization.initialize_oidx"><code class="name flex">
<span>def <span class="ident">initialize_oidx</span></span>(<span>self, interaction_on_idxs, perturbation_on_idxs)</span>
</code></dt>
<dd>
<div class="desc"><p>Pass in the parameters that change for every OTU - potentially</p>
<h2 id="parameters">Parameters</h2></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def initialize_oidx(self, interaction_on_idxs, perturbation_on_idxs):
    &#39;&#39;&#39;Pass in the parameters that change for every OTU - potentially

    Parameters
    ----------
    
    &#39;&#39;&#39;
    self.saved_interaction_on_idxs = interaction_on_idxs
    self.saved_perturbation_on_idxs = perturbation_on_idxs</code></pre>
</details>
</dd>
<dt id="mdsine2.clustering.SingleClusterFullParallelization.run"><code class="name flex">
<span>def <span class="ident">run</span></span>(<span>self, interaction_on_idxs, perturbation_on_idxs, cluster_config, log_mult_factor, cid, use_saved_params)</span>
</code></dt>
<dd>
<div class="desc"><p>Pass in the parameters for the specific cluster assignment for
the OTU and run the marginalization</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>interaction_on_idxs</code></strong> :&ensp;<code>np.array(int)</code></dt>
<dd>An array of indices for the interactions that are on. Assumes that the
clustering is in the order specified in <code>cluster_config</code></dd>
<dt><strong><code>perturbation_on_idxs</code></strong> :&ensp;<code>list(np.ndarray(int)), None</code></dt>
<dd>If there are perturbations, then we set the perturbation idxs on
Each element in the list are the indices of that perturbation that are on</dd>
<dt><strong><code>cluster_config</code></strong> :&ensp;<code>list(list(int))</code></dt>
<dd>This is the cluster configuration and in cluster order.</dd>
<dt><strong><code>log_mult_factor</code></strong> :&ensp;<code>float</code></dt>
<dd>This is the log multiplication factor that we add onto the marginalization</dd>
<dt><strong><code>use_saved_params</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, passed in <code>interaction_on_idxs</code> and <code>perturbation_on_idxs</code> are None
and we can use <code>saved_interaction_on_idxs</code> and <code>saved_perturbation_on_idxs</code></dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run(self, interaction_on_idxs, perturbation_on_idxs, cluster_config, log_mult_factor, cid, 
    use_saved_params):
    &#39;&#39;&#39;Pass in the parameters for the specific cluster assignment for
    the OTU and run the marginalization


    Parameters
    ----------
    interaction_on_idxs : np.array(int)
        An array of indices for the interactions that are on. Assumes that the
        clustering is in the order specified in `cluster_config`
    perturbation_on_idxs : list(np.ndarray(int)), None
        If there are perturbations, then we set the perturbation idxs on
        Each element in the list are the indices of that perturbation that are on
    cluster_config : list(list(int))
        This is the cluster configuration and in cluster order.
    log_mult_factor : float
        This is the log multiplication factor that we add onto the marginalization
    use_saved_params : bool
        If True, passed in `interaction_on_idxs` and `perturbation_on_idxs` are None
        and we can use `saved_interaction_on_idxs` and `saved_perturbation_on_idxs`
    &#39;&#39;&#39;
    if use_saved_params:
        interaction_on_idxs = self.saved_interaction_on_idxs
        perturbation_on_idxs = self.saved_perturbation_on_idxs

    # We need to make the arrays for interactions and perturbations
    self.set_clustering(cluster_config=cluster_config)
    Xinteractions = self.build_interactions_matrix(on_columns=interaction_on_idxs)

    if self.there_are_perturbations:
        Xperturbations = self.build_perturbations_matrix(on_columns=perturbation_on_idxs)
        X = scipy.sparse.hstack([Xperturbations, Xinteractions])
    else:
        X = Xinteractions
    self.X = X
    self.prior_mean = self.build_prior_mean(on_interactions=interaction_on_idxs,
        on_perturbations=perturbation_on_idxs)
    self.prior_cov, self.prior_prec, self.prior_prec_diag = self.build_prior_cov_and_prec_and_diag(
        on_interactions=interaction_on_idxs, on_perturbations=perturbation_on_idxs)
    
    return cid, self.calculate_marginal_loglikelihood_slow_fast_sparse()</code></pre>
</details>
</dd>
<dt id="mdsine2.clustering.SingleClusterFullParallelization.set_clustering"><code class="name flex">
<span>def <span class="ident">set_clustering</span></span>(<span>self, cluster_config)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_clustering(self, cluster_config):
    self.clustering = CondensedClustering(oidx2cidx=cluster_config)
    self.iidx2cidxpair = np.zeros(shape=(len(self.clustering)*(len(self.clustering)-1), 2), 
        dtype=int)
    self.iidx2cidxpair = SingleClusterFullParallelization.make_iidx2cidxpair(
        ret=self.iidx2cidxpair,
        n_clusters=len(self.clustering))</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="mdsine2" href="index.html">mdsine2</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="mdsine2.clustering.ClusterAssignments" href="#mdsine2.clustering.ClusterAssignments">ClusterAssignments</a></code></h4>
<ul class="">
<li><code><a title="mdsine2.clustering.ClusterAssignments.add_init_value" href="#mdsine2.clustering.ClusterAssignments.add_init_value">add_init_value</a></code></li>
<li><code><a title="mdsine2.clustering.ClusterAssignments.add_trace" href="#mdsine2.clustering.ClusterAssignments.add_trace">add_trace</a></code></li>
<li><code><a title="mdsine2.clustering.ClusterAssignments.calculate_marginal_loglikelihood_slow" href="#mdsine2.clustering.ClusterAssignments.calculate_marginal_loglikelihood_slow">calculate_marginal_loglikelihood_slow</a></code></li>
<li><code><a title="mdsine2.clustering.ClusterAssignments.calculate_marginal_loglikelihood_slow_fast" href="#mdsine2.clustering.ClusterAssignments.calculate_marginal_loglikelihood_slow_fast">calculate_marginal_loglikelihood_slow_fast</a></code></li>
<li><code><a title="mdsine2.clustering.ClusterAssignments.calculate_marginal_loglikelihood_slow_fast_sparse" href="#mdsine2.clustering.ClusterAssignments.calculate_marginal_loglikelihood_slow_fast_sparse">calculate_marginal_loglikelihood_slow_fast_sparse</a></code></li>
<li><code><a title="mdsine2.clustering.ClusterAssignments.gibbs_update_single_asv_parallel" href="#mdsine2.clustering.ClusterAssignments.gibbs_update_single_asv_parallel">gibbs_update_single_asv_parallel</a></code></li>
<li><code><a title="mdsine2.clustering.ClusterAssignments.gibbs_update_single_asv_slow" href="#mdsine2.clustering.ClusterAssignments.gibbs_update_single_asv_slow">gibbs_update_single_asv_slow</a></code></li>
<li><code><a title="mdsine2.clustering.ClusterAssignments.gibbs_update_single_asv_slow_fast" href="#mdsine2.clustering.ClusterAssignments.gibbs_update_single_asv_slow_fast">gibbs_update_single_asv_slow_fast</a></code></li>
<li><code><a title="mdsine2.clustering.ClusterAssignments.initialize" href="#mdsine2.clustering.ClusterAssignments.initialize">initialize</a></code></li>
<li><code><a title="mdsine2.clustering.ClusterAssignments.kill" href="#mdsine2.clustering.ClusterAssignments.kill">kill</a></code></li>
<li><code><a title="mdsine2.clustering.ClusterAssignments.sample_iter" href="#mdsine2.clustering.ClusterAssignments.sample_iter">sample_iter</a></code></li>
<li><code><a title="mdsine2.clustering.ClusterAssignments.set_trace" href="#mdsine2.clustering.ClusterAssignments.set_trace">set_trace</a></code></li>
<li><code><a title="mdsine2.clustering.ClusterAssignments.update_mp" href="#mdsine2.clustering.ClusterAssignments.update_mp">update_mp</a></code></li>
<li><code><a title="mdsine2.clustering.ClusterAssignments.update_slow" href="#mdsine2.clustering.ClusterAssignments.update_slow">update_slow</a></code></li>
<li><code><a title="mdsine2.clustering.ClusterAssignments.update_slow_fast" href="#mdsine2.clustering.ClusterAssignments.update_slow_fast">update_slow_fast</a></code></li>
<li><code><a title="mdsine2.clustering.ClusterAssignments.value" href="#mdsine2.clustering.ClusterAssignments.value">value</a></code></li>
<li><code><a title="mdsine2.clustering.ClusterAssignments.visualize_posterior" href="#mdsine2.clustering.ClusterAssignments.visualize_posterior">visualize_posterior</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="mdsine2.clustering.Concentration" href="#mdsine2.clustering.Concentration">Concentration</a></code></h4>
<ul class="">
<li><code><a title="mdsine2.clustering.Concentration.initialize" href="#mdsine2.clustering.Concentration.initialize">initialize</a></code></li>
<li><code><a title="mdsine2.clustering.Concentration.update" href="#mdsine2.clustering.Concentration.update">update</a></code></li>
<li><code><a title="mdsine2.clustering.Concentration.visualize_posterior" href="#mdsine2.clustering.Concentration.visualize_posterior">visualize_posterior</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="mdsine2.clustering.CondensedClustering" href="#mdsine2.clustering.CondensedClustering">CondensedClustering</a></code></h4>
</li>
<li>
<h4><code><a title="mdsine2.clustering.SingleClusterFullParallelization" href="#mdsine2.clustering.SingleClusterFullParallelization">SingleClusterFullParallelization</a></code></h4>
<ul class="">
<li><code><a title="mdsine2.clustering.SingleClusterFullParallelization.build_interactions_matrix" href="#mdsine2.clustering.SingleClusterFullParallelization.build_interactions_matrix">build_interactions_matrix</a></code></li>
<li><code><a title="mdsine2.clustering.SingleClusterFullParallelization.build_perturbations_matrix" href="#mdsine2.clustering.SingleClusterFullParallelization.build_perturbations_matrix">build_perturbations_matrix</a></code></li>
<li><code><a title="mdsine2.clustering.SingleClusterFullParallelization.build_prior_cov_and_prec_and_diag" href="#mdsine2.clustering.SingleClusterFullParallelization.build_prior_cov_and_prec_and_diag">build_prior_cov_and_prec_and_diag</a></code></li>
<li><code><a title="mdsine2.clustering.SingleClusterFullParallelization.build_prior_mean" href="#mdsine2.clustering.SingleClusterFullParallelization.build_prior_mean">build_prior_mean</a></code></li>
<li><code><a title="mdsine2.clustering.SingleClusterFullParallelization.calculate_marginal_loglikelihood_slow_fast_sparse" href="#mdsine2.clustering.SingleClusterFullParallelization.calculate_marginal_loglikelihood_slow_fast_sparse">calculate_marginal_loglikelihood_slow_fast_sparse</a></code></li>
<li><code><a title="mdsine2.clustering.SingleClusterFullParallelization.get_indices" href="#mdsine2.clustering.SingleClusterFullParallelization.get_indices">get_indices</a></code></li>
<li><code><a title="mdsine2.clustering.SingleClusterFullParallelization.initialize_gibbs" href="#mdsine2.clustering.SingleClusterFullParallelization.initialize_gibbs">initialize_gibbs</a></code></li>
<li><code><a title="mdsine2.clustering.SingleClusterFullParallelization.initialize_oidx" href="#mdsine2.clustering.SingleClusterFullParallelization.initialize_oidx">initialize_oidx</a></code></li>
<li><code><a title="mdsine2.clustering.SingleClusterFullParallelization.make_iidx2cidxpair" href="#mdsine2.clustering.SingleClusterFullParallelization.make_iidx2cidxpair">make_iidx2cidxpair</a></code></li>
<li><code><a title="mdsine2.clustering.SingleClusterFullParallelization.run" href="#mdsine2.clustering.SingleClusterFullParallelization.run">run</a></code></li>
<li><code><a title="mdsine2.clustering.SingleClusterFullParallelization.set_clustering" href="#mdsine2.clustering.SingleClusterFullParallelization.set_clustering">set_clustering</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.1</a>.</p>
</footer>
</body>
</html>